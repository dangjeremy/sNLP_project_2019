{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Finetune BERT for test set.ipynb","version":"0.3.2","provenance":[{"file_id":"1zjnnLFWeq4Ln7yGT8wL-yJHkk6fvk601","timestamp":1550943199879}],"collapsed_sections":["9JSPCPlhhP8b","kIotvvz9hmg0","oLuoNWS9jpDN","M0dqVa-wiIvw","YQmYjvjaiaOi","tSjMinL3kEoU","H1iZRlmolx2D","oSCY3DXCpeXD","hESTSfY1Cjuj","VqoUXW4dnOHV","sPwhHFD5y--Z","gq-bfPTd7M_S","s-PcHZ-RFyb1"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"CF5L2PZhftCU","colab_type":"text"},"cell_type":"markdown","source":["# NLP assignment 3 - finetune BERT for test sets"]},{"metadata":{"id":"7du_i1SiPAmj","colab_type":"text"},"cell_type":"markdown","source":["We use to PyTorch implementation of BERT from: https://github.com/huggingface/pytorch-pretrained-BERT\n","â€‹\n","We have used this blog post (https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d) and the supporting code (https://nbviewer.jupyter.org/github/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb) as a model for implementing our classifier. We refer to this below as Trivedi 2019. "]},{"metadata":{"id":"9JSPCPlhhP8b","colab_type":"text"},"cell_type":"markdown","source":["## Imports"]},{"metadata":{"id":"2vrYXENJfmwl","colab_type":"code","outputId":"dadcb335-810b-48c5-822d-521398beafbd","executionInfo":{"status":"ok","timestamp":1552314959328,"user_tz":0,"elapsed":5129,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":395}},"cell_type":"code","source":["!pip install pytorch-pretrained-bert"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.106)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2018.11.29)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.106 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.106)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.106->boto3->pytorch-pretrained-bert) (2.5.3)\n","Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.106->boto3->pytorch-pretrained-bert) (0.14)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.106->boto3->pytorch-pretrained-bert) (1.11.0)\n"],"name":"stdout"}]},{"metadata":{"id":"MG3RAjIZhT1w","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import pickle\n","from sklearn.metrics import f1_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KUg_POWFhTzY","colab_type":"code","outputId":"a18c6642-37b3-41c0-f321-a49e059495c3","executionInfo":{"status":"ok","timestamp":1552314965982,"user_tz":0,"elapsed":3105,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from pytorch_pretrained_bert.tokenization import BertTokenizer, WordpieceTokenizer\n","from pytorch_pretrained_bert.modeling import BertForPreTraining, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification #PretrainedBertModel\n","from pathlib import Path\n","import torch\n","import re\n","from torch import Tensor\n","from torch.nn import BCEWithLogitsLoss\n","from fastai.text import Tokenizer, Vocab\n","import collections\n","import os\n","import pdb\n","from tqdm import tqdm, trange\n","import sys\n","import random\n","from sklearn.model_selection import train_test_split\n","module_path = os.path.abspath(os.path.join('..'))\n","if module_path not in sys.path:\n","    sys.path.append(module_path)\n","\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from pytorch_pretrained_bert.optimization import BertAdam"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"],"name":"stdout"}]},{"metadata":{"id":"8fPxVghshTxB","colab_type":"code","colab":{}},"cell_type":"code","source":["import logging\n","logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                    datefmt='%m/%d/%Y %H:%M:%S',\n","                    level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QrUUrn1dhTuz","colab_type":"code","outputId":"fbbd062a-3afe-4e2b-d705-9162029c08dd","executionInfo":{"status":"ok","timestamp":1552314965987,"user_tz":0,"elapsed":1183,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"kIotvvz9hmg0","colab_type":"text"},"cell_type":"markdown","source":["## Define classes"]},{"metadata":{"id":"TKLh52fpVi-a","colab_type":"text"},"cell_type":"markdown","source":["These are all from the PyTorch BERT Github - copied in for reference when we were setting up the features."]},{"metadata":{"id":"P4CsPT76hlgy","colab_type":"code","colab":{}},"cell_type":"code","source":["class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single\n","            sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence.\n","            Only must be specified for sequence pair tasks.\n","            label: (Optional) string. The label of the example. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label = label"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IU3MZbKChqxj","colab_type":"code","colab":{}},"cell_type":"code","source":["class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n-jx3tOhhqva","colab_type":"code","colab":{}},"cell_type":"code","source":["def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n","\n","    label_map = {label : i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        tokens_a = tokenizer.tokenize(example.text_a)\n","\n","        tokens_b = None\n","        if example.text_b:\n","            tokens_b = tokenizer.tokenize(example.text_b)\n","            # Modifies `tokens_a` and `tokens_b` in place so that the total\n","            # length is less than the specified length.\n","            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n","        else:\n","            # Account for [CLS] and [SEP] with \"- 2\"\n","            if len(tokens_a) > max_seq_length - 2:\n","                tokens_a = tokens_a[:(max_seq_length - 2)]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids: 0   0   0   0  0     0 0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambigiously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens += tokens_b + [\"[SEP]\"]\n","            segment_ids += [1] * (len(tokens_b) + 1)\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding = [0] * (max_seq_length - len(input_ids))\n","        input_ids += padding\n","        input_mask += padding\n","        segment_ids += padding\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","\n","        label_ids = label_map[example.label]\n","        if ex_index < 5:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\" % (example.guid))\n","            logger.info(\"tokens: %s\" % \" \".join(\n","                    [str(x) for x in tokens]))\n","            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n","            logger.info(\n","                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n","            logger.info(\"label: %s (id = %d)\" % (example.label, label_ids))\n","\n","        features.append(\n","                InputFeatures(input_ids=input_ids,\n","                              input_mask=input_mask,\n","                              segment_ids=segment_ids,\n","                              label_ids=label_ids))\n","    return features"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oLuoNWS9jpDN","colab_type":"text"},"cell_type":"markdown","source":["## Training functions"]},{"metadata":{"id":"r7UfeObXVntj","colab_type":"text"},"cell_type":"markdown","source":["These functions are based on those from Trivedi 2019."]},{"metadata":{"id":"d3xOUO9Pj8Wh","colab_type":"code","colab":{}},"cell_type":"code","source":["def warmup_linear(x, warmup=0.002):\n","    if x < warmup:\n","        return x/warmup\n","    return 1.0 - x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1fnrlf3DjoiX","colab_type":"code","colab":{}},"cell_type":"code","source":["def fit(model, train_dataloader, device, optimizer, num_epochs):\n","    \n","    resultsdf = pd.DataFrame(columns = ['epoch', 'train loss', 'train accuracy', 'validation loss', 'validation accuracy'])\n","    batch_losses = []\n","    \n","    global_step = 0\n","    model.train()\n","    for i_ in (range(int(num_epochs))):\n","\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        for step, batch in enumerate(train_dataloader):\n","\n","            batch = tuple(t.to(device) for t in batch)\n","            input_ids, input_mask, segment_ids, label_ids = batch\n","            loss = model(input_ids, segment_ids, input_mask, label_ids)\n","            \n","            batch_losses.append(loss.item())\n","            \n","            if step % 50 ==0:\n","              logger.info(f\"Loss on batch {step}: {loss}\")\n","                      \n","            if args['fp16']:\n","              optimizer.backward(loss)\n","            else:\n","              loss.backward()\n","\n","            tr_loss += loss.item()\n","            nb_tr_examples += input_ids.size(0)\n","            nb_tr_steps += 1\n","\n","            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n","              if args['fp16']:\n","                # modify learning rate with special warm up BERT uses\n","                # if args.fp16 is False, BertAdam is used that handles this automatically\n","                lr_this_step = args['learning_rate'] * warmup_linear(global_step/num_train_optimization_steps, args['warmup_proportion'])\n","                for param_group in optimizer.param_groups:\n","                  param_group['lr'] = lr_this_step\n","              optimizer.step()\n","              optimizer.zero_grad()\n","              global_step += 1\n","            \n","        logger.info('Training loss after epoch {}'.format(tr_loss / nb_tr_steps))\n","        train_tup = eval(train_examples, train_features, model=model, device=device)\n","        logger.info('Training accuracy after epoch {}'.format(train_tup[0]['accuracy']))\n","        logger.info(\"***** Running evaluation *****\")\n","        logger.info('Eval after epoch {}'.format(i_+1))\n","        eval_tup = eval(eval_examples, eval_features, model = model, device = device)\n","        logger.info(eval_tup[0])\n","        \n","        resultsdf = resultsdf.append({\"epoch\": i_+1, \"train loss\": train_tup[0]['loss'], \"train accuracy\": train_tup[0]['accuracy'],\n","                         \"validation loss\": eval_tup[0]['loss'], \"validation accuracy\": eval_tup[0]['accuracy']}, ignore_index=True)\n","    \n","    return resultsdf, batch_losses"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M0dqVa-wiIvw","colab_type":"text"},"cell_type":"markdown","source":["## Evaluation functions"]},{"metadata":{"id":"uZP_b-7YVuGC","colab_type":"text"},"cell_type":"markdown","source":["The functions accuracy() and eval() are based on those from Trevedi 2019."]},{"metadata":{"id":"qvAztVqvhqte","colab_type":"code","colab":{}},"cell_type":"code","source":["def accuracy(out, labels):\n","    outputs = np.argmax(out, axis=1)\n","    return np.sum(outputs == labels)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6nXRbONRiNae","colab_type":"text"},"cell_type":"markdown","source":["Use this function to caluclate the accuracy on the balanced task"]},{"metadata":{"id":"PPnkNhbuhqrK","colab_type":"code","colab":{}},"cell_type":"code","source":["def balanced_accuracy(out, labels):\n","  \n","  #'out' should be the logits put into a softmax\n","  paired_pred = []\n","  \n","  for i in np.arange(0, len(out),2):\n","    if out[i][1] < out[i+1][1]:\n","      paired_pred.append(0)\n","      paired_pred.append(1)\n","    else:\n","      paired_pred.append(1)\n","      paired_pred.append(0)\n","  \n","  return np.sum(np.array(paired_pred) == labels)/len(out)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HqMEVVy-SXO_","colab_type":"code","colab":{}},"cell_type":"code","source":["def softmax(A):\n","    \"\"\"\n","    Calculates the value of softmax function\n","    \n","    Inputs:\n","    A -- N x 2 array of logits from BERT\n","\n","    Output:\n","    probs -- N x 2 array of probabilities based on the softmax function\n","    \"\"\" \n","    \n","    temp = np.exp(A)\n","    sumtemp = np.sum(temp, axis=1).reshape(-1,1)\n","    \n","    probs = temp/sumtemp\n","    \n","    return probs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EJXrdfzDi0TF","colab_type":"code","colab":{}},"cell_type":"code","source":["def eval(eval_examples, eval_features, model, device):\n","        \n","    args['output_dir'].mkdir(exist_ok=True)\n","\n","    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n","    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n","    all_label_ids = torch.tensor([f.label_ids for f in eval_features], dtype=torch.long)\n","    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","    # Run prediction for full data\n","    eval_sampler = SequentialSampler(eval_data)\n","    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n","    \n","    all_logits = None\n","    all_labels = None\n","    \n","    model.eval()\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n","        input_ids = input_ids.to(device)\n","        input_mask = input_mask.to(device)\n","        segment_ids = segment_ids.to(device)\n","        label_ids = label_ids.to(device)\n","\n","        with torch.no_grad():\n","            tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n","            logits = model(input_ids, segment_ids, input_mask)\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = label_ids.to('cpu').numpy()\n","        tmp_eval_accuracy = accuracy(logits, label_ids)\n","        \n","        if all_logits is None:\n","            all_logits = logits\n","        else:\n","            all_logits = np.concatenate((all_logits, logits), axis=0)\n","            \n","        if all_labels is None:\n","            all_labels = label_ids\n","        else:    \n","            all_labels = np.concatenate((all_labels, label_ids), axis=0)\n","\n","        eval_loss += tmp_eval_loss.mean().item()\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        nb_eval_examples += input_ids.size(0)\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_examples\n","    \n","    result = {'loss': eval_loss,\n","              'accuracy': eval_accuracy}\n","    \n","    return (result, all_logits, all_labels)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YQmYjvjaiaOi","colab_type":"text"},"cell_type":"markdown","source":["## Set an output path and the default value of the arguments"]},{"metadata":{"id":"uGBJMlnDhqo4","colab_type":"code","colab":{}},"cell_type":"code","source":["OUTPUT_PATH = Path('gdrive/My Drive/tmp/output')\n","OUTPUT_PATH.mkdir(parents=True, exist_ok = True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dtkmv0CtV1B0","colab_type":"text"},"cell_type":"markdown","source":["The default arguments are based on those from Trivedi (2019)"]},{"metadata":{"id":"zTmr3r4Mhqmo","colab_type":"code","colab":{}},"cell_type":"code","source":["args = {\n","    \"train_size\": -1,\n","    \"val_size\": -1,\n","    \"task_name\": \"sarcpol\",\n","    \"no_cuda\": False,\n","    \"bert_model\": 'bert-base-uncased',\n","    \"output_dir\": OUTPUT_PATH,\n","    \"max_seq_length\": 50,\n","    \"do_train\": True,\n","    \"do_eval\": True,\n","    \"do_lower_case\": True,\n","    \"train_batch_size\": 32 ,\n","    \"eval_batch_size\": 32,\n","    \"learning_rate\": 3e-5,\n","    \"num_train_epochs\": 5,\n","    \"warmup_proportion\": 0.1,\n","    \"no_cuda\": False,\n","    \"local_rank\": -1,\n","    \"seed\": 42,\n","    \"gradient_accumulation_steps\": 1,\n","    \"optimize_on_cpu\": False,\n","    \"fp16\": False,\n","    \"loss_scale\": 128\n","}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NseEBEB1FNt5","colab_type":"code","outputId":"e052a87f-5c2d-4adb-e007-c41fbec89240","executionInfo":{"status":"ok","timestamp":1552315618883,"user_tz":0,"elapsed":1066,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["if args[\"local_rank\"] == -1 or args[\"no_cuda\"]:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"cpu\")\n","    n_gpu = torch.cuda.device_count()\n","#     n_gpu = 1\n","else:\n","    torch.cuda.set_device(args['local_rank'])\n","    device = torch.device(\"cuda\", args['local_rank'])\n","    n_gpu = 1\n","    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","    torch.distributed.init_process_group(backend='nccl')\n","logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n","        device, n_gpu, bool(args['local_rank'] != -1), args['fp16']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/11/2019 14:46:58 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n"],"name":"stderr"}]},{"metadata":{"id":"tSjMinL3kEoU","colab_type":"text"},"cell_type":"markdown","source":["## Load in the training and validation sets"]},{"metadata":{"id":"TD4kY-K7hqkM","colab_type":"code","colab":{}},"cell_type":"code","source":["#Select the path contained the datasets\n","SARC_POL = '/content/gdrive/My Drive/SARC pol/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AzbKi19HkdvW","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load in the required training sets\n","#traindf_100 = pd.read_csv(SARC_POL+'project_data/project_training_100.csv', index_col=0)\n","#traindf_50 = pd.read_csv(SARC_POL+'project_data/project_training_50.csv', index_col=0)\n","#traindf_25 = pd.read_csv(SARC_POL+'project_data/project_training_25.csv', index_col=0)\n","#traindf_12 = pd.read_csv(SARC_POL+'project_data/project_training_12.csv', index_col=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nUUR7mTvkdrt","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load in the validation set\n","validdf = pd.read_csv(SARC_POL+'project_data/project_validation.csv', index_col = 0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NYW219KbBcTX","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load in the test sets\n","testdf_bal = pd.read_csv(SARC_POL+'project_data/balanced_test.csv', index_col = 0)\n","testdf_unbal = pd.read_csv(SARC_POL+'project_data/unbalanced_test.csv', index_col = 0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H1iZRlmolx2D","colab_type":"text"},"cell_type":"markdown","source":["## Process the training and validation sets"]},{"metadata":{"id":"s9fQp0_zlgcc","colab_type":"code","colab":{}},"cell_type":"code","source":["#Process the training examples\n","#train_examples100 = []\n","\n","#for i in range(0,len(traindf.index)):\n","#        train_examples100.append(InputExample(str(i), traindf.loc[i,'response'], None, str(traindf.loc[i,'label'])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L_kXjtBhmALo","colab_type":"code","colab":{}},"cell_type":"code","source":["#Process the validation examples\n","eval_examples = []\n","\n","for i in range(0,len(validdf.index)):\n","        eval_examples.append(InputExample(str(i), validdf.loc[i,'response'], None, str(validdf.loc[i,'label'])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gScM9S9LDNWm","colab_type":"code","colab":{}},"cell_type":"code","source":["#Process the balances test set examples\n","testbal_examples = []\n","\n","for i in range(0,len(testdf_bal.index)):\n","        testbal_examples.append(InputExample(str(i), testdf_bal.loc[i,'response'], None, str(testdf_bal.loc[i,'label'])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qpECrpgoJDtU","colab_type":"code","colab":{}},"cell_type":"code","source":["#Process the unbalanced test set examples\n","testunbal_examples = []\n","\n","for i in range(0,len(testdf_unbal.index)):\n","        testunbal_examples.append(InputExample(str(i), testdf_unbal.loc[i,'response'], None, str(testdf_unbal.loc[i,'label'])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"afZIsxn5mGEZ","colab_type":"code","colab":{}},"cell_type":"code","source":["#Create a list of labels\n","label_list = ['0', '1']\n","num_labels = len(label_list)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PGIigmlSmJX9","colab_type":"code","outputId":"3076d4bb-1cbf-44d5-d288-b6967ffc15fd","executionInfo":{"status":"ok","timestamp":1552315634459,"user_tz":0,"elapsed":1607,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":94}},"cell_type":"code","source":["#Instantiate the tokenizer\n","tokenizer = BertTokenizer.from_pretrained(args['bert_model'], do_lower_case=args['do_lower_case'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/11/2019 14:47:13 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"],"name":"stderr"}]},{"metadata":{"id":"wnU3CIBTmNJe","colab_type":"code","outputId":"af0d8b30-8ecc-4bcb-ad7c-4296599727df","executionInfo":{"status":"ok","timestamp":1551253141918,"user_tz":0,"elapsed":46391,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":632}},"cell_type":"code","source":["#Create the features based on the training set\n","#train_features = convert_examples_to_features(train_examples, label_list, args[\"max_seq_length\"], tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["02/27/2019 07:39:01 - INFO - __main__ -   *** Example ***\n","02/27/2019 07:39:01 - INFO - __main__ -   guid: 0\n","02/27/2019 07:39:01 - INFO - __main__ -   tokens: [CLS] or anyone that ' s ever had to make an appeal . [SEP]\n","02/27/2019 07:39:01 - INFO - __main__ -   input_ids: 101 2030 3087 2008 1005 1055 2412 2018 2000 2191 2019 5574 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   label: 0 (id = 0)\n","02/27/2019 07:39:01 - INFO - __main__ -   *** Example ***\n","02/27/2019 07:39:01 - INFO - __main__ -   guid: 1\n","02/27/2019 07:39:01 - INFO - __main__ -   tokens: [CLS] trump is the health ##iest president to ever take office , so he doesn ' t have to worry about that . [SEP]\n","02/27/2019 07:39:01 - INFO - __main__ -   input_ids: 101 8398 2003 1996 2740 10458 2343 2000 2412 2202 2436 1010 2061 2002 2987 1005 1056 2031 2000 4737 2055 2008 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   label: 1 (id = 1)\n","02/27/2019 07:39:01 - INFO - __main__ -   *** Example ***\n","02/27/2019 07:39:01 - INFO - __main__ -   guid: 2\n","02/27/2019 07:39:01 - INFO - __main__ -   tokens: [CLS] i love sanders and everything he believes in but i think his policy ' s won ' t make it into or out of congress . [SEP]\n","02/27/2019 07:39:01 - INFO - __main__ -   input_ids: 101 1045 2293 12055 1998 2673 2002 7164 1999 2021 1045 2228 2010 3343 1005 1055 2180 1005 1056 2191 2009 2046 2030 2041 1997 3519 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   label: 0 (id = 0)\n","02/27/2019 07:39:01 - INFO - __main__ -   *** Example ***\n","02/27/2019 07:39:01 - INFO - __main__ -   guid: 3\n","02/27/2019 07:39:01 - INFO - __main__ -   tokens: [CLS] then don ' t forget to phone ##bank canvas ##sing dona ##ting your rent money for this month to him and we can make him president but we don ' t vote so it didn ' t matter lo ##l . [SEP]\n","02/27/2019 07:39:01 - INFO - __main__ -   input_ids: 101 2059 2123 1005 1056 5293 2000 3042 9299 10683 7741 24260 3436 2115 9278 2769 2005 2023 3204 2000 2032 1998 2057 2064 2191 2032 2343 2021 2057 2123 1005 1056 3789 2061 2009 2134 1005 1056 3043 8840 2140 1012 102 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   label: 1 (id = 1)\n","02/27/2019 07:39:01 - INFO - __main__ -   *** Example ***\n","02/27/2019 07:39:01 - INFO - __main__ -   guid: 4\n","02/27/2019 07:39:01 - INFO - __main__ -   tokens: [CLS] totally not a perfect example of the racism inherent in the trump movement that makes ethnic minorities think that other candidates are a better path for america [SEP]\n","02/27/2019 07:39:01 - INFO - __main__ -   input_ids: 101 6135 2025 1037 3819 2742 1997 1996 14398 16112 1999 1996 8398 2929 2008 3084 5636 14302 2228 2008 2060 5347 2024 1037 2488 4130 2005 2637 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","02/27/2019 07:39:01 - INFO - __main__ -   label: 1 (id = 1)\n"],"name":"stderr"}]},{"metadata":{"id":"jQI47fFBmNHJ","colab_type":"code","outputId":"9556ad6a-6d02-46ff-ff3a-038741230194","executionInfo":{"status":"ok","timestamp":1552315638056,"user_tz":0,"elapsed":1654,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":672}},"cell_type":"code","source":["#Create the features based on the validation set\n","eval_features = convert_examples_to_features(eval_examples, label_list, args[\"max_seq_length\"], tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/11/2019 14:47:16 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:16 - INFO - __main__ -   guid: 0\n","03/11/2019 14:47:16 - INFO - __main__ -   tokens: [CLS] and if trump builds that wall they will be stuck here . [SEP]\n","03/11/2019 14:47:16 - INFO - __main__ -   input_ids: 101 1998 2065 8398 16473 2008 2813 2027 2097 2022 5881 2182 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   label: 1 (id = 1)\n","03/11/2019 14:47:16 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:16 - INFO - __main__ -   guid: 1\n","03/11/2019 14:47:16 - INFO - __main__ -   tokens: [CLS] says a voluntary survey . [SEP]\n","03/11/2019 14:47:16 - INFO - __main__ -   input_ids: 101 2758 1037 10758 5002 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   label: 0 (id = 0)\n","03/11/2019 14:47:16 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:16 - INFO - __main__ -   guid: 2\n","03/11/2019 14:47:16 - INFO - __main__ -   tokens: [CLS] i demand an apology to all those poor nazis ! [SEP]\n","03/11/2019 14:47:16 - INFO - __main__ -   input_ids: 101 1045 5157 2019 12480 2000 2035 2216 3532 13157 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   label: 1 (id = 1)\n","03/11/2019 14:47:16 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:16 - INFO - __main__ -   guid: 3\n","03/11/2019 14:47:16 - INFO - __main__ -   tokens: [CLS] why didn ##t hitler think of invalid ##ating nuremberg ? [SEP]\n","03/11/2019 14:47:16 - INFO - __main__ -   input_ids: 101 2339 2134 2102 8042 2228 1997 19528 5844 19346 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   label: 0 (id = 0)\n","03/11/2019 14:47:16 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:16 - INFO - __main__ -   guid: 4\n","03/11/2019 14:47:16 - INFO - __main__ -   tokens: [CLS] the amount of ne ##gat ##ivity here is massive . [SEP]\n","03/11/2019 14:47:16 - INFO - __main__ -   input_ids: 101 1996 3815 1997 11265 20697 7730 2182 2003 5294 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:16 - INFO - __main__ -   label: 0 (id = 0)\n"],"name":"stderr"}]},{"metadata":{"id":"3ovTsgqzDLfq","colab_type":"code","outputId":"baf67654-36b6-4cfa-acb0-14f247bd01f2","executionInfo":{"status":"ok","timestamp":1552315640056,"user_tz":0,"elapsed":2036,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":672}},"cell_type":"code","source":["#Create the features based on the balanced test set\n","testbal_features = convert_examples_to_features(testbal_examples, label_list, args[\"max_seq_length\"], tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/11/2019 14:47:18 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:18 - INFO - __main__ -   guid: 0\n","03/11/2019 14:47:18 - INFO - __main__ -   tokens: [CLS] and we ' re upset since the democrats would * never * try something as sneak ##y as this , right ? [SEP]\n","03/11/2019 14:47:18 - INFO - __main__ -   input_ids: 101 1998 2057 1005 2128 6314 2144 1996 8037 2052 1008 2196 1008 3046 2242 2004 13583 2100 2004 2023 1010 2157 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   label: 1 (id = 1)\n","03/11/2019 14:47:18 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:18 - INFO - __main__ -   guid: 1\n","03/11/2019 14:47:18 - INFO - __main__ -   tokens: [CLS] o ##oo ##h baby you caught me red handed creep ##in ' on the senate floor picture this we were ma ##kin ' up candidates being huge election whore ##s [SEP]\n","03/11/2019 14:47:18 - INFO - __main__ -   input_ids: 101 1051 9541 2232 3336 2017 3236 2033 2417 4375 19815 2378 1005 2006 1996 4001 2723 3861 2023 2057 2020 5003 4939 1005 2039 5347 2108 4121 2602 17219 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   label: 0 (id = 0)\n","03/11/2019 14:47:18 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:18 - INFO - __main__ -   guid: 2\n","03/11/2019 14:47:18 - INFO - __main__ -   tokens: [CLS] til , voluntary slavery by starving people who ' d work for food can ' potentially virtually ' wipe out unemployment [SEP]\n","03/11/2019 14:47:18 - INFO - __main__ -   input_ids: 101 18681 1010 10758 8864 2011 18025 2111 2040 1005 1040 2147 2005 2833 2064 1005 9280 8990 1005 13387 2041 12163 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   label: 1 (id = 1)\n","03/11/2019 14:47:18 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:18 - INFO - __main__ -   guid: 3\n","03/11/2019 14:47:18 - INFO - __main__ -   tokens: [CLS] welcome to china [SEP]\n","03/11/2019 14:47:18 - INFO - __main__ -   input_ids: 101 6160 2000 2859 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   label: 0 (id = 0)\n","03/11/2019 14:47:18 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:18 - INFO - __main__ -   guid: 4\n","03/11/2019 14:47:18 - INFO - __main__ -   tokens: [CLS] that ' d be great if only ron paul wasn ' t a christian ! [SEP]\n","03/11/2019 14:47:18 - INFO - __main__ -   input_ids: 101 2008 1005 1040 2022 2307 2065 2069 6902 2703 2347 1005 1056 1037 3017 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:18 - INFO - __main__ -   label: 1 (id = 1)\n"],"name":"stderr"}]},{"metadata":{"id":"UnsewRYpDLdQ","colab_type":"code","outputId":"a6edd0df-4999-4b4d-c969-de9942329dcf","executionInfo":{"status":"ok","timestamp":1552315662689,"user_tz":0,"elapsed":22658,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":672}},"cell_type":"code","source":["#Create the features based on the unbalance test set\n","testunbal_features = convert_examples_to_features(testunbal_examples, label_list, args[\"max_seq_length\"], tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/11/2019 14:47:20 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:20 - INFO - __main__ -   guid: 0\n","03/11/2019 14:47:20 - INFO - __main__ -   tokens: [CLS] well yeah , but it ' ll work this time . [SEP]\n","03/11/2019 14:47:20 - INFO - __main__ -   input_ids: 101 2092 3398 1010 2021 2009 1005 2222 2147 2023 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   label: 1 (id = 1)\n","03/11/2019 14:47:20 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:20 - INFO - __main__ -   guid: 1\n","03/11/2019 14:47:20 - INFO - __main__ -   tokens: [CLS] o ##oo ##h baby you caught me red handed creep ##in ' on the senate floor picture this we were ma ##kin ' up candidates being huge election whore ##s [SEP]\n","03/11/2019 14:47:20 - INFO - __main__ -   input_ids: 101 1051 9541 2232 3336 2017 3236 2033 2417 4375 19815 2378 1005 2006 1996 4001 2723 3861 2023 2057 2020 5003 4939 1005 2039 5347 2108 4121 2602 17219 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   label: 0 (id = 0)\n","03/11/2019 14:47:20 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:20 - INFO - __main__ -   guid: 2\n","03/11/2019 14:47:20 - INFO - __main__ -   tokens: [CLS] just like they do on the national level too . [SEP]\n","03/11/2019 14:47:20 - INFO - __main__ -   input_ids: 101 2074 2066 2027 2079 2006 1996 2120 2504 2205 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   label: 0 (id = 0)\n","03/11/2019 14:47:20 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:20 - INFO - __main__ -   guid: 3\n","03/11/2019 14:47:20 - INFO - __main__ -   tokens: [CLS] this is done all the time that ' s why that one black guy in sc carolina ran , state republican party gets a bus full of black men to register early to make fools of dem ##s and guarantee their guy wins . [SEP]\n","03/11/2019 14:47:20 - INFO - __main__ -   input_ids: 101 2023 2003 2589 2035 1996 2051 2008 1005 1055 2339 2008 2028 2304 3124 1999 8040 3792 2743 1010 2110 3951 2283 4152 1037 3902 2440 1997 2304 2273 2000 4236 2220 2000 2191 18656 1997 17183 2015 1998 11302 2037 3124 5222 1012 102 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   label: 0 (id = 0)\n","03/11/2019 14:47:20 - INFO - __main__ -   *** Example ***\n","03/11/2019 14:47:20 - INFO - __main__ -   guid: 4\n","03/11/2019 14:47:20 - INFO - __main__ -   tokens: [CLS] this is messed up and this tape is not br ##eit ##bar ##ted . [SEP]\n","03/11/2019 14:47:20 - INFO - __main__ -   input_ids: 101 2023 2003 18358 2039 1998 2023 6823 2003 2025 7987 20175 8237 3064 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 14:47:20 - INFO - __main__ -   label: 0 (id = 0)\n"],"name":"stderr"}]},{"metadata":{"id":"oSCY3DXCpeXD","colab_type":"text"},"cell_type":"markdown","source":["## Define function to train the model"]},{"metadata":{"id":"wGF6rzY1V7Pm","colab_type":"text"},"cell_type":"markdown","source":["This function is based on the training function from Trivedi (2019)"]},{"metadata":{"id":"i4keKOqwOEvb","colab_type":"code","colab":{}},"cell_type":"code","source":["def train():\n","\n","#Set up PyTorch options\n","\n","  num_train_optimization_steps = None\n","  if args[\"do_train\"]:\n","      num_train_optimization_steps = int(\n","          len(train_examples) / args['train_batch_size'] / args['gradient_accumulation_steps']) * args['num_train_epochs']\n","      if args[\"local_rank\"] != -1:\n","          num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n","  num_train_steps = int(\n","          len(train_examples) / args['train_batch_size'] / args['gradient_accumulation_steps'] * args['num_train_epochs'])\n","\n","#  if args[\"local_rank\"] == -1 or args[\"no_cuda\"]:\n","#      device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"cpu\")\n","#      n_gpu = torch.cuda.device_count()\n","#  #     n_gpu = 1\n","#  else:\n","#      torch.cuda.set_device(args['local_rank'])\n","#      device = torch.device(\"cuda\", args['local_rank'])\n","#      n_gpu = 1\n","#      # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","#      torch.distributed.init_process_group(backend='nccl')\n","#  logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n","#          device, n_gpu, bool(args['local_rank'] != -1), args['fp16']))\n","\n","  #Instantiate the model\n","\n","  model = BertForSequenceClassification.from_pretrained(args[\"bert_model\"],\n","            num_labels = num_labels)\n","  if args[\"fp16\"]:\n","      model.half()\n","  model.to(device)\n","  if args[\"local_rank\"] != -1:\n","      try:\n","          from apex.parallel import DistributedDataParallel as DDP\n","      except ImportError:\n","          raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n","\n","      model = DDP(model)\n","  elif n_gpu > 1:\n","      model = torch.nn.DataParallel(model)\n","\n","\n","  #Instantiate the optimizer\n","  param_optimizer = list(model.named_parameters())\n","  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","  optimizer_grouped_parameters = [\n","      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","      ]\n","  if args[\"fp16\"]:\n","      try:\n","          from apex.optimizers import FP16_Optimizer\n","          from apex.optimizers import FusedAdam\n","      except ImportError:\n","          raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n","\n","      optimizer = FusedAdam(optimizer_grouped_parameters,\n","                            lr=args[\"learning_rate\"],\n","                            bias_correction=False,\n","                            max_grad_norm=1.0)\n","      if args[\"loss_scale\"]== 0:\n","          optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n","      else:\n","          optimizer = FP16_Optimizer(optimizer, static_loss_scale=args[\"loss_scale\"])\n","\n","  else:\n","      optimizer = BertAdam(optimizer_grouped_parameters,\n","                           lr=args[\"learning_rate\"],\n","                           warmup=args[\"warmup_proportion\"],\n","                           t_total=num_train_optimization_steps)\n","\n","\n","  #Instantiate the PyTorch datasets and print key details\n","  logger.info(\"  Num examples = %d\", len(train_examples))\n","  logger.info(\"  Batch size = %d\", args['train_batch_size'])\n","  logger.info(\"  Num steps = %d\", num_train_steps)\n","  all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n","  all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n","  all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n","  all_label_ids = torch.tensor([f.label_ids for f in train_features], dtype=torch.long)\n","  train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","  if args['local_rank'] == -1:\n","      train_sampler = RandomSampler(train_data)\n","  else:\n","      train_sampler = DistributedSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args['train_batch_size'])\n","\n","  resultsdf, batch_losses = fit(model = model, train_dataloader = train_dataloader, device = device, optimizer = optimizer, num_epochs = args[\"num_train_epochs\"])\n","  \n","  return model, resultsdf, batch_losses"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hESTSfY1Cjuj","colab_type":"text"},"cell_type":"markdown","source":["## 100% project training set"]},{"metadata":{"id":"pYunrHAfCnte","colab_type":"text"},"cell_type":"markdown","source":["Best hyperparameters found in cross validation were batch size = 32, learning rate = 3e-5, training for 2 epochs. "]},{"metadata":{"id":"n8iRbLpmEA9a","colab_type":"code","colab":{}},"cell_type":"code","source":["#Set the hyperparamters\n","args[\"learning_rate\"] = 3e-5\n","args[\"train_batch_size\"] = 32\n","args[\"num_train_epochs\"] = 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dX2Z130_Gwq1","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"T_nYCOcsC4H4","colab_type":"text"},"cell_type":"markdown","source":["### Load in and process the training data"]},{"metadata":{"id":"5cjH-LKlCjQY","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load in the required training sets\n","traindf_100 = pd.read_csv(SARC_POL+'project_data/project_training_100.csv', index_col=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TTBNzT8lC_z7","colab_type":"code","colab":{}},"cell_type":"code","source":["#Process the training examples\n","train_examples = []\n","\n","for i in range(0,len(traindf_100.index)):\n","        train_examples.append(InputExample(str(i), traindf_100.loc[i,'response'], None, str(traindf_100.loc[i,'label'])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lhtt0p_cDHCw","colab_type":"code","outputId":"79ea06be-5220-4194-be81-91b4602ed78e","executionInfo":{"status":"ok","timestamp":1551442448698,"user_tz":0,"elapsed":4178,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":692}},"cell_type":"code","source":["#Create the features based on the training set\n","train_features = convert_examples_to_features(train_examples, label_list, args[\"max_seq_length\"], tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/01/2019 12:14:04 - INFO - __main__ -   *** Example ***\n","03/01/2019 12:14:04 - INFO - __main__ -   guid: 0\n","03/01/2019 12:14:04 - INFO - __main__ -   tokens: [CLS] donald did say he was interested in going to mars , didn ' t he ? [SEP]\n","03/01/2019 12:14:04 - INFO - __main__ -   input_ids: 101 6221 2106 2360 2002 2001 4699 1999 2183 2000 7733 1010 2134 1005 1056 2002 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   label: 0 (id = 0)\n","03/01/2019 12:14:04 - INFO - __main__ -   *** Example ***\n","03/01/2019 12:14:04 - INFO - __main__ -   guid: 1\n","03/01/2019 12:14:04 - INFO - __main__ -   tokens: [CLS] so you ' re saying it ' s possible . [SEP]\n","03/01/2019 12:14:04 - INFO - __main__ -   input_ids: 101 2061 2017 1005 2128 3038 2009 1005 1055 2825 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   label: 1 (id = 1)\n","03/01/2019 12:14:04 - INFO - __main__ -   *** Example ***\n","03/01/2019 12:14:04 - INFO - __main__ -   guid: 2\n","03/01/2019 12:14:04 - INFO - __main__ -   tokens: [CLS] not surprised they ' re disappearing when they ' re all hanging around american schools . [SEP]\n","03/01/2019 12:14:04 - INFO - __main__ -   input_ids: 101 2025 4527 2027 1005 2128 14489 2043 2027 1005 2128 2035 5689 2105 2137 2816 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   label: 1 (id = 1)\n","03/01/2019 12:14:04 - INFO - __main__ -   *** Example ***\n","03/01/2019 12:14:04 - INFO - __main__ -   guid: 3\n","03/01/2019 12:14:04 - INFO - __main__ -   tokens: [CLS] another reason to oppose dev ##os . [SEP]\n","03/01/2019 12:14:04 - INFO - __main__ -   input_ids: 101 2178 3114 2000 15391 16475 2891 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   label: 0 (id = 0)\n","03/01/2019 12:14:04 - INFO - __main__ -   *** Example ***\n","03/01/2019 12:14:04 - INFO - __main__ -   guid: 4\n","03/01/2019 12:14:04 - INFO - __main__ -   tokens: [CLS] this is absolutely hilarious but clearly false and fake since it reflects negatively on the don . . . lo ##l . [SEP]\n","03/01/2019 12:14:04 - INFO - __main__ -   input_ids: 101 2023 2003 7078 26316 2021 4415 6270 1998 8275 2144 2009 11138 19762 2006 1996 2123 1012 1012 1012 8840 2140 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 12:14:04 - INFO - __main__ -   label: 0 (id = 0)\n"],"name":"stderr"}]},{"metadata":{"id":"J1uWfjkqnHSr","colab_type":"text"},"cell_type":"markdown","source":["### Train the model, save the models and results"]},{"metadata":{"id":"ehq5rDrbdlxT","colab_type":"code","outputId":"371b1621-09fa-4057-ae3e-f5752531d326","executionInfo":{"status":"ok","timestamp":1551466153383,"user_tz":0,"elapsed":4514300,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":4202}},"cell_type":"code","source":["#Not explicitly setting seed, each run because each lopp will generate a different random number from the seed set in args\n","for n in range(0,5):\n","  model, resultsdf, batch_losses = train()\n","  \n","  with open(SARC_POL+f\"models/100_{n}.pickle\", 'wb') as handle:\n","    pickle.dump(model, handle)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/01/2019 17:34:00 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 17:34:00 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp58d106kx\n","03/01/2019 17:34:04 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 17:34:09 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 17:34:09 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 17:34:09 - INFO - __main__ -     Num examples = 10934\n","03/01/2019 17:34:09 - INFO - __main__ -     Batch size = 32\n","03/01/2019 17:34:09 - INFO - __main__ -     Num steps = 683\n","03/01/2019 17:34:10 - INFO - __main__ -   Loss on batch 0: 0.672766387462616\n","03/01/2019 17:34:45 - INFO - __main__ -   Loss on batch 50: 0.703899085521698\n","03/01/2019 17:35:20 - INFO - __main__ -   Loss on batch 100: 0.7062108516693115\n","03/01/2019 17:35:56 - INFO - __main__ -   Loss on batch 150: 0.6640022397041321\n","03/01/2019 17:36:31 - INFO - __main__ -   Loss on batch 200: 0.5339528918266296\n","03/01/2019 17:37:06 - INFO - __main__ -   Loss on batch 250: 0.5699236989021301\n","03/01/2019 17:37:42 - INFO - __main__ -   Loss on batch 300: 0.5598949790000916\n","03/01/2019 17:38:11 - INFO - __main__ -   Training loss after epoch 0.6087494188407708\n","03/01/2019 17:40:36 - INFO - __main__ -   Training accuracy after epoch 0.8162612035851472\n","03/01/2019 17:40:36 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 17:40:36 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 17:41:12 - INFO - __main__ -   {'loss': 0.5422636783400248, 'accuracy': 0.7278712509144111}\n","03/01/2019 17:41:12 - INFO - __main__ -   Loss on batch 0: 0.34774336218833923\n","03/01/2019 17:41:47 - INFO - __main__ -   Loss on batch 50: 0.18389320373535156\n","03/01/2019 17:42:22 - INFO - __main__ -   Loss on batch 100: 0.39899736642837524\n","03/01/2019 17:42:58 - INFO - __main__ -   Loss on batch 150: 0.4004588723182678\n","03/01/2019 17:43:33 - INFO - __main__ -   Loss on batch 200: 0.5864831805229187\n","03/01/2019 17:44:08 - INFO - __main__ -   Loss on batch 250: 0.2513495087623596\n","03/01/2019 17:44:43 - INFO - __main__ -   Loss on batch 300: 0.2507989704608917\n","03/01/2019 17:45:12 - INFO - __main__ -   Training loss after epoch 0.38690073722810076\n","03/01/2019 17:47:37 - INFO - __main__ -   Training accuracy after epoch 0.9213462593744284\n","03/01/2019 17:47:37 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 17:47:37 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 17:48:13 - INFO - __main__ -   {'loss': 0.5697771723187247, 'accuracy': 0.7333577176298464}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.810335        0.740752       0.742641\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 17:49:03 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 17:49:03 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmprltaf3vw\n","03/01/2019 17:49:07 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 17:49:12 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 17:49:12 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 17:49:12 - INFO - __main__ -     Num examples = 10934\n","03/01/2019 17:49:12 - INFO - __main__ -     Batch size = 32\n","03/01/2019 17:49:12 - INFO - __main__ -     Num steps = 683\n","03/01/2019 17:49:13 - INFO - __main__ -   Loss on batch 0: 0.7254418730735779\n","03/01/2019 17:49:48 - INFO - __main__ -   Loss on batch 50: 0.6725723743438721\n","03/01/2019 17:50:23 - INFO - __main__ -   Loss on batch 100: 0.5947771072387695\n","03/01/2019 17:50:59 - INFO - __main__ -   Loss on batch 150: 0.6054837703704834\n","03/01/2019 17:51:34 - INFO - __main__ -   Loss on batch 200: 0.551299512386322\n","03/01/2019 17:52:10 - INFO - __main__ -   Loss on batch 250: 0.5952452421188354\n","03/01/2019 17:52:45 - INFO - __main__ -   Loss on batch 300: 0.4549368917942047\n","03/01/2019 17:53:14 - INFO - __main__ -   Training loss after epoch 0.6192765599280073\n","03/01/2019 17:55:39 - INFO - __main__ -   Training accuracy after epoch 0.8097676970916408\n","03/01/2019 17:55:39 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 17:55:39 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 17:56:15 - INFO - __main__ -   {'loss': 0.5450590478126393, 'accuracy': 0.7289685442574981}\n","03/01/2019 17:56:16 - INFO - __main__ -   Loss on batch 0: 0.43320757150650024\n","03/01/2019 17:56:51 - INFO - __main__ -   Loss on batch 50: 0.49933600425720215\n","03/01/2019 17:57:26 - INFO - __main__ -   Loss on batch 100: 0.7122204303741455\n","03/01/2019 17:58:01 - INFO - __main__ -   Loss on batch 150: 0.3640863597393036\n","03/01/2019 17:58:36 - INFO - __main__ -   Loss on batch 200: 0.41048961877822876\n","03/01/2019 17:59:11 - INFO - __main__ -   Loss on batch 250: 0.3140200674533844\n","03/01/2019 17:59:46 - INFO - __main__ -   Loss on batch 300: 0.4124203324317932\n","03/01/2019 18:00:15 - INFO - __main__ -   Training loss after epoch 0.41690360349521305\n","03/01/2019 18:02:40 - INFO - __main__ -   Training accuracy after epoch 0.9125663069325041\n","03/01/2019 18:02:40 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 18:02:40 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 18:03:16 - INFO - __main__ -   {'loss': 0.5399843090495398, 'accuracy': 0.7377468910021946}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.810335        0.740752       0.742641\n","1        0.812096        0.733118       0.727436\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 18:04:05 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 18:04:05 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp2p38qats\n","03/01/2019 18:04:10 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 18:04:15 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 18:04:15 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 18:04:15 - INFO - __main__ -     Num examples = 10934\n","03/01/2019 18:04:15 - INFO - __main__ -     Batch size = 32\n","03/01/2019 18:04:15 - INFO - __main__ -     Num steps = 683\n","03/01/2019 18:04:15 - INFO - __main__ -   Loss on batch 0: 0.6390652656555176\n","03/01/2019 18:04:51 - INFO - __main__ -   Loss on batch 50: 0.6693255305290222\n","03/01/2019 18:05:26 - INFO - __main__ -   Loss on batch 100: 0.5595573782920837\n","03/01/2019 18:06:01 - INFO - __main__ -   Loss on batch 150: 0.5677676200866699\n","03/01/2019 18:06:37 - INFO - __main__ -   Loss on batch 200: 0.7062984704971313\n","03/01/2019 18:07:12 - INFO - __main__ -   Loss on batch 250: 0.5904119610786438\n","03/01/2019 18:07:47 - INFO - __main__ -   Loss on batch 300: 0.5797698497772217\n","03/01/2019 18:08:17 - INFO - __main__ -   Training loss after epoch 0.6004505563549131\n","03/01/2019 18:10:42 - INFO - __main__ -   Training accuracy after epoch 0.7915675873422352\n","03/01/2019 18:10:42 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 18:10:42 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 18:11:18 - INFO - __main__ -   {'loss': 0.5740831913643105, 'accuracy': 0.6964155084125823}\n","03/01/2019 18:11:18 - INFO - __main__ -   Loss on batch 0: 0.5154169201850891\n","03/01/2019 18:11:53 - INFO - __main__ -   Loss on batch 50: 0.37348228693008423\n","03/01/2019 18:12:28 - INFO - __main__ -   Loss on batch 100: 0.5245339870452881\n","03/01/2019 18:13:03 - INFO - __main__ -   Loss on batch 150: 0.3181084990501404\n","03/01/2019 18:13:38 - INFO - __main__ -   Loss on batch 200: 0.3255578279495239\n","03/01/2019 18:14:13 - INFO - __main__ -   Loss on batch 250: 0.5350772738456726\n","03/01/2019 18:14:48 - INFO - __main__ -   Loss on batch 300: 0.215565025806427\n","03/01/2019 18:15:18 - INFO - __main__ -   Training loss after epoch 0.39204039668653445\n","03/01/2019 18:17:42 - INFO - __main__ -   Training accuracy after epoch 0.9236327053228461\n","03/01/2019 18:17:42 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 18:17:42 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 18:18:18 - INFO - __main__ -   {'loss': 0.5573362957599551, 'accuracy': 0.731528895391368}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.810335        0.740752       0.742641\n","1        0.812096        0.733118       0.727436\n","2        0.815032        0.736348       0.734947\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 18:19:07 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 18:19:07 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp36_w5l0e\n","03/01/2019 18:19:12 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 18:19:17 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 18:19:17 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 18:19:17 - INFO - __main__ -     Num examples = 10934\n","03/01/2019 18:19:17 - INFO - __main__ -     Batch size = 32\n","03/01/2019 18:19:17 - INFO - __main__ -     Num steps = 683\n","03/01/2019 18:19:17 - INFO - __main__ -   Loss on batch 0: 0.693205714225769\n","03/01/2019 18:19:53 - INFO - __main__ -   Loss on batch 50: 0.5580579042434692\n","03/01/2019 18:20:28 - INFO - __main__ -   Loss on batch 100: 0.6121017932891846\n","03/01/2019 18:21:03 - INFO - __main__ -   Loss on batch 150: 0.4688974916934967\n","03/01/2019 18:21:39 - INFO - __main__ -   Loss on batch 200: 0.547101616859436\n","03/01/2019 18:22:14 - INFO - __main__ -   Loss on batch 250: 0.4689711332321167\n","03/01/2019 18:22:49 - INFO - __main__ -   Loss on batch 300: 0.5232961177825928\n","03/01/2019 18:23:19 - INFO - __main__ -   Training loss after epoch 0.6008203580365543\n","03/01/2019 18:25:43 - INFO - __main__ -   Training accuracy after epoch 0.8240351198097677\n","03/01/2019 18:25:43 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 18:25:43 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 18:26:20 - INFO - __main__ -   {'loss': 0.5368166288664175, 'accuracy': 0.7351865398683248}\n","03/01/2019 18:26:20 - INFO - __main__ -   Loss on batch 0: 0.42438530921936035\n","03/01/2019 18:26:55 - INFO - __main__ -   Loss on batch 50: 0.361778199672699\n","03/01/2019 18:27:30 - INFO - __main__ -   Loss on batch 100: 0.42907246947288513\n","03/01/2019 18:28:05 - INFO - __main__ -   Loss on batch 150: 0.32421019673347473\n","03/01/2019 18:28:40 - INFO - __main__ -   Loss on batch 200: 0.32814398407936096\n","03/01/2019 18:29:15 - INFO - __main__ -   Loss on batch 250: 0.23150481283664703\n","03/01/2019 18:29:50 - INFO - __main__ -   Loss on batch 300: 0.3579596281051636\n","03/01/2019 18:30:19 - INFO - __main__ -   Training loss after epoch 0.37044828372043476\n","03/01/2019 18:32:44 - INFO - __main__ -   Training accuracy after epoch 0.9331443204682641\n","03/01/2019 18:32:44 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 18:32:44 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 18:33:20 - INFO - __main__ -   {'loss': 0.5673627125662427, 'accuracy': 0.7318946598390637}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.810335        0.740752       0.742641\n","1        0.812096        0.733118       0.727436\n","2        0.815032        0.736348       0.734947\n","3        0.806224        0.735467       0.732562\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 18:34:09 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 18:34:09 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpj1nmyr2p\n","03/01/2019 18:34:14 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 18:34:19 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 18:34:19 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 18:34:19 - INFO - __main__ -     Num examples = 10934\n","03/01/2019 18:34:19 - INFO - __main__ -     Batch size = 32\n","03/01/2019 18:34:19 - INFO - __main__ -     Num steps = 683\n","03/01/2019 18:34:20 - INFO - __main__ -   Loss on batch 0: 0.6883023381233215\n","03/01/2019 18:34:55 - INFO - __main__ -   Loss on batch 50: 0.5805556774139404\n","03/01/2019 18:35:30 - INFO - __main__ -   Loss on batch 100: 0.5656372308731079\n","03/01/2019 18:36:06 - INFO - __main__ -   Loss on batch 150: 0.5491476058959961\n","03/01/2019 18:36:41 - INFO - __main__ -   Loss on batch 200: 0.6038985848426819\n","03/01/2019 18:37:16 - INFO - __main__ -   Loss on batch 250: 0.4610523581504822\n","03/01/2019 18:37:52 - INFO - __main__ -   Loss on batch 300: 0.7024189233779907\n","03/01/2019 18:38:21 - INFO - __main__ -   Training loss after epoch 0.606028501750433\n","03/01/2019 18:40:46 - INFO - __main__ -   Training accuracy after epoch 0.8134260106091092\n","03/01/2019 18:40:46 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 18:40:46 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 18:41:22 - INFO - __main__ -   {'loss': 0.5489738344453102, 'accuracy': 0.7275054864667154}\n","03/01/2019 18:41:22 - INFO - __main__ -   Loss on batch 0: 0.558925986289978\n","03/01/2019 18:41:57 - INFO - __main__ -   Loss on batch 50: 0.34473562240600586\n","03/01/2019 18:42:32 - INFO - __main__ -   Loss on batch 100: 0.5138324499130249\n","03/01/2019 18:43:08 - INFO - __main__ -   Loss on batch 150: 0.43890613317489624\n","03/01/2019 18:43:43 - INFO - __main__ -   Loss on batch 200: 0.4479072093963623\n","03/01/2019 18:44:18 - INFO - __main__ -   Loss on batch 250: 0.2699219286441803\n","03/01/2019 18:44:53 - INFO - __main__ -   Loss on batch 300: 0.2701198160648346\n","03/01/2019 18:45:22 - INFO - __main__ -   Training loss after epoch 0.3931622489938262\n","03/01/2019 18:47:46 - INFO - __main__ -   Training accuracy after epoch 0.9236327053228461\n","03/01/2019 18:47:46 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 18:47:46 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 18:48:23 - INFO - __main__ -   {'loss': 0.5651976331028827, 'accuracy': 0.7362838332114119}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.810335        0.740752       0.742641\n","1        0.812096        0.733118       0.727436\n","2        0.815032        0.736348       0.734947\n","3        0.806224        0.735467       0.732562\n","4        0.820317        0.739577       0.730804\n"],"name":"stdout"}]},{"metadata":{"id":"wRuAeT8zWpZT","colab_type":"code","colab":{}},"cell_type":"code","source":["record_100 = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GLA1sYFALYLp","colab_type":"code","outputId":"680763b2-f033-4e94-c1cd-06129eb7372d","executionInfo":{"status":"ok","timestamp":1552317556692,"user_tz":0,"elapsed":230089,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["for n in range(0,5):\n","  \n","  with open(SARC_POL+f\"models/100_{n}.pickle\", 'rb') as handle:\n","    model = pickle.load(handle)\n","\n","  #Evaluate on the balanced test set\n","  tupb = eval(testbal_examples, testbal_features, model, device)\n","  bal_test_balacc = balanced_accuracy(softmax(tupb[1]), tupb[2])\n","  bal_test_acc = tupb[0]['accuracy']\n","  bal_test_F1 = f1_score(tupb[2], np.argmax(tupb[1], axis=1), labels=None, pos_label=1, average='binary', sample_weight=None)\n","\n","  #Store the results\n","  record_100 = record_100.append({'Acc (bal, bal)': bal_test_balacc, 'Acc (bal, reg)': bal_test_acc, 'F1 (bal, reg)': bal_test_F1}, ignore_index=True)\n","\n","print(record_100)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.812096        0.740752       0.742641\n","1        0.816207        0.733118       0.727436\n","2        0.816794        0.736348       0.734947\n","3        0.808573        0.735467       0.732562\n","4        0.818555        0.739577       0.730804\n"],"name":"stdout"}]},{"metadata":{"id":"eQPXx_Yhmjd7","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test100.pickle\", 'wb') as handle:\n","  pickle.dump(record_100, handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VqoUXW4dnOHV","colab_type":"text"},"cell_type":"markdown","source":["## 50% project training set"]},{"metadata":{"id":"ILmCPQTqn_Rm","colab_type":"text"},"cell_type":"markdown","source":["Best hyperparameters found in cross validation were batch size = 32, learning rate = 2e-5, training for 2 epochs. "]},{"metadata":{"id":"nZw7XF18nVfA","colab_type":"code","colab":{}},"cell_type":"code","source":["#Set the hyperparamters\n","args[\"learning_rate\"] = 2e-5\n","args[\"train_batch_size\"] = 32\n","args[\"num_train_epochs\"] = 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FRNEt7NsnZxp","colab_type":"text"},"cell_type":"markdown","source":["### Load in and process the training data"]},{"metadata":{"id":"9MD91WwUnVZ5","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load in the required training sets\n","traindf_50 = pd.read_csv(SARC_POL+'project_data/project_training_50.csv', index_col=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZYtWJlLEnVYA","colab_type":"code","colab":{}},"cell_type":"code","source":["#Process the training examples\n","train_examples = []\n","\n","for i in range(0,len(traindf_50.index)):\n","        train_examples.append(InputExample(str(i), traindf_50.loc[i,'response'], None, str(traindf_50.loc[i,'label'])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ux4_VwQQnVU7","colab_type":"code","outputId":"49c04518-b676-4e4a-99cf-4cc28dfa2457","executionInfo":{"status":"ok","timestamp":1551466775476,"user_tz":0,"elapsed":2749,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":672}},"cell_type":"code","source":["#Create the features based on the training set\n","train_features = convert_examples_to_features(train_examples, label_list, args[\"max_seq_length\"], tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/01/2019 18:59:33 - INFO - __main__ -   *** Example ***\n","03/01/2019 18:59:33 - INFO - __main__ -   guid: 0\n","03/01/2019 18:59:33 - INFO - __main__ -   tokens: [CLS] i bet it ' s alabama or something . . . click link nailed it ! [SEP]\n","03/01/2019 18:59:33 - INFO - __main__ -   input_ids: 101 1045 6655 2009 1005 1055 6041 2030 2242 1012 1012 1012 11562 4957 26304 2009 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   label: 0 (id = 0)\n","03/01/2019 18:59:33 - INFO - __main__ -   *** Example ***\n","03/01/2019 18:59:33 - INFO - __main__ -   guid: 1\n","03/01/2019 18:59:33 - INFO - __main__ -   tokens: [CLS] and this is the exact reason why we should have a smaller federal government and larger state governments [SEP]\n","03/01/2019 18:59:33 - INFO - __main__ -   input_ids: 101 1998 2023 2003 1996 6635 3114 2339 2057 2323 2031 1037 3760 2976 2231 1998 3469 2110 6867 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   label: 1 (id = 1)\n","03/01/2019 18:59:33 - INFO - __main__ -   *** Example ***\n","03/01/2019 18:59:33 - INFO - __main__ -   guid: 2\n","03/01/2019 18:59:33 - INFO - __main__ -   tokens: [CLS] it ' s a good thing we saved ourselves from the secret republican , hillary clinton ! [SEP]\n","03/01/2019 18:59:33 - INFO - __main__ -   input_ids: 101 2009 1005 1055 1037 2204 2518 2057 5552 9731 2013 1996 3595 3951 1010 18520 7207 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   label: 0 (id = 0)\n","03/01/2019 18:59:33 - INFO - __main__ -   *** Example ***\n","03/01/2019 18:59:33 - INFO - __main__ -   guid: 3\n","03/01/2019 18:59:33 - INFO - __main__ -   tokens: [CLS] donald trump , already destroying civil rights before he even has the power to do so ! [SEP]\n","03/01/2019 18:59:33 - INFO - __main__ -   input_ids: 101 6221 8398 1010 2525 9846 2942 2916 2077 2002 2130 2038 1996 2373 2000 2079 2061 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   label: 1 (id = 1)\n","03/01/2019 18:59:33 - INFO - __main__ -   *** Example ***\n","03/01/2019 18:59:33 - INFO - __main__ -   guid: 4\n","03/01/2019 18:59:33 - INFO - __main__ -   tokens: [CLS] dead ##beat donald wouldn ' t have to do his own dancing if he paid the \" usa freedom kids \" what he owed . [SEP]\n","03/01/2019 18:59:33 - INFO - __main__ -   input_ids: 101 2757 19442 6221 2876 1005 1056 2031 2000 2079 2010 2219 5613 2065 2002 3825 1996 1000 3915 4071 4268 1000 2054 2002 12232 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 18:59:33 - INFO - __main__ -   label: 0 (id = 0)\n"],"name":"stderr"}]},{"metadata":{"id":"qpbCaeZUnm6x","colab_type":"text"},"cell_type":"markdown","source":["### Train the model, save the models and results"]},{"metadata":{"id":"kx0EZLCEnVIm","colab_type":"code","outputId":"4a26890f-1db1-4796-e391-266089a373ef","executionInfo":{"status":"ok","timestamp":1551469373024,"user_tz":0,"elapsed":2584192,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":3692}},"cell_type":"code","source":["#Not explicitly setting seed, each run because each lopp will generate a different random number from the seed set in args\n","for n in range(0,5):\n","  model, resultsdf, batch_losses = train()\n","  \n","  with open(SARC_POL+f\"models/50_{n}.pickle\", 'wb') as handle:\n","    pickle.dump(model, handle)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/01/2019 18:59:49 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 18:59:49 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpg6_c5dcd\n","03/01/2019 18:59:54 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 18:59:59 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 18:59:59 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 18:59:59 - INFO - __main__ -     Num examples = 5466\n","03/01/2019 18:59:59 - INFO - __main__ -     Batch size = 32\n","03/01/2019 18:59:59 - INFO - __main__ -     Num steps = 341\n","03/01/2019 18:59:59 - INFO - __main__ -   Loss on batch 0: 0.7079049944877625\n","03/01/2019 19:00:34 - INFO - __main__ -   Loss on batch 50: 0.6471995711326599\n","03/01/2019 19:01:10 - INFO - __main__ -   Loss on batch 100: 0.7112643718719482\n","03/01/2019 19:01:45 - INFO - __main__ -   Loss on batch 150: 0.6511231660842896\n","03/01/2019 19:01:59 - INFO - __main__ -   Training loss after epoch 0.6793260722481019\n","03/01/2019 19:03:12 - INFO - __main__ -   Training accuracy after epoch 0.6915477497255763\n","03/01/2019 19:03:12 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:03:12 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 19:03:48 - INFO - __main__ -   {'loss': 0.656018499718156, 'accuracy': 0.6671543525969276}\n","03/01/2019 19:03:48 - INFO - __main__ -   Loss on batch 0: 0.5418087244033813\n","03/01/2019 19:04:23 - INFO - __main__ -   Loss on batch 50: 0.5973238348960876\n","03/01/2019 19:04:58 - INFO - __main__ -   Loss on batch 100: 0.5442720651626587\n","03/01/2019 19:05:33 - INFO - __main__ -   Loss on batch 150: 0.767108678817749\n","03/01/2019 19:05:47 - INFO - __main__ -   Training loss after epoch 0.566531606410679\n","03/01/2019 19:07:00 - INFO - __main__ -   Training accuracy after epoch 0.7914379802414928\n","03/01/2019 19:07:00 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:07:00 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 19:07:36 - INFO - __main__ -   {'loss': 0.5797167590191198, 'accuracy': 0.7088514996342355}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.749266        0.701703       0.678277\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 19:08:25 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 19:08:25 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp6eq5roel\n","03/01/2019 19:08:30 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 19:08:35 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 19:08:35 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 19:08:35 - INFO - __main__ -     Num examples = 5466\n","03/01/2019 19:08:35 - INFO - __main__ -     Batch size = 32\n","03/01/2019 19:08:35 - INFO - __main__ -     Num steps = 341\n","03/01/2019 19:08:35 - INFO - __main__ -   Loss on batch 0: 0.675032377243042\n","03/01/2019 19:09:11 - INFO - __main__ -   Loss on batch 50: 0.6416246891021729\n","03/01/2019 19:09:46 - INFO - __main__ -   Loss on batch 100: 0.6338894367218018\n","03/01/2019 19:10:21 - INFO - __main__ -   Loss on batch 150: 0.6180747747421265\n","03/01/2019 19:10:36 - INFO - __main__ -   Training loss after epoch 0.6331311405053612\n","03/01/2019 19:11:48 - INFO - __main__ -   Training accuracy after epoch 0.7925356750823271\n","03/01/2019 19:11:48 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:11:48 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 19:12:24 - INFO - __main__ -   {'loss': 0.5679405820231105, 'accuracy': 0.7084857351865399}\n","03/01/2019 19:12:24 - INFO - __main__ -   Loss on batch 0: 0.4481258690357208\n","03/01/2019 19:12:59 - INFO - __main__ -   Loss on batch 50: 0.4349796772003174\n","03/01/2019 19:13:34 - INFO - __main__ -   Loss on batch 100: 0.45782339572906494\n","03/01/2019 19:14:10 - INFO - __main__ -   Loss on batch 150: 0.44444867968559265\n","03/01/2019 19:14:24 - INFO - __main__ -   Training loss after epoch 0.4300532618984144\n","03/01/2019 19:15:36 - INFO - __main__ -   Training accuracy after epoch 0.9052323454079766\n","03/01/2019 19:15:36 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:15:36 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 19:16:13 - INFO - __main__ -   {'loss': 0.586876874053201, 'accuracy': 0.7242136064374542}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.749266        0.701703       0.678277\n","1        0.777452        0.723429       0.713329\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 19:17:01 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 19:17:01 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpjw95ao5f\n","03/01/2019 19:17:06 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 19:17:11 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 19:17:11 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 19:17:11 - INFO - __main__ -     Num examples = 5466\n","03/01/2019 19:17:11 - INFO - __main__ -     Batch size = 32\n","03/01/2019 19:17:11 - INFO - __main__ -     Num steps = 341\n","03/01/2019 19:17:11 - INFO - __main__ -   Loss on batch 0: 0.7697505950927734\n","03/01/2019 19:17:47 - INFO - __main__ -   Loss on batch 50: 0.615980863571167\n","03/01/2019 19:18:22 - INFO - __main__ -   Loss on batch 100: 0.5683155655860901\n","03/01/2019 19:18:57 - INFO - __main__ -   Loss on batch 150: 0.5804382562637329\n","03/01/2019 19:19:12 - INFO - __main__ -   Training loss after epoch 0.6381482466271049\n","03/01/2019 19:20:24 - INFO - __main__ -   Training accuracy after epoch 0.7872301500182949\n","03/01/2019 19:20:24 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:20:24 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 19:21:00 - INFO - __main__ -   {'loss': 0.5642562175905982, 'accuracy': 0.709583028529627}\n","03/01/2019 19:21:01 - INFO - __main__ -   Loss on batch 0: 0.49428385496139526\n","03/01/2019 19:21:36 - INFO - __main__ -   Loss on batch 50: 0.434513121843338\n","03/01/2019 19:22:11 - INFO - __main__ -   Loss on batch 100: 0.5406176447868347\n","03/01/2019 19:22:46 - INFO - __main__ -   Loss on batch 150: 0.40343722701072693\n","03/01/2019 19:23:00 - INFO - __main__ -   Training loss after epoch 0.4351816353393577\n","03/01/2019 19:24:12 - INFO - __main__ -   Training accuracy after epoch 0.8966337358214417\n","03/01/2019 19:24:12 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:24:12 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 19:24:49 - INFO - __main__ -   {'loss': 0.5718867855016575, 'accuracy': 0.7154352596927579}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.749266        0.701703       0.678277\n","1        0.777452        0.723429       0.713329\n","2        0.788608        0.710804       0.696830\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 19:25:38 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 19:25:38 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpl2lcl6t1\n","03/01/2019 19:25:43 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 19:25:47 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 19:25:47 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 19:25:48 - INFO - __main__ -     Num examples = 5466\n","03/01/2019 19:25:48 - INFO - __main__ -     Batch size = 32\n","03/01/2019 19:25:48 - INFO - __main__ -     Num steps = 341\n","03/01/2019 19:25:48 - INFO - __main__ -   Loss on batch 0: 0.695209264755249\n","03/01/2019 19:26:23 - INFO - __main__ -   Loss on batch 50: 0.6609693169593811\n","03/01/2019 19:26:59 - INFO - __main__ -   Loss on batch 100: 0.4621526598930359\n","03/01/2019 19:27:34 - INFO - __main__ -   Loss on batch 150: 0.6612864136695862\n","03/01/2019 19:27:48 - INFO - __main__ -   Training loss after epoch 0.6324493881554631\n","03/01/2019 19:29:01 - INFO - __main__ -   Training accuracy after epoch 0.796377607025247\n","03/01/2019 19:29:01 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:29:01 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 19:29:37 - INFO - __main__ -   {'loss': 0.5630433500506157, 'accuracy': 0.7132406730065838}\n","03/01/2019 19:29:37 - INFO - __main__ -   Loss on batch 0: 0.4967637360095978\n","03/01/2019 19:30:12 - INFO - __main__ -   Loss on batch 50: 0.5706250071525574\n","03/01/2019 19:30:47 - INFO - __main__ -   Loss on batch 100: 0.5606027841567993\n","03/01/2019 19:31:22 - INFO - __main__ -   Loss on batch 150: 0.30870288610458374\n","03/01/2019 19:31:37 - INFO - __main__ -   Training loss after epoch 0.4195282281490794\n","03/01/2019 19:32:49 - INFO - __main__ -   Training accuracy after epoch 0.907427735089645\n","03/01/2019 19:32:49 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:32:49 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 19:33:25 - INFO - __main__ -   {'loss': 0.5814798668373463, 'accuracy': 0.7260424286759327}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.749266        0.701703       0.678277\n","1        0.777452        0.723429       0.713329\n","2        0.788608        0.710804       0.696830\n","3        0.781562        0.719025       0.718612\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 19:34:15 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 19:34:15 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpikruwp80\n","03/01/2019 19:34:20 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 19:34:25 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 19:34:25 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 19:34:25 - INFO - __main__ -     Num examples = 5466\n","03/01/2019 19:34:25 - INFO - __main__ -     Batch size = 32\n","03/01/2019 19:34:25 - INFO - __main__ -     Num steps = 341\n","03/01/2019 19:34:25 - INFO - __main__ -   Loss on batch 0: 0.7411754131317139\n","03/01/2019 19:35:01 - INFO - __main__ -   Loss on batch 50: 0.6308631300926208\n","03/01/2019 19:35:36 - INFO - __main__ -   Loss on batch 100: 0.6315828561782837\n","03/01/2019 19:36:11 - INFO - __main__ -   Loss on batch 150: 0.6085035800933838\n","03/01/2019 19:36:26 - INFO - __main__ -   Training loss after epoch 0.6254082817083214\n","03/01/2019 19:37:38 - INFO - __main__ -   Training accuracy after epoch 0.8152213684595683\n","03/01/2019 19:37:38 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:37:38 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 19:38:14 - INFO - __main__ -   {'loss': 0.5646532964567805, 'accuracy': 0.7110460863204097}\n","03/01/2019 19:38:15 - INFO - __main__ -   Loss on batch 0: 0.39014625549316406\n","03/01/2019 19:38:50 - INFO - __main__ -   Loss on batch 50: 0.5610561370849609\n","03/01/2019 19:39:25 - INFO - __main__ -   Loss on batch 100: 0.3398039937019348\n","03/01/2019 19:40:00 - INFO - __main__ -   Loss on batch 150: 0.4272637367248535\n","03/01/2019 19:40:14 - INFO - __main__ -   Training loss after epoch 0.39511761287151026\n","03/01/2019 19:41:26 - INFO - __main__ -   Training accuracy after epoch 0.9173069886571533\n","03/01/2019 19:41:26 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:41:26 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 19:42:03 - INFO - __main__ -   {'loss': 0.6053664902614992, 'accuracy': 0.710680321872714}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.749266        0.701703       0.678277\n","1        0.777452        0.723429       0.713329\n","2        0.788608        0.710804       0.696830\n","3        0.781562        0.719025       0.718612\n","4        0.772167        0.716970       0.714962\n"],"name":"stdout"}]},{"metadata":{"id":"KDjCTqSnW9yx","colab_type":"code","colab":{}},"cell_type":"code","source":["record_50 = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i4Jx-d91Wv3q","colab_type":"code","outputId":"0b6f67da-3c48-42b6-dc6e-386864904ff8","executionInfo":{"status":"ok","timestamp":1552318201050,"user_tz":0,"elapsed":251165,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["for n in range(0,5):\n","  \n","  with open(SARC_POL+f\"models/50_{n}.pickle\", 'rb') as handle:\n","    model = pickle.load(handle)\n","\n","  #Evaluate on the balanced test set\n","  tupb = eval(testbal_examples, testbal_features, model, device)\n","  bal_test_balacc = balanced_accuracy(softmax(tupb[1]), tupb[2])\n","  bal_test_acc = tupb[0]['accuracy']\n","  bal_test_F1 = f1_score(tupb[2], np.argmax(tupb[1], axis=1), labels=None, pos_label=1, average='binary', sample_weight=None)\n","\n","  #Store the results\n","  record_50 = record_50.append({'Acc (bal, bal)': bal_test_balacc, 'Acc (bal, reg)': bal_test_acc, 'F1 (bal, reg)': bal_test_F1}, ignore_index=True)\n","\n","print(record_50)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.746330        0.701703       0.678277\n","1        0.780975        0.723429       0.713329\n","2        0.783911        0.710804       0.696830\n","3        0.786260        0.719025       0.718612\n","4        0.774516        0.716970       0.714962\n"],"name":"stdout"}]},{"metadata":{"id":"jvKN1k7Qyl6M","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test50.pickle\", 'wb') as handle:\n","  pickle.dump(record_50, handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sPwhHFD5y--Z","colab_type":"text"},"cell_type":"markdown","source":["## 25% project training set"]},{"metadata":{"id":"pEqt_MtwzC0l","colab_type":"text"},"cell_type":"markdown","source":["Best hyperparameters found in cross validation were batch size = 16, learning rate = 2e-5, training for 2 epochs. "]},{"metadata":{"id":"ulNxl6LHy44b","colab_type":"code","colab":{}},"cell_type":"code","source":["#Set the hyperparamters\n","args[\"learning_rate\"] = 2e-5\n","args[\"train_batch_size\"] = 16\n","args[\"num_train_epochs\"] = 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ydw-IePdzNnc","colab_type":"text"},"cell_type":"markdown","source":["### Load in and process the training data"]},{"metadata":{"id":"31eq0EktzKc7","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load in the required training sets\n","traindf_25 = pd.read_csv(SARC_POL+'project_data/project_training_25.csv', index_col=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZpHvDxflzKZT","colab_type":"code","colab":{}},"cell_type":"code","source":["#Process the training examples\n","train_examples = []\n","\n","for i in range(0,len(traindf_25.index)):\n","        train_examples.append(InputExample(str(i), traindf_25.loc[i,'response'], None, str(traindf_25.loc[i,'label'])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8N3q13jOzKXH","colab_type":"code","outputId":"0e973045-9614-4325-cfd1-5fa01ee65885","executionInfo":{"status":"ok","timestamp":1551469853735,"user_tz":0,"elapsed":2645,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":672}},"cell_type":"code","source":["#Create the features based on the training set\n","train_features = convert_examples_to_features(train_examples, label_list, args[\"max_seq_length\"], tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/01/2019 19:50:51 - INFO - __main__ -   *** Example ***\n","03/01/2019 19:50:51 - INFO - __main__ -   guid: 0\n","03/01/2019 19:50:51 - INFO - __main__ -   tokens: [CLS] watching the go ##p imp ##lo ##de , knowing it is personally responsible for this cluster ##fu ##ck , is absolutely delicious . [SEP]\n","03/01/2019 19:50:51 - INFO - __main__ -   input_ids: 101 3666 1996 2175 2361 17727 4135 3207 1010 4209 2009 2003 7714 3625 2005 2023 9324 11263 3600 1010 2003 7078 12090 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   label: 0 (id = 0)\n","03/01/2019 19:50:51 - INFO - __main__ -   *** Example ***\n","03/01/2019 19:50:51 - INFO - __main__ -   guid: 1\n","03/01/2019 19:50:51 - INFO - __main__ -   tokens: [CLS] oh no , go ##p , trump wasn ' t a mis ##ogy ##nist before , that ' s a new thing ! [SEP]\n","03/01/2019 19:50:51 - INFO - __main__ -   input_ids: 101 2821 2053 1010 2175 2361 1010 8398 2347 1005 1056 1037 28616 15707 26942 2077 1010 2008 1005 1055 1037 2047 2518 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   label: 1 (id = 1)\n","03/01/2019 19:50:51 - INFO - __main__ -   *** Example ***\n","03/01/2019 19:50:51 - INFO - __main__ -   guid: 2\n","03/01/2019 19:50:51 - INFO - __main__ -   tokens: [CLS] considering the timing and manning ' s connection to wi ##ki ##lea ##ks , maybe they ' re trying to influence ass ##ange by tor ##turing his acc ##omp ##lice . [SEP]\n","03/01/2019 19:50:51 - INFO - __main__ -   input_ids: 101 6195 1996 10984 1998 11956 1005 1055 4434 2000 15536 3211 19738 5705 1010 2672 2027 1005 2128 2667 2000 3747 4632 22043 2011 17153 16037 2010 16222 25377 13231 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   label: 0 (id = 0)\n","03/01/2019 19:50:51 - INFO - __main__ -   *** Example ***\n","03/01/2019 19:50:51 - INFO - __main__ -   guid: 3\n","03/01/2019 19:50:51 - INFO - __main__ -   tokens: [CLS] seems like compassionate and common punishment . . . not cruel or unusual at all . . . [SEP]\n","03/01/2019 19:50:51 - INFO - __main__ -   input_ids: 101 3849 2066 29353 1998 2691 7750 1012 1012 1012 2025 10311 2030 5866 2012 2035 1012 1012 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   label: 1 (id = 1)\n","03/01/2019 19:50:51 - INFO - __main__ -   *** Example ***\n","03/01/2019 19:50:51 - INFO - __main__ -   guid: 4\n","03/01/2019 19:50:51 - INFO - __main__ -   tokens: [CLS] yeah fuck the next generation ! [SEP]\n","03/01/2019 19:50:51 - INFO - __main__ -   input_ids: 101 3398 6616 1996 2279 4245 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 19:50:51 - INFO - __main__ -   label: 1 (id = 1)\n"],"name":"stderr"}]},{"metadata":{"id":"CM6IYGqrzYvp","colab_type":"text"},"cell_type":"markdown","source":["### Train the model, save the models and results"]},{"metadata":{"id":"d978LAXpzKTp","colab_type":"code","outputId":"b608ba8f-3a90-4960-aef2-7aa2569a15fe","executionInfo":{"status":"ok","timestamp":1551471665024,"user_tz":0,"elapsed":1799694,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":3692}},"cell_type":"code","source":["#Not explicitly setting seed, each run because each lopp will generate a different random number from the seed set in args\n","for n in range(0,5):\n","  model, resultsdf, batch_losses = train()\n","  \n","  with open(SARC_POL+f\"models/25_{n}.pickle\", 'wb') as handle:\n","    pickle.dump(model, handle)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/01/2019 19:51:06 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 19:51:06 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpj_1t_baw\n","03/01/2019 19:51:11 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 19:51:15 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 19:51:15 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 19:51:16 - INFO - __main__ -     Num examples = 2732\n","03/01/2019 19:51:16 - INFO - __main__ -     Batch size = 16\n","03/01/2019 19:51:16 - INFO - __main__ -     Num steps = 341\n","03/01/2019 19:51:16 - INFO - __main__ -   Loss on batch 0: 0.7596280574798584\n","03/01/2019 19:51:39 - INFO - __main__ -   Loss on batch 50: 0.7060744762420654\n","03/01/2019 19:52:01 - INFO - __main__ -   Loss on batch 100: 0.6639136672019958\n","03/01/2019 19:52:24 - INFO - __main__ -   Loss on batch 150: 0.46546509861946106\n","03/01/2019 19:52:34 - INFO - __main__ -   Training loss after epoch 0.6685943131209814\n","03/01/2019 19:53:10 - INFO - __main__ -   Training accuracy after epoch 0.7591508052708639\n","03/01/2019 19:53:10 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:53:10 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 19:53:46 - INFO - __main__ -   {'loss': 0.6100490266500518, 'accuracy': 0.6653255303584492}\n","03/01/2019 19:53:46 - INFO - __main__ -   Loss on batch 0: 0.7342947125434875\n","03/01/2019 19:54:08 - INFO - __main__ -   Loss on batch 50: 0.4463273882865906\n","03/01/2019 19:54:31 - INFO - __main__ -   Loss on batch 100: 0.38988497853279114\n","03/01/2019 19:54:54 - INFO - __main__ -   Loss on batch 150: 0.4132745862007141\n","03/01/2019 19:55:03 - INFO - __main__ -   Training loss after epoch 0.43158607821018374\n","03/01/2019 19:55:39 - INFO - __main__ -   Training accuracy after epoch 0.9234992679355783\n","03/01/2019 19:55:39 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:55:39 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 19:56:15 - INFO - __main__ -   {'loss': 0.6072177647851235, 'accuracy': 0.6982443306510607}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.763946        0.698767       0.684502\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 19:57:04 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 19:57:04 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmprr3tamu3\n","03/01/2019 19:57:09 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 19:57:14 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 19:57:14 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 19:57:14 - INFO - __main__ -     Num examples = 2732\n","03/01/2019 19:57:14 - INFO - __main__ -     Batch size = 16\n","03/01/2019 19:57:14 - INFO - __main__ -     Num steps = 341\n","03/01/2019 19:57:14 - INFO - __main__ -   Loss on batch 0: 0.6897497773170471\n","03/01/2019 19:57:37 - INFO - __main__ -   Loss on batch 50: 0.5044269561767578\n","03/01/2019 19:58:00 - INFO - __main__ -   Loss on batch 100: 0.6721419095993042\n","03/01/2019 19:58:23 - INFO - __main__ -   Loss on batch 150: 0.7288784980773926\n","03/01/2019 19:58:32 - INFO - __main__ -   Training loss after epoch 0.6494148467716417\n","03/01/2019 19:59:08 - INFO - __main__ -   Training accuracy after epoch 0.7642752562225475\n","03/01/2019 19:59:08 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 19:59:08 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 19:59:45 - INFO - __main__ -   {'loss': 0.5937985396662424, 'accuracy': 0.6982443306510607}\n","03/01/2019 19:59:45 - INFO - __main__ -   Loss on batch 0: 0.6842126846313477\n","03/01/2019 20:00:07 - INFO - __main__ -   Loss on batch 50: 0.364718496799469\n","03/01/2019 20:00:30 - INFO - __main__ -   Loss on batch 100: 0.2593478858470917\n","03/01/2019 20:00:53 - INFO - __main__ -   Loss on batch 150: 0.4898148477077484\n","03/01/2019 20:01:02 - INFO - __main__ -   Training loss after epoch 0.45898311432690647\n","03/01/2019 20:01:38 - INFO - __main__ -   Training accuracy after epoch 0.886896046852123\n","03/01/2019 20:01:38 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:01:38 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 20:02:14 - INFO - __main__ -   {'loss': 0.5865008439435515, 'accuracy': 0.7051938551572787}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.763946        0.698767       0.684502\n","1        0.762184        0.699060       0.670100\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 20:03:04 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 20:03:04 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmptn38x4g2\n","03/01/2019 20:03:08 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 20:03:13 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 20:03:13 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 20:03:13 - INFO - __main__ -     Num examples = 2732\n","03/01/2019 20:03:13 - INFO - __main__ -     Batch size = 16\n","03/01/2019 20:03:13 - INFO - __main__ -     Num steps = 341\n","03/01/2019 20:03:13 - INFO - __main__ -   Loss on batch 0: 0.6628037095069885\n","03/01/2019 20:03:36 - INFO - __main__ -   Loss on batch 50: 0.7932032346725464\n","03/01/2019 20:03:59 - INFO - __main__ -   Loss on batch 100: 0.6449647545814514\n","03/01/2019 20:04:22 - INFO - __main__ -   Loss on batch 150: 0.6641594171524048\n","03/01/2019 20:04:31 - INFO - __main__ -   Training loss after epoch 0.6958107021119859\n","03/01/2019 20:05:07 - INFO - __main__ -   Training accuracy after epoch 0.592606149341142\n","03/01/2019 20:05:07 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:05:07 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 20:05:43 - INFO - __main__ -   {'loss': 0.6693695762822794, 'accuracy': 0.5804681784930504}\n","03/01/2019 20:05:43 - INFO - __main__ -   Loss on batch 0: 0.6412487626075745\n","03/01/2019 20:06:06 - INFO - __main__ -   Loss on batch 50: 0.6514641046524048\n","03/01/2019 20:06:29 - INFO - __main__ -   Loss on batch 100: 0.6082141399383545\n","03/01/2019 20:06:52 - INFO - __main__ -   Loss on batch 150: 0.4832232892513275\n","03/01/2019 20:07:01 - INFO - __main__ -   Training loss after epoch 0.6373693521259821\n","03/01/2019 20:07:37 - INFO - __main__ -   Training accuracy after epoch 0.6727672035139092\n","03/01/2019 20:07:37 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:07:37 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 20:08:13 - INFO - __main__ -   {'loss': 0.6483672995899998, 'accuracy': 0.6144842721287491}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.763946        0.698767       0.684502\n","1        0.762184        0.699060       0.670100\n","2        0.684087        0.622137       0.507085\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 20:09:02 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 20:09:02 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp1f3e1v52\n","03/01/2019 20:09:07 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 20:09:12 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 20:09:12 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 20:09:12 - INFO - __main__ -     Num examples = 2732\n","03/01/2019 20:09:12 - INFO - __main__ -     Batch size = 16\n","03/01/2019 20:09:12 - INFO - __main__ -     Num steps = 341\n","03/01/2019 20:09:12 - INFO - __main__ -   Loss on batch 0: 0.6381250619888306\n","03/01/2019 20:09:35 - INFO - __main__ -   Loss on batch 50: 0.5843510031700134\n","03/01/2019 20:09:58 - INFO - __main__ -   Loss on batch 100: 0.5699535608291626\n","03/01/2019 20:10:21 - INFO - __main__ -   Loss on batch 150: 0.5881538987159729\n","03/01/2019 20:10:30 - INFO - __main__ -   Training loss after epoch 0.6447010625872696\n","03/01/2019 20:11:06 - INFO - __main__ -   Training accuracy after epoch 0.787701317715959\n","03/01/2019 20:11:06 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:11:06 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 20:11:43 - INFO - __main__ -   {'loss': 0.5953229017728983, 'accuracy': 0.6865398683247989}\n","03/01/2019 20:11:43 - INFO - __main__ -   Loss on batch 0: 0.45669594407081604\n","03/01/2019 20:12:06 - INFO - __main__ -   Loss on batch 50: 0.46509119868278503\n","03/01/2019 20:12:28 - INFO - __main__ -   Loss on batch 100: 0.5873467922210693\n","03/01/2019 20:12:51 - INFO - __main__ -   Loss on batch 150: 0.5810237526893616\n","03/01/2019 20:13:01 - INFO - __main__ -   Training loss after epoch 0.43101638651382157\n","03/01/2019 20:13:37 - INFO - __main__ -   Training accuracy after epoch 0.9191068814055637\n","03/01/2019 20:13:37 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:13:37 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 20:14:13 - INFO - __main__ -   {'loss': 0.6161606117736461, 'accuracy': 0.6920263350402341}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.763946        0.698767       0.684502\n","1        0.762184        0.699060       0.670100\n","2        0.684087        0.622137       0.507085\n","3        0.756312        0.691720       0.697057\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 20:15:03 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 20:15:03 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmprdddfv2n\n","03/01/2019 20:15:08 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 20:15:13 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 20:15:13 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 20:15:13 - INFO - __main__ -     Num examples = 2732\n","03/01/2019 20:15:13 - INFO - __main__ -     Batch size = 16\n","03/01/2019 20:15:13 - INFO - __main__ -     Num steps = 341\n","03/01/2019 20:15:13 - INFO - __main__ -   Loss on batch 0: 0.6512439846992493\n","03/01/2019 20:15:36 - INFO - __main__ -   Loss on batch 50: 0.7313960194587708\n","03/01/2019 20:15:59 - INFO - __main__ -   Loss on batch 100: 0.7153106927871704\n","03/01/2019 20:16:22 - INFO - __main__ -   Loss on batch 150: 0.7036690711975098\n","03/01/2019 20:16:32 - INFO - __main__ -   Training loss after epoch 0.6617551418075784\n","03/01/2019 20:17:08 - INFO - __main__ -   Training accuracy after epoch 0.7426793557833089\n","03/01/2019 20:17:08 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:17:08 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 20:17:44 - INFO - __main__ -   {'loss': 0.5943287721900052, 'accuracy': 0.6883686905632772}\n","03/01/2019 20:17:44 - INFO - __main__ -   Loss on batch 0: 0.44768041372299194\n","03/01/2019 20:18:07 - INFO - __main__ -   Loss on batch 50: 0.4389657974243164\n","03/01/2019 20:18:30 - INFO - __main__ -   Loss on batch 100: 0.7252281904220581\n","03/01/2019 20:18:52 - INFO - __main__ -   Loss on batch 150: 0.6646631360054016\n","03/01/2019 20:19:02 - INFO - __main__ -   Training loss after epoch 0.46479322687227126\n","03/01/2019 20:19:38 - INFO - __main__ -   Training accuracy after epoch 0.8898243045387995\n","03/01/2019 20:19:38 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:19:38 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 20:20:14 - INFO - __main__ -   {'loss': 0.6077383060094922, 'accuracy': 0.7000731528895391}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.763946        0.698767       0.684502\n","1        0.762184        0.699060       0.670100\n","2        0.684087        0.622137       0.507085\n","3        0.756312        0.691720       0.697057\n","4        0.755725        0.693776       0.688564\n"],"name":"stdout"}]},{"metadata":{"id":"iP6PzDOgXGT0","colab_type":"code","colab":{}},"cell_type":"code","source":["record_25 = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"79deH0r0XHPw","colab_type":"code","outputId":"3a5027ff-ff32-4fe3-d478-5755ff3ae98c","executionInfo":{"status":"ok","timestamp":1552319048314,"user_tz":0,"elapsed":246794,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["for n in range(0,5):\n","  \n","  with open(SARC_POL+f\"models/25_{n}.pickle\", 'rb') as handle:\n","    model = pickle.load(handle)\n","\n","  #Evaluate on the balanced test set\n","  tupb = eval(testbal_examples, testbal_features, model, device)\n","  bal_test_balacc = balanced_accuracy(softmax(tupb[1]), tupb[2])\n","  bal_test_acc = tupb[0]['accuracy']\n","  bal_test_F1 = f1_score(tupb[2], np.argmax(tupb[1], axis=1), labels=None, pos_label=1, average='binary', sample_weight=None)\n","  \n","  #Store the results\n","  record_25 = record_25.append({'Acc (bal, bal)': bal_test_balacc, 'Acc (bal, reg)': bal_test_acc, 'F1 (bal, reg)': bal_test_F1}, ignore_index=True)\n","  \n","print(record_25)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.763359        0.698767       0.684502\n","1        0.762772        0.699060       0.670100\n","2        0.678215        0.622137       0.507085\n","3        0.756900        0.691720       0.697057\n","4        0.753376        0.693776       0.688564\n"],"name":"stdout"}]},{"metadata":{"id":"daYwmJcyzKRe","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test25.pickle\", 'wb') as handle:\n","  pickle.dump(record_25, handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gq-bfPTd7M_S","colab_type":"text"},"cell_type":"markdown","source":["## 12.5% project training set"]},{"metadata":{"id":"W9415y7X7dhC","colab_type":"text"},"cell_type":"markdown","source":["Best hyperparameters found in cross validation were batch size = 16, learning rate = 2e-5, training for 5 epochs. "]},{"metadata":{"id":"W1z7C39HzKOH","colab_type":"code","colab":{}},"cell_type":"code","source":["#Set the hyperparamters\n","args[\"learning_rate\"] = 2e-5\n","args[\"train_batch_size\"] = 16\n","args[\"num_train_epochs\"] = 5"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mZbCpMCA7j6H","colab_type":"text"},"cell_type":"markdown","source":["### Load in and process the training data"]},{"metadata":{"id":"HZMW5dNu7fj9","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load in the required training sets\n","traindf_12 = pd.read_csv(SARC_POL+'project_data/project_training_12.csv', index_col=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Tpy-I1Fe7fgO","colab_type":"code","colab":{}},"cell_type":"code","source":["#Process the training examples\n","train_examples = []\n","\n","for i in range(0,len(traindf_12.index)):\n","        train_examples.append(InputExample(str(i), traindf_12.loc[i,'response'], None, str(traindf_12.loc[i,'label'])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xmw4z1vl7fd7","colab_type":"code","outputId":"a65f9541-affe-4f4e-e355-dbcbbd688e27","executionInfo":{"status":"ok","timestamp":1551472050484,"user_tz":0,"elapsed":1147,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":652}},"cell_type":"code","source":["#Create the features based on the training set\n","train_features = convert_examples_to_features(train_examples, label_list, args[\"max_seq_length\"], tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/01/2019 20:27:29 - INFO - __main__ -   *** Example ***\n","03/01/2019 20:27:29 - INFO - __main__ -   guid: 0\n","03/01/2019 20:27:29 - INFO - __main__ -   tokens: [CLS] or anyone that ' s ever had to make an appeal . [SEP]\n","03/01/2019 20:27:29 - INFO - __main__ -   input_ids: 101 2030 3087 2008 1005 1055 2412 2018 2000 2191 2019 5574 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   label: 0 (id = 0)\n","03/01/2019 20:27:29 - INFO - __main__ -   *** Example ***\n","03/01/2019 20:27:29 - INFO - __main__ -   guid: 1\n","03/01/2019 20:27:29 - INFO - __main__ -   tokens: [CLS] trump is the health ##iest president to ever take office , so he doesn ' t have to worry about that . [SEP]\n","03/01/2019 20:27:29 - INFO - __main__ -   input_ids: 101 8398 2003 1996 2740 10458 2343 2000 2412 2202 2436 1010 2061 2002 2987 1005 1056 2031 2000 4737 2055 2008 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   label: 1 (id = 1)\n","03/01/2019 20:27:29 - INFO - __main__ -   *** Example ***\n","03/01/2019 20:27:29 - INFO - __main__ -   guid: 2\n","03/01/2019 20:27:29 - INFO - __main__ -   tokens: [CLS] i love sanders and everything he believes in but i think his policy ' s won ' t make it into or out of congress . [SEP]\n","03/01/2019 20:27:29 - INFO - __main__ -   input_ids: 101 1045 2293 12055 1998 2673 2002 7164 1999 2021 1045 2228 2010 3343 1005 1055 2180 1005 1056 2191 2009 2046 2030 2041 1997 3519 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   label: 0 (id = 0)\n","03/01/2019 20:27:29 - INFO - __main__ -   *** Example ***\n","03/01/2019 20:27:29 - INFO - __main__ -   guid: 3\n","03/01/2019 20:27:29 - INFO - __main__ -   tokens: [CLS] then don ' t forget to phone ##bank canvas ##sing dona ##ting your rent money for this month to him and we can make him president but we don ' t vote so it didn ' t matter lo ##l . [SEP]\n","03/01/2019 20:27:29 - INFO - __main__ -   input_ids: 101 2059 2123 1005 1056 5293 2000 3042 9299 10683 7741 24260 3436 2115 9278 2769 2005 2023 3204 2000 2032 1998 2057 2064 2191 2032 2343 2021 2057 2123 1005 1056 3789 2061 2009 2134 1005 1056 3043 8840 2140 1012 102 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   label: 1 (id = 1)\n","03/01/2019 20:27:29 - INFO - __main__ -   *** Example ***\n","03/01/2019 20:27:29 - INFO - __main__ -   guid: 4\n","03/01/2019 20:27:29 - INFO - __main__ -   tokens: [CLS] totally not a perfect example of the racism inherent in the trump movement that makes ethnic minorities think that other candidates are a better path for america [SEP]\n","03/01/2019 20:27:29 - INFO - __main__ -   input_ids: 101 6135 2025 1037 3819 2742 1997 1996 14398 16112 1999 1996 8398 2929 2008 3084 5636 14302 2228 2008 2060 5347 2024 1037 2488 4130 2005 2637 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/01/2019 20:27:29 - INFO - __main__ -   label: 1 (id = 1)\n"],"name":"stderr"}]},{"metadata":{"id":"_U_-Pdp27uyE","colab_type":"text"},"cell_type":"markdown","source":["### Train the model, save the models and results"]},{"metadata":{"id":"MCpbyfzA7fbo","colab_type":"code","outputId":"913c87b8-fdda-4014-bb76-7770fce6b7e3","executionInfo":{"status":"ok","timestamp":1551474685809,"user_tz":0,"elapsed":2632623,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":5137}},"cell_type":"code","source":["#Not explicitly setting seed, each run because each lopp will generate a different random number from the seed set in args\n","for n in range(0,5):\n","  model, resultsdf, batch_losses = train()\n","  \n","  with open(SARC_POL+f\"models/12_{n}.pickle\", 'wb') as handle:\n","    pickle.dump(model, handle)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/01/2019 20:27:34 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 20:27:34 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp2eteo_zx\n","03/01/2019 20:27:39 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 20:27:43 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 20:27:43 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 20:27:44 - INFO - __main__ -     Num examples = 1366\n","03/01/2019 20:27:44 - INFO - __main__ -     Batch size = 16\n","03/01/2019 20:27:44 - INFO - __main__ -     Num steps = 426\n","03/01/2019 20:27:44 - INFO - __main__ -   Loss on batch 0: 0.7376357316970825\n","03/01/2019 20:28:07 - INFO - __main__ -   Loss on batch 50: 0.5769243240356445\n","03/01/2019 20:28:23 - INFO - __main__ -   Training loss after epoch 0.6661752250998519\n","03/01/2019 20:28:41 - INFO - __main__ -   Training accuracy after epoch 0.8228404099560761\n","03/01/2019 20:28:41 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:28:41 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 20:29:17 - INFO - __main__ -   {'loss': 0.6041128829468129, 'accuracy': 0.6905632772494513}\n","03/01/2019 20:29:17 - INFO - __main__ -   Loss on batch 0: 0.49297207593917847\n","03/01/2019 20:29:40 - INFO - __main__ -   Loss on batch 50: 0.366639643907547\n","03/01/2019 20:29:56 - INFO - __main__ -   Training loss after epoch 0.400271323271269\n","03/01/2019 20:30:14 - INFO - __main__ -   Training accuracy after epoch 0.9773060029282576\n","03/01/2019 20:30:14 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:30:14 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 20:30:51 - INFO - __main__ -   {'loss': 0.6692549769961557, 'accuracy': 0.6912948061448427}\n","03/01/2019 20:30:51 - INFO - __main__ -   Loss on batch 0: 0.07083840668201447\n","03/01/2019 20:31:13 - INFO - __main__ -   Loss on batch 50: 0.26796725392341614\n","03/01/2019 20:31:29 - INFO - __main__ -   Training loss after epoch 0.07693159948428009\n","03/01/2019 20:31:48 - INFO - __main__ -   Training accuracy after epoch 0.9970717423133236\n","03/01/2019 20:31:48 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:31:48 - INFO - __main__ -   Eval after epoch 3\n","03/01/2019 20:32:24 - INFO - __main__ -   {'loss': 1.0295391547125439, 'accuracy': 0.6591075347476225}\n","03/01/2019 20:32:24 - INFO - __main__ -   Loss on batch 0: 0.009214967489242554\n","03/01/2019 20:32:47 - INFO - __main__ -   Loss on batch 50: 0.005957894027233124\n","03/01/2019 20:33:03 - INFO - __main__ -   Training loss after epoch 0.013878351591830684\n","03/01/2019 20:33:21 - INFO - __main__ -   Training accuracy after epoch 0.9985358711566618\n","03/01/2019 20:33:21 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:33:21 - INFO - __main__ -   Eval after epoch 4\n","03/01/2019 20:33:57 - INFO - __main__ -   {'loss': 1.1166156987811244, 'accuracy': 0.6762984637893197}\n","03/01/2019 20:33:57 - INFO - __main__ -   Loss on batch 0: 0.006017424166202545\n","03/01/2019 20:34:20 - INFO - __main__ -   Loss on batch 50: 0.0041268616914749146\n","03/01/2019 20:34:36 - INFO - __main__ -   Training loss after epoch 0.006638799929384922\n","03/01/2019 20:34:54 - INFO - __main__ -   Training accuracy after epoch 0.9992679355783309\n","03/01/2019 20:34:54 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:34:54 - INFO - __main__ -   Eval after epoch 5\n","03/01/2019 20:35:30 - INFO - __main__ -   {'loss': 1.151357498972915, 'accuracy': 0.6766642282370153}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.738109        0.675573       0.664643\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 20:36:19 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 20:36:19 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpd9sxfwju\n","03/01/2019 20:36:24 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 20:36:29 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 20:36:29 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 20:36:29 - INFO - __main__ -     Num examples = 1366\n","03/01/2019 20:36:29 - INFO - __main__ -     Batch size = 16\n","03/01/2019 20:36:29 - INFO - __main__ -     Num steps = 426\n","03/01/2019 20:36:29 - INFO - __main__ -   Loss on batch 0: 0.7316524386405945\n","03/01/2019 20:36:52 - INFO - __main__ -   Loss on batch 50: 0.6723372340202332\n","03/01/2019 20:37:08 - INFO - __main__ -   Training loss after epoch 0.6911210109328114\n","03/01/2019 20:37:26 - INFO - __main__ -   Training accuracy after epoch 0.7313323572474377\n","03/01/2019 20:37:26 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:37:26 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 20:38:02 - INFO - __main__ -   {'loss': 0.6456526580244996, 'accuracy': 0.6700804681784931}\n","03/01/2019 20:38:03 - INFO - __main__ -   Loss on batch 0: 0.6275575160980225\n","03/01/2019 20:38:25 - INFO - __main__ -   Loss on batch 50: 0.43065783381462097\n","03/01/2019 20:38:41 - INFO - __main__ -   Training loss after epoch 0.5378660764112029\n","03/01/2019 20:39:00 - INFO - __main__ -   Training accuracy after epoch 0.95900439238653\n","03/01/2019 20:39:00 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:39:00 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 20:39:36 - INFO - __main__ -   {'loss': 0.6337662217921989, 'accuracy': 0.6916605705925384}\n","03/01/2019 20:39:36 - INFO - __main__ -   Loss on batch 0: 0.14925403892993927\n","03/01/2019 20:39:59 - INFO - __main__ -   Loss on batch 50: 0.012588687241077423\n","03/01/2019 20:40:15 - INFO - __main__ -   Training loss after epoch 0.12187455494909785\n","03/01/2019 20:40:33 - INFO - __main__ -   Training accuracy after epoch 0.9919472913616398\n","03/01/2019 20:40:33 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:40:33 - INFO - __main__ -   Eval after epoch 3\n","03/01/2019 20:41:09 - INFO - __main__ -   {'loss': 0.9633230934309405, 'accuracy': 0.6638624725676664}\n","03/01/2019 20:41:09 - INFO - __main__ -   Loss on batch 0: 0.008051648736000061\n","03/01/2019 20:41:32 - INFO - __main__ -   Loss on batch 50: 0.020099535584449768\n","03/01/2019 20:41:48 - INFO - __main__ -   Training loss after epoch 0.022264963283939936\n","03/01/2019 20:42:06 - INFO - __main__ -   Training accuracy after epoch 0.9978038067349927\n","03/01/2019 20:42:06 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:42:06 - INFO - __main__ -   Eval after epoch 4\n","03/01/2019 20:42:43 - INFO - __main__ -   {'loss': 1.1882981893628142, 'accuracy': 0.674835405998537}\n","03/01/2019 20:42:43 - INFO - __main__ -   Loss on batch 0: 0.003993779420852661\n","03/01/2019 20:43:05 - INFO - __main__ -   Loss on batch 50: 0.002334311604499817\n","03/01/2019 20:43:21 - INFO - __main__ -   Training loss after epoch 0.008813651408566985\n","03/01/2019 20:43:40 - INFO - __main__ -   Training accuracy after epoch 0.9992679355783309\n","03/01/2019 20:43:40 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:43:40 - INFO - __main__ -   Eval after epoch 5\n","03/01/2019 20:44:16 - INFO - __main__ -   {'loss': 1.1972756766995718, 'accuracy': 0.6810534016093636}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.738109        0.675573       0.664643\n","1        0.733412        0.679977       0.675595\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 20:45:05 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 20:45:05 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp2iii8uom\n","03/01/2019 20:45:10 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 20:45:15 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 20:45:15 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 20:45:15 - INFO - __main__ -     Num examples = 1366\n","03/01/2019 20:45:15 - INFO - __main__ -     Batch size = 16\n","03/01/2019 20:45:15 - INFO - __main__ -     Num steps = 426\n","03/01/2019 20:45:15 - INFO - __main__ -   Loss on batch 0: 0.7177366018295288\n","03/01/2019 20:45:38 - INFO - __main__ -   Loss on batch 50: 0.6478205323219299\n","03/01/2019 20:45:54 - INFO - __main__ -   Training loss after epoch 0.6756689181161482\n","03/01/2019 20:46:12 - INFO - __main__ -   Training accuracy after epoch 0.7986822840409956\n","03/01/2019 20:46:12 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:46:12 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 20:46:48 - INFO - __main__ -   {'loss': 0.621661368844121, 'accuracy': 0.6825164594001463}\n","03/01/2019 20:46:49 - INFO - __main__ -   Loss on batch 0: 0.495863676071167\n","03/01/2019 20:47:11 - INFO - __main__ -   Loss on batch 50: 0.5470331907272339\n","03/01/2019 20:47:28 - INFO - __main__ -   Training loss after epoch 0.4488861477652261\n","03/01/2019 20:47:46 - INFO - __main__ -   Training accuracy after epoch 0.9267935578330894\n","03/01/2019 20:47:46 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:47:46 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 20:48:22 - INFO - __main__ -   {'loss': 0.7578669796849407, 'accuracy': 0.6313094367227505}\n","03/01/2019 20:48:22 - INFO - __main__ -   Loss on batch 0: 0.27081289887428284\n","03/01/2019 20:48:45 - INFO - __main__ -   Loss on batch 50: 0.06820100545883179\n","03/01/2019 20:49:01 - INFO - __main__ -   Training loss after epoch 0.13782396797783847\n","03/01/2019 20:49:19 - INFO - __main__ -   Training accuracy after epoch 0.9904831625183016\n","03/01/2019 20:49:19 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:49:19 - INFO - __main__ -   Eval after epoch 3\n","03/01/2019 20:49:55 - INFO - __main__ -   {'loss': 0.939355804476627, 'accuracy': 0.6605705925384052}\n","03/01/2019 20:49:56 - INFO - __main__ -   Loss on batch 0: 0.026918578892946243\n","03/01/2019 20:50:18 - INFO - __main__ -   Loss on batch 50: 0.007610231637954712\n","03/01/2019 20:50:34 - INFO - __main__ -   Training loss after epoch 0.03336130145958863\n","03/01/2019 20:50:53 - INFO - __main__ -   Training accuracy after epoch 0.9963396778916545\n","03/01/2019 20:50:53 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:50:53 - INFO - __main__ -   Eval after epoch 4\n","03/01/2019 20:51:29 - INFO - __main__ -   {'loss': 1.1504886226598607, 'accuracy': 0.6671543525969276}\n","03/01/2019 20:51:29 - INFO - __main__ -   Loss on batch 0: 0.008007064461708069\n","03/01/2019 20:51:52 - INFO - __main__ -   Loss on batch 50: 0.006290838122367859\n","03/01/2019 20:52:08 - INFO - __main__ -   Training loss after epoch 0.01726168339400617\n","03/01/2019 20:52:26 - INFO - __main__ -   Training accuracy after epoch 0.9970717423133236\n","03/01/2019 20:52:26 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:52:26 - INFO - __main__ -   Eval after epoch 5\n","03/01/2019 20:53:02 - INFO - __main__ -   {'loss': 1.1949730397656906, 'accuracy': 0.6645940014630578}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.738109        0.675573       0.664643\n","1        0.733412        0.679977       0.675595\n","2        0.713447        0.663535       0.659537\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 20:53:51 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 20:53:51 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp8fo8u3al\n","03/01/2019 20:53:56 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 20:54:00 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 20:54:00 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 20:54:01 - INFO - __main__ -     Num examples = 1366\n","03/01/2019 20:54:01 - INFO - __main__ -     Batch size = 16\n","03/01/2019 20:54:01 - INFO - __main__ -     Num steps = 426\n","03/01/2019 20:54:01 - INFO - __main__ -   Loss on batch 0: 0.6538424491882324\n","03/01/2019 20:54:24 - INFO - __main__ -   Loss on batch 50: 0.7366175651550293\n","03/01/2019 20:54:40 - INFO - __main__ -   Training loss after epoch 0.6789960292882697\n","03/01/2019 20:54:58 - INFO - __main__ -   Training accuracy after epoch 0.7598828696925329\n","03/01/2019 20:54:58 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:54:58 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 20:55:35 - INFO - __main__ -   {'loss': 0.6321183255938596, 'accuracy': 0.6675201170446232}\n","03/01/2019 20:55:35 - INFO - __main__ -   Loss on batch 0: 0.5995318293571472\n","03/01/2019 20:55:57 - INFO - __main__ -   Loss on batch 50: 0.4045688509941101\n","03/01/2019 20:56:14 - INFO - __main__ -   Training loss after epoch 0.48142165257487185\n","03/01/2019 20:56:32 - INFO - __main__ -   Training accuracy after epoch 0.9465592972181552\n","03/01/2019 20:56:32 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:56:32 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 20:57:08 - INFO - __main__ -   {'loss': 0.6344978390044944, 'accuracy': 0.6722750548646672}\n","03/01/2019 20:57:08 - INFO - __main__ -   Loss on batch 0: 0.13607098162174225\n","03/01/2019 20:57:31 - INFO - __main__ -   Loss on batch 50: 0.018468670547008514\n","03/01/2019 20:57:47 - INFO - __main__ -   Training loss after epoch 0.12316457448570534\n","03/01/2019 20:58:05 - INFO - __main__ -   Training accuracy after epoch 0.9948755490483162\n","03/01/2019 20:58:05 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:58:05 - INFO - __main__ -   Eval after epoch 3\n","03/01/2019 20:58:41 - INFO - __main__ -   {'loss': 0.9694072829429493, 'accuracy': 0.6847110460863204}\n","03/01/2019 20:58:41 - INFO - __main__ -   Loss on batch 0: 0.023264318704605103\n","03/01/2019 20:59:04 - INFO - __main__ -   Loss on batch 50: 0.0049834102392196655\n","03/01/2019 20:59:20 - INFO - __main__ -   Training loss after epoch 0.01939097003534783\n","03/01/2019 20:59:38 - INFO - __main__ -   Training accuracy after epoch 0.9978038067349927\n","03/01/2019 20:59:38 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 20:59:38 - INFO - __main__ -   Eval after epoch 4\n","03/01/2019 21:00:15 - INFO - __main__ -   {'loss': 1.143878619338191, 'accuracy': 0.6741038771031456}\n","03/01/2019 21:00:15 - INFO - __main__ -   Loss on batch 0: 0.00389176607131958\n","03/01/2019 21:00:38 - INFO - __main__ -   Loss on batch 50: 0.0032852143049240112\n","03/01/2019 21:00:54 - INFO - __main__ -   Training loss after epoch 0.009330849244454226\n","03/01/2019 21:01:12 - INFO - __main__ -   Training accuracy after epoch 0.9985358711566618\n","03/01/2019 21:01:12 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 21:01:12 - INFO - __main__ -   Eval after epoch 5\n","03/01/2019 21:01:48 - INFO - __main__ -   {'loss': 1.1704468207303869, 'accuracy': 0.6744696415508412}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.738109        0.675573       0.664643\n","1        0.733412        0.679977       0.675595\n","2        0.713447        0.663535       0.659537\n","3        0.711098        0.668233       0.667060\n"],"name":"stdout"},{"output_type":"stream","text":["03/01/2019 21:02:38 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/01/2019 21:02:38 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp15_9gm34\n","03/01/2019 21:02:43 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/01/2019 21:02:48 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/01/2019 21:02:48 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/01/2019 21:02:48 - INFO - __main__ -     Num examples = 1366\n","03/01/2019 21:02:48 - INFO - __main__ -     Batch size = 16\n","03/01/2019 21:02:48 - INFO - __main__ -     Num steps = 426\n","03/01/2019 21:02:48 - INFO - __main__ -   Loss on batch 0: 0.6728334426879883\n","03/01/2019 21:03:11 - INFO - __main__ -   Loss on batch 50: 0.691570520401001\n","03/01/2019 21:03:27 - INFO - __main__ -   Training loss after epoch 0.676655040230862\n","03/01/2019 21:03:46 - INFO - __main__ -   Training accuracy after epoch 0.7459736456808199\n","03/01/2019 21:03:46 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 21:03:46 - INFO - __main__ -   Eval after epoch 1\n","03/01/2019 21:04:22 - INFO - __main__ -   {'loss': 0.6215523689292198, 'accuracy': 0.6580102414045355}\n","03/01/2019 21:04:22 - INFO - __main__ -   Loss on batch 0: 0.5897401571273804\n","03/01/2019 21:04:45 - INFO - __main__ -   Loss on batch 50: 0.49580901861190796\n","03/01/2019 21:05:01 - INFO - __main__ -   Training loss after epoch 0.5189774639384691\n","03/01/2019 21:05:19 - INFO - __main__ -   Training accuracy after epoch 0.9538799414348462\n","03/01/2019 21:05:19 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 21:05:19 - INFO - __main__ -   Eval after epoch 2\n","03/01/2019 21:05:55 - INFO - __main__ -   {'loss': 0.6176181113304093, 'accuracy': 0.6744696415508412}\n","03/01/2019 21:05:55 - INFO - __main__ -   Loss on batch 0: 0.24793748557567596\n","03/01/2019 21:06:18 - INFO - __main__ -   Loss on batch 50: 0.03114049881696701\n","03/01/2019 21:06:34 - INFO - __main__ -   Training loss after epoch 0.13671002870555535\n","03/01/2019 21:06:53 - INFO - __main__ -   Training accuracy after epoch 0.9934114202049781\n","03/01/2019 21:06:53 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 21:06:53 - INFO - __main__ -   Eval after epoch 3\n","03/01/2019 21:07:29 - INFO - __main__ -   {'loss': 0.9247695512549822, 'accuracy': 0.6656912948061449}\n","03/01/2019 21:07:29 - INFO - __main__ -   Loss on batch 0: 0.014141395688056946\n","03/01/2019 21:07:52 - INFO - __main__ -   Loss on batch 50: 0.0088987797498703\n","03/01/2019 21:08:08 - INFO - __main__ -   Training loss after epoch 0.03171991528726594\n","03/01/2019 21:08:26 - INFO - __main__ -   Training accuracy after epoch 0.9985358711566618\n","03/01/2019 21:08:26 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 21:08:26 - INFO - __main__ -   Eval after epoch 4\n","03/01/2019 21:09:02 - INFO - __main__ -   {'loss': 1.194915650888931, 'accuracy': 0.6667885881492319}\n","03/01/2019 21:09:02 - INFO - __main__ -   Loss on batch 0: 0.012562848627567291\n","03/01/2019 21:09:25 - INFO - __main__ -   Loss on batch 50: 0.0052239298820495605\n","03/01/2019 21:09:41 - INFO - __main__ -   Training loss after epoch 0.011529335310834265\n","03/01/2019 21:09:59 - INFO - __main__ -   Training accuracy after epoch 0.9985358711566618\n","03/01/2019 21:09:59 - INFO - __main__ -   ***** Running evaluation *****\n","03/01/2019 21:09:59 - INFO - __main__ -   Eval after epoch 5\n","03/01/2019 21:10:35 - INFO - __main__ -   {'loss': 1.2401900735012321, 'accuracy': 0.6686174103877103}\n"],"name":"stderr"},{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.738109        0.675573       0.664643\n","1        0.733412        0.679977       0.675595\n","2        0.713447        0.663535       0.659537\n","3        0.711098        0.668233       0.667060\n","4        0.737522        0.672930       0.646798\n"],"name":"stdout"}]},{"metadata":{"id":"xVJ5JOG1ZoMe","colab_type":"code","colab":{}},"cell_type":"code","source":["record_12 = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Sy4HfL3eZo3h","colab_type":"code","outputId":"d654f77d-6eaa-4d31-fe60-5232968cb8fe","executionInfo":{"status":"ok","timestamp":1552320554999,"user_tz":0,"elapsed":236450,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["for n in range(0,5):\n","  \n","  with open(SARC_POL+f\"models/12_{n}.pickle\", 'rb') as handle:\n","    model = pickle.load(handle)\n","\n","  #Evaluate on the balanced test set\n","  tupb = eval(testbal_examples, testbal_features, model, device)\n","  bal_test_balacc = balanced_accuracy(softmax(tupb[1]), tupb[2])\n","  bal_test_acc = tupb[0]['accuracy']\n","  bal_test_F1 = f1_score(tupb[2], np.argmax(tupb[1], axis=1), labels=None, pos_label=1, average='binary', sample_weight=None)\n","  \n","  #Store the results\n","  record_12 = record_12.append({'Acc (bal, bal)': bal_test_balacc, 'Acc (bal, reg)': bal_test_acc, 'F1 (bal, reg)': bal_test_F1}, ignore_index=True)\n","  \n","print(record_12)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.738109        0.675573       0.664643\n","1        0.735173        0.679977       0.675595\n","2        0.715208        0.663535       0.659537\n","3        0.718732        0.668233       0.667060\n","4        0.732824        0.672930       0.646798\n"],"name":"stdout"}]},{"metadata":{"id":"PCTSpGD97fX3","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test12.pickle\", 'wb') as handle:\n","  pickle.dump(record_12, handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KmI9TvdOYdi_","colab_type":"text"},"cell_type":"markdown","source":["## 6.25% project training set\n"]},{"metadata":{"id":"dPpH2oCCYk2e","colab_type":"text"},"cell_type":"markdown","source":["Best hyperparameters found in cross validation were batch size = 16, learning rate = 3e-5, training for 5 epochs."]},{"metadata":{"id":"kuAhjWThYpLf","colab_type":"code","colab":{}},"cell_type":"code","source":["#Set the hyperparamters\n","args[\"learning_rate\"] = 3e-5\n","args[\"train_batch_size\"] = 16\n","args[\"num_train_epochs\"] = 5"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SjCoMzqdYwc7","colab_type":"text"},"cell_type":"markdown","source":["### Load in and process the training data"]},{"metadata":{"id":"WIGriTt-YpHB","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load in the required training sets\n","traindf_6 = pd.read_csv(SARC_POL+'project_data/project_training_6.csv', index_col=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G6vRA6RtYpEP","colab_type":"code","colab":{}},"cell_type":"code","source":["#Process the training examples\n","train_examples = []\n","\n","for i in range(0,len(traindf_6.index)):\n","        train_examples.append(InputExample(str(i), traindf_6.loc[i,'response'], None, str(traindf_6.loc[i,'label'])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zpXJiraNYpBU","colab_type":"code","outputId":"3bdd6fd0-61a1-472b-a72c-617ef765ef0f","executionInfo":{"status":"ok","timestamp":1552320662048,"user_tz":0,"elapsed":1113,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":632}},"cell_type":"code","source":["#Create the features based on the training set\n","train_features = convert_examples_to_features(train_examples, label_list, args[\"max_seq_length\"], tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/11/2019 16:11:01 - INFO - __main__ -   *** Example ***\n","03/11/2019 16:11:01 - INFO - __main__ -   guid: 0\n","03/11/2019 16:11:01 - INFO - __main__ -   tokens: [CLS] the ride never ends . [SEP]\n","03/11/2019 16:11:01 - INFO - __main__ -   input_ids: 101 1996 4536 2196 4515 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   label: 0 (id = 0)\n","03/11/2019 16:11:01 - INFO - __main__ -   *** Example ***\n","03/11/2019 16:11:01 - INFO - __main__ -   guid: 1\n","03/11/2019 16:11:01 - INFO - __main__ -   tokens: [CLS] the obama appointed john kirby is keeping the go ##p witch hunt alive i see . [SEP]\n","03/11/2019 16:11:01 - INFO - __main__ -   input_ids: 101 1996 8112 2805 2198 15129 2003 4363 1996 2175 2361 6965 5690 4142 1045 2156 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   label: 1 (id = 1)\n","03/11/2019 16:11:01 - INFO - __main__ -   *** Example ***\n","03/11/2019 16:11:01 - INFO - __main__ -   guid: 2\n","03/11/2019 16:11:01 - INFO - __main__ -   tokens: [CLS] hillary , ct ##r , obama , rigged polls , mag ##a [SEP]\n","03/11/2019 16:11:01 - INFO - __main__ -   input_ids: 101 18520 1010 14931 2099 1010 8112 1010 25216 14592 1010 23848 2050 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   label: 1 (id = 1)\n","03/11/2019 16:11:01 - INFO - __main__ -   *** Example ***\n","03/11/2019 16:11:01 - INFO - __main__ -   guid: 3\n","03/11/2019 16:11:01 - INFO - __main__ -   tokens: [CLS] so what [SEP]\n","03/11/2019 16:11:01 - INFO - __main__ -   input_ids: 101 2061 2054 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   label: 0 (id = 0)\n","03/11/2019 16:11:01 - INFO - __main__ -   *** Example ***\n","03/11/2019 16:11:01 - INFO - __main__ -   guid: 4\n","03/11/2019 16:11:01 - INFO - __main__ -   tokens: [CLS] romney : \" see , now that i won the debate , employers are now excited about my upcoming victory and are hiring again ! \" [SEP]\n","03/11/2019 16:11:01 - INFO - __main__ -   input_ids: 101 19615 1024 1000 2156 1010 2085 2008 1045 2180 1996 5981 1010 12433 2024 2085 7568 2055 2026 9046 3377 1998 2024 14763 2153 999 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","03/11/2019 16:11:01 - INFO - __main__ -   label: 1 (id = 1)\n"],"name":"stderr"}]},{"metadata":{"id":"_B4rTAcEY_Aj","colab_type":"text"},"cell_type":"markdown","source":["### Train the model, save the models and results"]},{"metadata":{"id":"3-5viPZhYo9I","colab_type":"code","outputId":"c7e9943b-c1d4-40a4-c518-969e8f18d980","executionInfo":{"status":"ok","timestamp":1552322831409,"user_tz":0,"elapsed":1684581,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":4372}},"cell_type":"code","source":["#Not explicitly setting seed, each run because each lopp will generate a different random number from the seed set in args\n","for n in range(0,5):\n","  model, resultsdf, batch_losses = train()\n","  \n","  with open(SARC_POL+f\"models/6_{n}.pickle\", 'wb') as handle:\n","    pickle.dump(model, handle)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["03/11/2019 16:19:07 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/11/2019 16:19:07 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp9qn5xy5y\n","03/11/2019 16:19:12 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/11/2019 16:19:17 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/11/2019 16:19:17 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/11/2019 16:19:17 - INFO - __main__ -     Num examples = 682\n","03/11/2019 16:19:17 - INFO - __main__ -     Batch size = 16\n","03/11/2019 16:19:17 - INFO - __main__ -     Num steps = 213\n","03/11/2019 16:19:17 - INFO - __main__ -   Loss on batch 0: 0.6869816780090332\n","03/11/2019 16:19:36 - INFO - __main__ -   Training loss after epoch 0.7084434115609457\n","03/11/2019 16:19:45 - INFO - __main__ -   Training accuracy after epoch 0.5146627565982405\n","03/11/2019 16:19:45 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:19:45 - INFO - __main__ -   Eval after epoch 1\n","03/11/2019 16:20:21 - INFO - __main__ -   {'loss': 0.6808939194956491, 'accuracy': 0.5054864667154353}\n","03/11/2019 16:20:22 - INFO - __main__ -   Loss on batch 0: 0.7035794854164124\n","03/11/2019 16:20:41 - INFO - __main__ -   Training loss after epoch 0.6300891540771307\n","03/11/2019 16:20:50 - INFO - __main__ -   Training accuracy after epoch 0.8607038123167156\n","03/11/2019 16:20:50 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:20:50 - INFO - __main__ -   Eval after epoch 2\n","03/11/2019 16:21:26 - INFO - __main__ -   {'loss': 0.6423626857441526, 'accuracy': 0.6495976591075348}\n","03/11/2019 16:21:26 - INFO - __main__ -   Loss on batch 0: 0.24974316358566284\n","03/11/2019 16:21:45 - INFO - __main__ -   Training loss after epoch 0.3157162208889806\n","03/11/2019 16:21:54 - INFO - __main__ -   Training accuracy after epoch 0.9736070381231672\n","03/11/2019 16:21:54 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:21:54 - INFO - __main__ -   Eval after epoch 3\n","03/11/2019 16:22:30 - INFO - __main__ -   {'loss': 0.8330685721580372, 'accuracy': 0.6561814191660571}\n","03/11/2019 16:22:31 - INFO - __main__ -   Loss on batch 0: 0.046898253262043\n","03/11/2019 16:22:50 - INFO - __main__ -   Training loss after epoch 0.09635899146628935\n","03/11/2019 16:22:59 - INFO - __main__ -   Training accuracy after epoch 0.9941348973607038\n","03/11/2019 16:22:59 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:22:59 - INFO - __main__ -   Eval after epoch 4\n","03/11/2019 16:23:35 - INFO - __main__ -   {'loss': 1.048725651447163, 'accuracy': 0.6547183613752743}\n","03/11/2019 16:23:35 - INFO - __main__ -   Loss on batch 0: 0.014112427830696106\n","03/11/2019 16:23:54 - INFO - __main__ -   Training loss after epoch 0.03566591697203558\n","03/11/2019 16:24:03 - INFO - __main__ -   Training accuracy after epoch 0.9941348973607038\n","03/11/2019 16:24:03 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:24:03 - INFO - __main__ -   Eval after epoch 5\n","03/11/2019 16:24:39 - INFO - __main__ -   {'loss': 1.13383502322574, 'accuracy': 0.6525237746891002}\n","03/11/2019 16:24:44 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/11/2019 16:24:44 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpjs5wkfj3\n","03/11/2019 16:24:49 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/11/2019 16:24:53 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/11/2019 16:24:53 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/11/2019 16:24:53 - INFO - __main__ -     Num examples = 682\n","03/11/2019 16:24:53 - INFO - __main__ -     Batch size = 16\n","03/11/2019 16:24:53 - INFO - __main__ -     Num steps = 213\n","03/11/2019 16:24:53 - INFO - __main__ -   Loss on batch 0: 0.7243474721908569\n","03/11/2019 16:25:13 - INFO - __main__ -   Training loss after epoch 0.6790912941444752\n","03/11/2019 16:25:22 - INFO - __main__ -   Training accuracy after epoch 0.7932551319648093\n","03/11/2019 16:25:22 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:25:22 - INFO - __main__ -   Eval after epoch 1\n","03/11/2019 16:25:58 - INFO - __main__ -   {'loss': 0.6577062870180884, 'accuracy': 0.5998536942209217}\n","03/11/2019 16:25:58 - INFO - __main__ -   Loss on batch 0: 0.4576590955257416\n","03/11/2019 16:26:17 - INFO - __main__ -   Training loss after epoch 0.41420323141785553\n","03/11/2019 16:26:26 - INFO - __main__ -   Training accuracy after epoch 0.9809384164222874\n","03/11/2019 16:26:26 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:26:26 - INFO - __main__ -   Eval after epoch 2\n","03/11/2019 16:27:03 - INFO - __main__ -   {'loss': 0.789310003435889, 'accuracy': 0.6583760058522312}\n","03/11/2019 16:27:03 - INFO - __main__ -   Loss on batch 0: 0.05719060078263283\n","03/11/2019 16:27:22 - INFO - __main__ -   Training loss after epoch 0.07071830999366073\n","03/11/2019 16:27:31 - INFO - __main__ -   Training accuracy after epoch 0.998533724340176\n","03/11/2019 16:27:31 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:27:31 - INFO - __main__ -   Eval after epoch 3\n","03/11/2019 16:28:07 - INFO - __main__ -   {'loss': 1.1495487225610157, 'accuracy': 0.6715435259692758}\n","03/11/2019 16:28:07 - INFO - __main__ -   Loss on batch 0: 0.008709825575351715\n","03/11/2019 16:28:26 - INFO - __main__ -   Training loss after epoch 0.01484462873317128\n","03/11/2019 16:28:35 - INFO - __main__ -   Training accuracy after epoch 1.0\n","03/11/2019 16:28:35 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:28:35 - INFO - __main__ -   Eval after epoch 4\n","03/11/2019 16:29:11 - INFO - __main__ -   {'loss': 1.2604311981866525, 'accuracy': 0.6700804681784931}\n","03/11/2019 16:29:12 - INFO - __main__ -   Loss on batch 0: 0.0032312721014022827\n","03/11/2019 16:29:31 - INFO - __main__ -   Training loss after epoch 0.0033096570283347782\n","03/11/2019 16:29:40 - INFO - __main__ -   Training accuracy after epoch 1.0\n","03/11/2019 16:29:40 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:29:40 - INFO - __main__ -   Eval after epoch 5\n","03/11/2019 16:30:16 - INFO - __main__ -   {'loss': 1.2909407400807669, 'accuracy': 0.6715435259692758}\n","03/11/2019 16:30:20 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/11/2019 16:30:20 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpt_j6ce5u\n","03/11/2019 16:30:25 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/11/2019 16:30:29 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/11/2019 16:30:29 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/11/2019 16:30:29 - INFO - __main__ -     Num examples = 682\n","03/11/2019 16:30:29 - INFO - __main__ -     Batch size = 16\n","03/11/2019 16:30:29 - INFO - __main__ -     Num steps = 213\n","03/11/2019 16:30:29 - INFO - __main__ -   Loss on batch 0: 0.7751578688621521\n","03/11/2019 16:30:49 - INFO - __main__ -   Training loss after epoch 0.7135940540668576\n","03/11/2019 16:30:58 - INFO - __main__ -   Training accuracy after epoch 0.5\n","03/11/2019 16:30:58 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:30:58 - INFO - __main__ -   Eval after epoch 1\n","03/11/2019 16:31:34 - INFO - __main__ -   {'loss': 0.7009198610172716, 'accuracy': 0.5}\n","03/11/2019 16:31:34 - INFO - __main__ -   Loss on batch 0: 0.6643955111503601\n","03/11/2019 16:31:53 - INFO - __main__ -   Training loss after epoch 0.7052534957264744\n","03/11/2019 16:32:02 - INFO - __main__ -   Training accuracy after epoch 0.5659824046920822\n","03/11/2019 16:32:02 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:32:02 - INFO - __main__ -   Eval after epoch 2\n","03/11/2019 16:32:39 - INFO - __main__ -   {'loss': 0.6936936974525452, 'accuracy': 0.556693489392831}\n","03/11/2019 16:32:39 - INFO - __main__ -   Loss on batch 0: 0.6566827297210693\n","03/11/2019 16:32:58 - INFO - __main__ -   Training loss after epoch 0.6851122018902801\n","03/11/2019 16:33:07 - INFO - __main__ -   Training accuracy after epoch 0.7096774193548387\n","03/11/2019 16:33:07 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:33:07 - INFO - __main__ -   Eval after epoch 3\n","03/11/2019 16:33:43 - INFO - __main__ -   {'loss': 0.6605512388916903, 'accuracy': 0.6097293343087052}\n","03/11/2019 16:33:43 - INFO - __main__ -   Loss on batch 0: 0.650482714176178\n","03/11/2019 16:34:03 - INFO - __main__ -   Training loss after epoch 0.5498067147509996\n","03/11/2019 16:34:12 - INFO - __main__ -   Training accuracy after epoch 0.8651026392961877\n","03/11/2019 16:34:12 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:34:12 - INFO - __main__ -   Eval after epoch 4\n","03/11/2019 16:34:48 - INFO - __main__ -   {'loss': 0.6615567689025125, 'accuracy': 0.6393562545720556}\n","03/11/2019 16:34:48 - INFO - __main__ -   Loss on batch 0: 0.29068952798843384\n","03/11/2019 16:35:07 - INFO - __main__ -   Training loss after epoch 0.25003062188625336\n","03/11/2019 16:35:16 - INFO - __main__ -   Training accuracy after epoch 0.9545454545454546\n","03/11/2019 16:35:16 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:35:16 - INFO - __main__ -   Eval after epoch 5\n","03/11/2019 16:35:53 - INFO - __main__ -   {'loss': 0.8116609845743623, 'accuracy': 0.6583760058522312}\n","03/11/2019 16:35:57 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/11/2019 16:35:57 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpp56hv7l0\n","03/11/2019 16:36:02 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/11/2019 16:36:06 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/11/2019 16:36:06 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/11/2019 16:36:07 - INFO - __main__ -     Num examples = 682\n","03/11/2019 16:36:07 - INFO - __main__ -     Batch size = 16\n","03/11/2019 16:36:07 - INFO - __main__ -     Num steps = 213\n","03/11/2019 16:36:07 - INFO - __main__ -   Loss on batch 0: 0.6709990501403809\n","03/11/2019 16:36:26 - INFO - __main__ -   Training loss after epoch 0.7098731149074643\n","03/11/2019 16:36:35 - INFO - __main__ -   Training accuracy after epoch 0.5\n","03/11/2019 16:36:35 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:36:35 - INFO - __main__ -   Eval after epoch 1\n","03/11/2019 16:37:12 - INFO - __main__ -   {'loss': 0.696125004180642, 'accuracy': 0.5}\n","03/11/2019 16:37:12 - INFO - __main__ -   Loss on batch 0: 0.67491614818573\n","03/11/2019 16:37:31 - INFO - __main__ -   Training loss after epoch 0.6045246775760207\n","03/11/2019 16:37:40 - INFO - __main__ -   Training accuracy after epoch 0.8856304985337243\n","03/11/2019 16:37:40 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:37:40 - INFO - __main__ -   Eval after epoch 2\n","03/11/2019 16:38:16 - INFO - __main__ -   {'loss': 0.6684605735679006, 'accuracy': 0.6196049743964887}\n","03/11/2019 16:38:16 - INFO - __main__ -   Loss on batch 0: 0.36400023102760315\n","03/11/2019 16:38:35 - INFO - __main__ -   Training loss after epoch 0.24065571061747018\n","03/11/2019 16:38:44 - INFO - __main__ -   Training accuracy after epoch 0.9736070381231672\n","03/11/2019 16:38:44 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:38:44 - INFO - __main__ -   Eval after epoch 3\n","03/11/2019 16:39:20 - INFO - __main__ -   {'loss': 1.135962837657263, 'accuracy': 0.6210680321872714}\n","03/11/2019 16:39:21 - INFO - __main__ -   Loss on batch 0: 0.020877152681350708\n","03/11/2019 16:39:40 - INFO - __main__ -   Training loss after epoch 0.060612185316723445\n","03/11/2019 16:39:49 - INFO - __main__ -   Training accuracy after epoch 0.998533724340176\n","03/11/2019 16:39:49 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:39:49 - INFO - __main__ -   Eval after epoch 4\n","03/11/2019 16:40:25 - INFO - __main__ -   {'loss': 1.1031296409146731, 'accuracy': 0.6602048280907096}\n","03/11/2019 16:40:26 - INFO - __main__ -   Loss on batch 0: 0.0086875781416893\n","03/11/2019 16:40:45 - INFO - __main__ -   Training loss after epoch 0.015144445142860329\n","03/11/2019 16:40:54 - INFO - __main__ -   Training accuracy after epoch 0.998533724340176\n","03/11/2019 16:40:54 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:40:54 - INFO - __main__ -   Eval after epoch 5\n","03/11/2019 16:41:30 - INFO - __main__ -   {'loss': 1.1362709749576658, 'accuracy': 0.6576444769568398}\n","03/11/2019 16:41:34 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","03/11/2019 16:41:34 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp6ibzu8jz\n","03/11/2019 16:41:39 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","03/11/2019 16:41:44 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","03/11/2019 16:41:44 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","03/11/2019 16:41:44 - INFO - __main__ -     Num examples = 682\n","03/11/2019 16:41:44 - INFO - __main__ -     Batch size = 16\n","03/11/2019 16:41:44 - INFO - __main__ -     Num steps = 213\n","03/11/2019 16:41:44 - INFO - __main__ -   Loss on batch 0: 0.6614305973052979\n","03/11/2019 16:42:04 - INFO - __main__ -   Training loss after epoch 0.7049874760383783\n","03/11/2019 16:42:13 - INFO - __main__ -   Training accuracy after epoch 0.6891495601173021\n","03/11/2019 16:42:13 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:42:13 - INFO - __main__ -   Eval after epoch 1\n","03/11/2019 16:42:49 - INFO - __main__ -   {'loss': 0.6657686476097551, 'accuracy': 0.5687637161667886}\n","03/11/2019 16:42:49 - INFO - __main__ -   Loss on batch 0: 0.6395055651664734\n","03/11/2019 16:43:08 - INFO - __main__ -   Training loss after epoch 0.5407228587671767\n","03/11/2019 16:43:17 - INFO - __main__ -   Training accuracy after epoch 0.9618768328445748\n","03/11/2019 16:43:17 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:43:17 - INFO - __main__ -   Eval after epoch 2\n","03/11/2019 16:43:53 - INFO - __main__ -   {'loss': 0.6375037144089855, 'accuracy': 0.6623994147768837}\n","03/11/2019 16:43:53 - INFO - __main__ -   Loss on batch 0: 0.17094245553016663\n","03/11/2019 16:44:13 - INFO - __main__ -   Training loss after epoch 0.10661598271148842\n","03/11/2019 16:44:22 - INFO - __main__ -   Training accuracy after epoch 1.0\n","03/11/2019 16:44:22 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:44:22 - INFO - __main__ -   Eval after epoch 3\n","03/11/2019 16:44:58 - INFO - __main__ -   {'loss': 1.030194120698197, 'accuracy': 0.6649597659107535}\n","03/11/2019 16:44:58 - INFO - __main__ -   Loss on batch 0: 0.021075837314128876\n","03/11/2019 16:45:17 - INFO - __main__ -   Training loss after epoch 0.010785965151478385\n","03/11/2019 16:45:26 - INFO - __main__ -   Training accuracy after epoch 1.0\n","03/11/2019 16:45:26 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:45:26 - INFO - __main__ -   Eval after epoch 4\n","03/11/2019 16:46:02 - INFO - __main__ -   {'loss': 1.2497452712336252, 'accuracy': 0.6642282370153622}\n","03/11/2019 16:46:02 - INFO - __main__ -   Loss on batch 0: 0.004488714039325714\n","03/11/2019 16:46:22 - INFO - __main__ -   Training loss after epoch 0.002772180320218552\n","03/11/2019 16:46:31 - INFO - __main__ -   Training accuracy after epoch 1.0\n","03/11/2019 16:46:31 - INFO - __main__ -   ***** Running evaluation *****\n","03/11/2019 16:46:31 - INFO - __main__ -   Eval after epoch 5\n","03/11/2019 16:47:07 - INFO - __main__ -   {'loss': 1.3005888760089874, 'accuracy': 0.667885881492319}\n"],"name":"stderr"}]},{"metadata":{"id":"pEfwnSYvYo6Y","colab_type":"code","colab":{}},"cell_type":"code","source":["record_6 = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4Kai3Pq3Yo0I","colab_type":"code","outputId":"95c1ba81-62cc-4d34-dee9-d1b1d2dfdc00","executionInfo":{"status":"ok","timestamp":1552323110132,"user_tz":0,"elapsed":238852,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["for n in range(0,5):\n","  \n","  with open(SARC_POL+f\"models/6_{n}.pickle\", 'rb') as handle:\n","    model = pickle.load(handle)\n","\n","  #Evaluate on the balanced test set\n","  tupb = eval(testbal_examples, testbal_features, model, device)\n","  bal_test_balacc = balanced_accuracy(softmax(tupb[1]), tupb[2])\n","  bal_test_acc = tupb[0]['accuracy']\n","  bal_test_F1 = f1_score(tupb[2], np.argmax(tupb[1], axis=1), labels=None, pos_label=1, average='binary', sample_weight=None)\n","\n","  #Store the results\n","  record_6 = record_6.append({'Acc (bal, bal)': bal_test_balacc, 'Acc (bal, reg)': bal_test_acc, 'F1 (bal, reg)': bal_test_F1}, ignore_index=True)\n","\n","print(record_6)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.706400        0.650323       0.633200\n","1        0.726365        0.656489       0.669492\n","2        0.696418        0.657957       0.646863\n","3        0.702290        0.654140       0.665910\n","4        0.710511        0.657957       0.649835\n"],"name":"stdout"}]},{"metadata":{"id":"Op8f-PBdZUOi","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test6.pickle\", 'wb') as handle:\n","  pickle.dump(record_6, handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s-PcHZ-RFyb1","colab_type":"text"},"cell_type":"markdown","source":["## Show results"]},{"metadata":{"id":"89A8RCTlF1nH","colab_type":"text"},"cell_type":"markdown","source":["### 100%"]},{"metadata":{"id":"X0uaPS4L7fVh","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test100.pickle\", 'rb') as handle:\n","  df100 = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"64Uf3R4XHOYs","colab_type":"code","outputId":"7e1745f9-e7a9-4fe5-fbdb-1d4f8d064128","executionInfo":{"status":"ok","timestamp":1552323212829,"user_tz":0,"elapsed":639,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["df100"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Acc (bal, bal)</th>\n","      <th>Acc (bal, reg)</th>\n","      <th>F1 (bal, reg)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.812096</td>\n","      <td>0.740752</td>\n","      <td>0.742641</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.816207</td>\n","      <td>0.733118</td>\n","      <td>0.727436</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.816794</td>\n","      <td>0.736348</td>\n","      <td>0.734947</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.808573</td>\n","      <td>0.735467</td>\n","      <td>0.732562</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.818555</td>\n","      <td>0.739577</td>\n","      <td>0.730804</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.812096        0.740752       0.742641\n","1        0.816207        0.733118       0.727436\n","2        0.816794        0.736348       0.734947\n","3        0.808573        0.735467       0.732562\n","4        0.818555        0.739577       0.730804"]},"metadata":{"tags":[]},"execution_count":80}]},{"metadata":{"id":"W2YaQTZyGYyS","colab_type":"text"},"cell_type":"markdown","source":["### 50%"]},{"metadata":{"id":"Q44Xfa537fR5","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test50.pickle\", 'rb') as handle:\n","  df50 = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IiW9hYiHGWuU","colab_type":"code","outputId":"560c52dc-cf2f-47ad-dd97-1a8e4e4ef0d8","executionInfo":{"status":"ok","timestamp":1552323216290,"user_tz":0,"elapsed":953,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["df50"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Acc (bal, bal)</th>\n","      <th>Acc (bal, reg)</th>\n","      <th>F1 (bal, reg)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.746330</td>\n","      <td>0.701703</td>\n","      <td>0.678277</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.780975</td>\n","      <td>0.723429</td>\n","      <td>0.713329</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.783911</td>\n","      <td>0.710804</td>\n","      <td>0.696830</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.786260</td>\n","      <td>0.719025</td>\n","      <td>0.718612</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.774516</td>\n","      <td>0.716970</td>\n","      <td>0.714962</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.746330        0.701703       0.678277\n","1        0.780975        0.723429       0.713329\n","2        0.783911        0.710804       0.696830\n","3        0.786260        0.719025       0.718612\n","4        0.774516        0.716970       0.714962"]},"metadata":{"tags":[]},"execution_count":82}]},{"metadata":{"id":"p8A-GSdBGbbB","colab_type":"text"},"cell_type":"markdown","source":["### 25%"]},{"metadata":{"id":"e78mYVeYGXn0","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test25.pickle\", 'rb') as handle:\n","  df25 = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NqKhlQPFGhWB","colab_type":"code","outputId":"8465cd0f-d2d5-48cb-9dd3-b1a99c1ae179","executionInfo":{"status":"ok","timestamp":1552323221591,"user_tz":0,"elapsed":915,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["df25"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Acc (bal, bal)</th>\n","      <th>Acc (bal, reg)</th>\n","      <th>F1 (bal, reg)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.763359</td>\n","      <td>0.698767</td>\n","      <td>0.684502</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.762772</td>\n","      <td>0.699060</td>\n","      <td>0.670100</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.678215</td>\n","      <td>0.622137</td>\n","      <td>0.507085</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.756900</td>\n","      <td>0.691720</td>\n","      <td>0.697057</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.753376</td>\n","      <td>0.693776</td>\n","      <td>0.688564</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.763359        0.698767       0.684502\n","1        0.762772        0.699060       0.670100\n","2        0.678215        0.622137       0.507085\n","3        0.756900        0.691720       0.697057\n","4        0.753376        0.693776       0.688564"]},"metadata":{"tags":[]},"execution_count":84}]},{"metadata":{"id":"bVhEG3HmGkoZ","colab_type":"text"},"cell_type":"markdown","source":["### 12.5%"]},{"metadata":{"id":"XE8N3XJmGiHi","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test12.pickle\", 'rb') as handle:\n","  df12 = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8eaqVxz6Gos7","colab_type":"code","outputId":"5ce91116-4395-4da2-d2c8-7de632ebdbb2","executionInfo":{"status":"ok","timestamp":1552323227007,"user_tz":0,"elapsed":750,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["df12"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Acc (bal, bal)</th>\n","      <th>Acc (bal, reg)</th>\n","      <th>F1 (bal, reg)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.738109</td>\n","      <td>0.675573</td>\n","      <td>0.664643</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.735173</td>\n","      <td>0.679977</td>\n","      <td>0.675595</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.715208</td>\n","      <td>0.663535</td>\n","      <td>0.659537</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.718732</td>\n","      <td>0.668233</td>\n","      <td>0.667060</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.732824</td>\n","      <td>0.672930</td>\n","      <td>0.646798</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.738109        0.675573       0.664643\n","1        0.735173        0.679977       0.675595\n","2        0.715208        0.663535       0.659537\n","3        0.718732        0.668233       0.667060\n","4        0.732824        0.672930       0.646798"]},"metadata":{"tags":[]},"execution_count":86}]},{"metadata":{"id":"c0F4YFNLZbTn","colab_type":"text"},"cell_type":"markdown","source":["### 6.25%"]},{"metadata":{"id":"IPT42TLfZaFU","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/test6.pickle\", 'rb') as handle:\n","  df6 = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"togAGd6QZZ74","colab_type":"code","outputId":"e522cd98-a0d9-4f13-859d-e03a72f9bd68","executionInfo":{"status":"ok","timestamp":1552323230148,"user_tz":0,"elapsed":1002,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["df6"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Acc (bal, bal)</th>\n","      <th>Acc (bal, reg)</th>\n","      <th>F1 (bal, reg)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.706400</td>\n","      <td>0.650323</td>\n","      <td>0.633200</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.726365</td>\n","      <td>0.656489</td>\n","      <td>0.669492</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.696418</td>\n","      <td>0.657957</td>\n","      <td>0.646863</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.702290</td>\n","      <td>0.654140</td>\n","      <td>0.665910</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.710511</td>\n","      <td>0.657957</td>\n","      <td>0.649835</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.706400        0.650323       0.633200\n","1        0.726365        0.656489       0.669492\n","2        0.696418        0.657957       0.646863\n","3        0.702290        0.654140       0.665910\n","4        0.710511        0.657957       0.649835"]},"metadata":{"tags":[]},"execution_count":88}]},{"metadata":{"id":"AE8iSh9lq189","colab_type":"text"},"cell_type":"markdown","source":["### Compile dataframe of averages and standard deviations over the five repeats"]},{"metadata":{"id":"K3HWGvq7rAXh","colab_type":"code","colab":{}},"cell_type":"code","source":["m100 = np.mean(df100, axis=0)\n","m50 = np.mean(df50, axis=0)\n","m25 = np.mean(df25, axis=0)\n","m12 = np.mean(df12, axis=0)\n","m6 = np.mean(df6, axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ohTcnGHNrFRE","colab_type":"code","colab":{}},"cell_type":"code","source":["meandf = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Rr-0ZxuvrTxj","colab_type":"code","colab":{}},"cell_type":"code","source":["meandf = meandf.append(m100, ignore_index=True)\n","meandf = meandf.append(m50, ignore_index=True)\n","meandf = meandf.append(m25, ignore_index=True)\n","meandf = meandf.append(m12, ignore_index=True)\n","meandf = meandf.append(m6, ignore_index=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"52o7LFLIrXbj","colab_type":"code","colab":{}},"cell_type":"code","source":["meandf.index = ['100%', '50%', '25%', '12.5%', '6.25%']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5hy4xJDbr4IG","colab_type":"code","outputId":"fafa3e83-c94c-4a1a-cd0c-8b87f0ff69cc","executionInfo":{"status":"ok","timestamp":1552323436896,"user_tz":0,"elapsed":1690,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["meandf"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Acc (bal, bal)</th>\n","      <th>Acc (bal, reg)</th>\n","      <th>F1 (bal, reg)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>100%</th>\n","      <td>0.814445</td>\n","      <td>0.737052</td>\n","      <td>0.733678</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.774398</td>\n","      <td>0.714386</td>\n","      <td>0.704402</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.742924</td>\n","      <td>0.681092</td>\n","      <td>0.649462</td>\n","    </tr>\n","    <tr>\n","      <th>12.5%</th>\n","      <td>0.728009</td>\n","      <td>0.672049</td>\n","      <td>0.662726</td>\n","    </tr>\n","    <tr>\n","      <th>6.25%</th>\n","      <td>0.708397</td>\n","      <td>0.655373</td>\n","      <td>0.653060</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","100%         0.814445        0.737052       0.733678\n","50%          0.774398        0.714386       0.704402\n","25%          0.742924        0.681092       0.649462\n","12.5%        0.728009        0.672049       0.662726\n","6.25%        0.708397        0.655373       0.653060"]},"metadata":{"tags":[]},"execution_count":102}]},{"metadata":{"id":"C5NC-nG0raxA","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/mean_summary.pickle\", 'wb') as handle:\n","  pickle.dump(meandf, handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SuK4lsWHsBXi","colab_type":"code","colab":{}},"cell_type":"code","source":["sd100 = np.std(df100, axis=0)\n","sd50 = np.std(df50, axis=0)\n","sd25 = np.std(df25, axis=0)\n","sd12 = np.std(df12, axis=0)\n","sd6 = np.std(df6, axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KYeDngi5sI20","colab_type":"code","colab":{}},"cell_type":"code","source":["stddf = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SacBdpJQsItr","colab_type":"code","colab":{}},"cell_type":"code","source":["stddf = stddf.append(sd100, ignore_index=True)\n","stddf = stddf.append(sd50, ignore_index=True)\n","stddf = stddf.append(sd25, ignore_index=True)\n","stddf = stddf.append(sd12, ignore_index=True)\n","stddf = stddf.append(sd6, ignore_index=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q36zvHg2sIki","colab_type":"code","colab":{}},"cell_type":"code","source":["stddf.index = ['100%', '50%', '25%', '12.5%', '6.25%']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RUj_vFw_sXRD","colab_type":"code","outputId":"7cf1601a-f3dd-4e64-a75f-0e2045f0de8b","executionInfo":{"status":"ok","timestamp":1552323544032,"user_tz":0,"elapsed":766,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["stddf"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Acc (bal, bal)</th>\n","      <th>Acc (bal, reg)</th>\n","      <th>F1 (bal, reg)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>100%</th>\n","      <td>0.003620</td>\n","      <td>0.002777</td>\n","      <td>0.005107</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.014576</td>\n","      <td>0.007532</td>\n","      <td>0.015053</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.032568</td>\n","      <td>0.029613</td>\n","      <td>0.071720</td>\n","    </tr>\n","    <tr>\n","      <th>12.5%</th>\n","      <td>0.009235</td>\n","      <td>0.005711</td>\n","      <td>0.009507</td>\n","    </tr>\n","    <tr>\n","      <th>6.25%</th>\n","      <td>0.010120</td>\n","      <td>0.002885</td>\n","      <td>0.013254</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","100%         0.003620        0.002777       0.005107\n","50%          0.014576        0.007532       0.015053\n","25%          0.032568        0.029613       0.071720\n","12.5%        0.009235        0.005711       0.009507\n","6.25%        0.010120        0.002885       0.013254"]},"metadata":{"tags":[]},"execution_count":109}]},{"metadata":{"id":"EKkMCMdvsYAD","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(SARC_POL+f\"test_set_results/std_summary.pickle\", 'wb') as handle:\n","  pickle.dump(stddf, handle)"],"execution_count":0,"outputs":[]}]}