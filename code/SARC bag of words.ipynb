{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ben_v3 - SARC bag of words.ipynb","version":"0.3.2","provenance":[{"file_id":"1xQsyu2BaO3tO932IzLNnE9-H8dhEbcWi","timestamp":1552484211141},{"file_id":"1VNv0lh0SqiUNgxOGrEDNcc3EvzY0G_Ju","timestamp":1551615990477}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"1FlZqqLIMyVn","colab_type":"text"},"cell_type":"markdown","source":["## The majority of the code in this notebook is work produced by Khodak et al. (2017) and can be found at the following links: https://github.com/NLPrinceton/SARC and https://github.com/NLPrinceton/text_embedding\n","\n","## Code from this repositories has been referenced below.\n","\n"]},{"metadata":{"id":"PB2veJx4ZfVZ","colab_type":"text"},"cell_type":"markdown","source":["## Import libraries and data"]},{"metadata":{"id":"g8o0dxPrZfVd","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import csv\n","import json\n","from sklearn.model_selection import train_test_split\n","\n","import argparse\n","import nltk\n","from sklearn.linear_model import LogisticRegressionCV as LogitCV\n","from sklearn.preprocessing import normalize\n","from sklearn.metrics import f1_score\n","\n","from collections import Counter\n","from itertools import chain\n","from itertools import groupby\n","from operator import itemgetter\n","#from string import punctuation\n","from unicodedata import category\n","import nltk\n","import numpy as np\n","from scipy import sparse as sp"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gArFUBK2g8sh","colab_type":"code","outputId":"63fca8cc-7894-49dd-ddfe-a12a97746aec","executionInfo":{"status":"ok","timestamp":1552484063061,"user_tz":0,"elapsed":25344,"user":{"displayName":"Benjamin Klasmer","photoUrl":"https://lh6.googleusercontent.com/-2TOv2lnfZ5E/AAAAAAAAAAI/AAAAAAAAABY/2YIpXmBA3FY/s64/photo.jpg","userId":"11802859716997612757"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","\n","### SARC Directory Paths ###\n","SARC_POL = '/content/gdrive/My Drive/SARC pol/project_data/'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"0xOj9eUNZfVk","colab_type":"code","colab":{}},"cell_type":"code","source":["############################################ Khodak et al. 2017\n","\n","def tokenize(documents):\n","  '''tokenizes documents\n","  Args:\n","    documents: iterable of strings\n","  Returns:\n","    list of list of strings\n","  '''\n","\n","  return [list(split_on_punctuation(doc)) for doc in documents]\n","\n","###############################################################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JRyd0lwMZfVo","colab_type":"code","colab":{}},"cell_type":"code","source":["############################################ Khodak et al. 2017\n","\n","#PUNCTUATION = set(punctuation)\n","PUNCTUATION = {'M', 'P', 'S'}\n","UINT = np.uint16\n","from unicodedata import category\n","\n","def split_on_punctuation(document):\n","  '''tokenizes string by splitting on spaces and punctuation\n","  Args:\n","    document: string\n","  Returns:\n","    str generator\n","  '''\n","\n","  for token in document.split():\n","    if len(token) == 1:\n","      yield token\n","    else:\n","      chunk = token[0]\n","      for char0, char1 in zip(token[:-1], token[1:]):\n","        #if (char0 in PUNCTUATION) == (char1 in PUNCTUATION):\n","        if (category(char0)[0] in PUNCTUATION) == (category(char1)[0] in PUNCTUATION):\n","          chunk += char1\n","        else:\n","          yield chunk\n","          chunk = char1\n","      if chunk:\n","        yield chunk\n","        \n","###############################################################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6yTdpsvUZfVr","colab_type":"code","colab":{}},"cell_type":"code","source":["############################################ Khodak et al. 2017\n","\n","def feature_counts(documents):\n","  '''computes feature counts from featurized documents\n","  Args:\n","    documents: iterable of lists of hashable features\n","  Returns:\n","    dict mapping features to counts\n","  '''\n","\n","  return Counter(feat for doc in documents for feat in doc)\n","\n","\n","def feature_vocab(documents, min_count=1, sorted_features=sorted):\n","  '''gets feature vocabulary from featurized documents\n","  Args:\n","    documents: iterable of lists of hashable features\n","    min_count: minimum number of times feature must appear to be included in the vocabulary\n","    sorted_features: function that sorts the features\n","  Returns:\n","    {feature: index} dict\n","  '''\n","  \n","  return {feat: i for i, feat in enumerate(sorted_features(feat for feat, count in feature_counts(documents).items() if count >= min_count))}\n","\n","\n","def docs2bofs(documents, vocabulary=None, weights=None, default=1.0, format='csr', **kwargs):\n","  '''constructs sparse BoF representations from featurized documents\n","  Args:\n","    documents: iterable of lists of hashable features\n","    vocabulary: dict mapping features to indices (nonnegative ints) or a list of features; if None will compute automatically from documents\n","    weights: dict mapping features to weights (floats) or a list/np.ndarray of weights; if None will compute unweighted BoFs\n","    default: default feature weight if not feature in weights; ignored if weights is None\n","    format: sparse matrix format\n","    kwargs: passed to feature_vocab; ignored if not vocabulary is None\n","  Returns:\n","    sparse BoF matrix in CSR format of size (len(documents), len(vocabulary))\n","  '''\n","\n","  if vocabulary is None:\n","    vocabulary = feature_vocab(documents, **kwargs)\n","  elif type(vocabulary) == list:\n","    vocabulary = {feat: i for i, feat in enumerate(vocabulary)}\n","\n","  rows, cols, values = zip(*((row, col, count) for (row, col), count in Counter((i, vocabulary.get(feat, -1)) for i, doc in enumerate(documents) for feat in doc).items() if not col==-1))\n","  m = len(documents)\n","  V = len(vocabulary)\n","  if weights is None:\n","    return sp.coo_matrix((values, (rows, cols)), shape=(m, V), dtype=UINT).asformat(format)\n","  bofs = sp.coo_matrix((values, (rows, cols)), shape=(m, V)).tocsr()\n","\n","  if type(weights) == dict:\n","    diag = np.empty(V)\n","    for feat, i in vocabulary.items():\n","      diag[i] = weights.gets(feat, default)\n","  else:\n","    assert len(weights) == V, \"if weights passed as a list/np.ndarray, length must be same as vocabulary size\"\n","    if type(weights) == list:\n","      diag = np.array(weights)\n","    else:\n","      diag = weights\n","  return bofs.dot(sp.diags(diag, 0)).asformat(format)\n","\n","        \n","###############################################################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2BSGM1-YpdDa","colab_type":"text"},"cell_type":"markdown","source":["## Run the classifier"]},{"metadata":{"id":"1ARx14OApez1","colab_type":"code","outputId":"35c03ffd-3713-4616-d4b6-503c877c5fa2","executionInfo":{"status":"ok","timestamp":1552484165234,"user_tz":0,"elapsed":40460,"user":{"displayName":"Benjamin Klasmer","photoUrl":"https://lh6.googleusercontent.com/-2TOv2lnfZ5E/AAAAAAAAAAI/AAAAAAAAABY/2YIpXmBA3FY/s64/photo.jpg","userId":"11802859716997612757"}},"colab":{"base_uri":"https://localhost:8080/","height":2567}},"cell_type":"code","source":["filename = ['project_training_100.csv','project_training_50.csv','project_training_25.csv','project_training_12.csv','project_training_6.csv']\n","\n","# Build a dict to contain all the wrong predictions for each split\n","wrongPredDict = dict()\n","\n","# Let's set up lists to append into so we can plot our results\n","trainAccList = []\n","testAccList = []\n","balScoreList = []\n","f1ScoreList = []\n","\n","#Load in the test set\n","validdf = pd.read_csv(SARC_POL+'balanced_test.csv', index_col = 0)\n","\n","for fname in filename:\n","  \n","  #Load in the training set\n","  traindf = pd.read_csv(SARC_POL+fname, index_col = 0)\n","  \n","  # Only use responses for this method. Ignore ancestors.\n","  train_resp = traindf['response'].values.tolist()\n","  test_resp = validdf['response'].values.tolist()\n","  train_labels = traindf['label'].values.tolist()\n","  test_labels = validdf['label'].values.tolist()\n","\n","  ########## Manipulate the data so that it is in the same form that the code from Khodak et al. uses\n","  # Train responses\n","  first_resp = []\n","  second_resp = []\n","  for idx, resp in enumerate(train_resp):\n","\n","    if idx % 2 == 0:\n","      first_resp.append(resp)\n","    else:\n","      second_resp.append(resp)\n","\n","  train_docs = {0: first_resp, 1: second_resp}\n","\n","\n","  # Test responses\n","  first_resp = []\n","  second_resp = []\n","  for idx, resp in enumerate(test_resp):\n","\n","    if idx % 2 == 0:\n","      first_resp.append(resp)\n","    else:\n","      second_resp.append(resp)\n","\n","  test_docs = {0: first_resp, 1: second_resp}\n","\n","\n","  # Train labels\n","  first_label = []\n","  second_label = []\n","  for idx, label in enumerate(train_labels):\n","\n","    if idx % 2 == 0:\n","      first_label.append(label)\n","    else:\n","      second_label.append(label)\n","\n","  train_labels = {0: first_label, 1: second_label}\n","\n","\n","  # Test labels\n","  first_label = []\n","  second_label = []\n","  for idx, label in enumerate(test_labels):\n","\n","    if idx % 2 == 0:\n","      first_label.append(label)\n","    else:\n","      second_label.append(label)\n","\n","  test_labels = {0: first_label, 1: second_label}\n","  \n","  ############################################ Khodak et al. 2017\n","\n","  # Train a classifier on all responses in training data. We will later use this\n","  # classifier to determine for every sequence which of the 2 responses is more sarcastic.\n","  train_all_docs_tok = tokenize(train_docs[0] + train_docs[1])\n","  test_all_docs_tok = tokenize(test_docs[0] + test_docs[1])\n","  train_all_labels = np.array(train_labels[0] + train_labels[1])\n","  test_all_labels = np.array(test_labels[0] + test_labels[1])\n","\n","  n = 1\n","  min_count = 1\n","  train_ngrams = [sum((list(nltk.ngrams(doc, k)) for k in range(1, n+1)), []) for doc in train_all_docs_tok]\n","  test_ngrams = [sum((list(nltk.ngrams(doc, k)) for k in range(1, n+1)), []) for doc in test_all_docs_tok]\n","  vocabulary = feature_vocab(train_ngrams, min_count=min_count)\n","  train_all_vecs = docs2bofs(train_ngrams, vocabulary)\n","  test_all_vecs = docs2bofs(test_ngrams, vocabulary)\n","  \n","  print(fname, \"\\n----------------------------------\")\n","  \n","  wrongPred = []\n","    \n","  for seed in [0, 24, 729, 857, 403]:\n","\n","    # Evaluate this classifier on all responses.\n","    clf = LogitCV(Cs=[10**i for i in range(-2, 3)], fit_intercept=False, cv=2, dual=np.less(*train_all_vecs.shape), solver='liblinear', n_jobs=-1, random_state=seed) \n","    clf.fit(train_all_vecs, train_all_labels)\n","    print('Train acc: ', clf.score(train_all_vecs, train_all_labels))\n","    print('Test acc: ', clf.score(test_all_vecs, test_all_labels))\n","    \n","  ###############################################################\n","    \n","    # Balanced Test Score Calculation\n","    nAncestors = len(test_labels[0])\n","    countCorrect = 0\n","    for i in range(nAncestors):\n","      \n","      \n","      # For each ancestor grab the two responses\n","      scoreResponse0 = clf.predict_proba(test_all_vecs[i])[0][1]\n","      scoreResponse1 = clf.predict_proba(test_all_vecs[i+nAncestors])[0][1]\n","      \n","      # Calculate which response is more likely to be sarcastic and check whether it is the case\n","      if scoreResponse0 > scoreResponse1 and test_labels[0][i] == 1:\n","        countCorrect += 1\n","        \n","      elif scoreResponse1 > scoreResponse0 and test_labels[1][i] == 1:\n","        countCorrect += 1\n","        \n","\n","      # Collect the responses that we have misclassified\n","      if seed == 403:\n","\n","        if scoreResponse0 > scoreResponse1 and test_labels[0][i] == 0:\n","          wrongPred.append(test_all_docs_tok[i])\n","\n","        elif scoreResponse1 > scoreResponse0 and test_labels[1][i] == 0:\n","          wrongPred.append(test_all_docs_tok[nAncestors+i])\n","      \n","    print(\"Balanced Test Score:\", countCorrect/nAncestors)\n","    \n","    # F1 Prediction\n","    \n","    y_pred = []\n","    y_true = []\n","    \n","    for i in range(len(test_labels[0])+len(test_labels[1])):\n","      \n","      # For each response, classify the response as the label with the highest probability and stores it's true label\n","      scoreResponse = clf.predict_proba(test_all_vecs[i])[0][1]\n","      \n","      if i < nAncestors:\n","        \n","        y_true.append(test_labels[0][i])\n","        \n","      else:\n","        \n","        y_true.append(test_labels[1][i-nAncestors])\n","      \n","      if scoreResponse > 0.5:\n","        y_pred.append(1)\n","        \n","      else:\n","        y_pred.append(0)\n","       \n","    f1Score = f1_score(y_true, y_pred, pos_label=1, average='binary', sample_weight=None)\n","    print(\"F1 Score:\", f1Score, \"\\n\")\n","    \n","    # Append all of our results\n","\n","    trainAccList.append(clf.score(train_all_vecs, train_all_labels))\n","    testAccList.append(clf.score(test_all_vecs, test_all_labels))\n","    balScoreList.append(countCorrect/nAncestors)\n","    f1ScoreList.append(f1Score)\n","        \n","  wrongPredDict[fname] = wrongPred     \n","   \n","  print(\"---------------------------------- \\n\\n\")\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["project_training_100.csv \n","----------------------------------\n","Train acc:  0.7784891165172856\n","Test acc:  0.694950088079859\n","Balanced Test Score: 0.7469172049324722\n","F1 Score: 0.6854374810778081 \n","\n","Train acc:  0.7784891165172856\n","Test acc:  0.694950088079859\n","Balanced Test Score: 0.7469172049324722\n","F1 Score: 0.6854374810778081 \n","\n","Train acc:  0.7784891165172856\n","Test acc:  0.694950088079859\n","Balanced Test Score: 0.7469172049324722\n","F1 Score: 0.6854374810778081 \n","\n","Train acc:  0.7784891165172856\n","Test acc:  0.694950088079859\n","Balanced Test Score: 0.7469172049324722\n","F1 Score: 0.6854374810778081 \n","\n","Train acc:  0.7784891165172856\n","Test acc:  0.694950088079859\n","Balanced Test Score: 0.7469172049324722\n","F1 Score: 0.6854374810778081 \n","\n","---------------------------------- \n","\n","\n","project_training_50.csv \n","----------------------------------\n","Train acc:  0.7943651664837176\n","Test acc:  0.6723429242513213\n","Balanced Test Score: 0.731650029359953\n","F1 Score: 0.6582976117575016 \n","\n","Train acc:  0.7943651664837176\n","Test acc:  0.6723429242513213\n","Balanced Test Score: 0.731650029359953\n","F1 Score: 0.6582976117575016 \n","\n","Train acc:  0.7943651664837176\n","Test acc:  0.6723429242513213\n","Balanced Test Score: 0.731650029359953\n","F1 Score: 0.6582976117575016 \n","\n","Train acc:  0.7943651664837176\n","Test acc:  0.6723429242513213\n","Balanced Test Score: 0.731650029359953\n","F1 Score: 0.6582976117575016 \n","\n","Train acc:  0.7943651664837176\n","Test acc:  0.6723429242513213\n","Balanced Test Score: 0.731650029359953\n","F1 Score: 0.6582976117575016 \n","\n","---------------------------------- \n","\n","\n","project_training_25.csv \n","----------------------------------\n","Train acc:  0.8030746705710102\n","Test acc:  0.6567821491485614\n","Balanced Test Score: 0.715208455666471\n","F1 Score: 0.6406394097755918 \n","\n","Train acc:  0.8030746705710102\n","Test acc:  0.6567821491485614\n","Balanced Test Score: 0.715208455666471\n","F1 Score: 0.6406394097755918 \n","\n","Train acc:  0.8030746705710102\n","Test acc:  0.6567821491485614\n","Balanced Test Score: 0.715208455666471\n","F1 Score: 0.6406394097755918 \n","\n","Train acc:  0.8030746705710102\n","Test acc:  0.6567821491485614\n","Balanced Test Score: 0.715208455666471\n","F1 Score: 0.6406394097755918 \n","\n","Train acc:  0.8030746705710102\n","Test acc:  0.6567821491485614\n","Balanced Test Score: 0.715208455666471\n","F1 Score: 0.6406394097755918 \n","\n","---------------------------------- \n","\n","\n","project_training_12.csv \n","----------------------------------\n","Train acc:  0.822108345534407\n","Test acc:  0.6388725778038755\n","Balanced Test Score: 0.6788021139166177\n","F1 Score: 0.6165835411471322 \n","\n","Train acc:  0.822108345534407\n","Test acc:  0.6388725778038755\n","Balanced Test Score: 0.6788021139166177\n","F1 Score: 0.6165835411471322 \n","\n","Train acc:  0.822108345534407\n","Test acc:  0.6388725778038755\n","Balanced Test Score: 0.6788021139166177\n","F1 Score: 0.6165835411471322 \n","\n","Train acc:  0.822108345534407\n","Test acc:  0.6388725778038755\n","Balanced Test Score: 0.6788021139166177\n","F1 Score: 0.6165835411471322 \n","\n","Train acc:  0.822108345534407\n","Test acc:  0.6388725778038755\n","Balanced Test Score: 0.6788021139166177\n","F1 Score: 0.6165835411471322 \n","\n","---------------------------------- \n","\n","\n","project_training_6.csv \n","----------------------------------\n","Train acc:  0.9868035190615836\n","Test acc:  0.6004110393423371\n","Balanced Test Score: 0.632413388138579\n","F1 Score: 0.6047051989544001 \n","\n","Train acc:  0.9868035190615836\n","Test acc:  0.6004110393423371\n","Balanced Test Score: 0.632413388138579\n","F1 Score: 0.6047051989544001 \n","\n","Train acc:  0.9868035190615836\n","Test acc:  0.6004110393423371\n","Balanced Test Score: 0.632413388138579\n","F1 Score: 0.6047051989544001 \n","\n","Train acc:  0.9868035190615836\n","Test acc:  0.6004110393423371\n","Balanced Test Score: 0.632413388138579\n","F1 Score: 0.6047051989544001 \n","\n","Train acc:  0.9868035190615836\n","Test acc:  0.6004110393423371\n","Balanced Test Score: 0.632413388138579\n","F1 Score: 0.6047051989544001 \n","\n","---------------------------------- \n","\n","\n"],"name":"stdout"}]},{"metadata":{"id":"pe9CYciKRN5v","colab_type":"text"},"cell_type":"markdown","source":["## Finished"]}]}