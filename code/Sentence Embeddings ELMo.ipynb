{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Sentence Embeddings ELMo.ipynb","version":"0.3.2","provenance":[{"file_id":"1UY3OrGvvn3mTtQucw-m-ZwuonVc2TTj_","timestamp":1552732858720},{"file_id":"12xTWTuB-j03e9HvpNweKsA1O1J5GLcig","timestamp":1552424720725},{"file_id":"1uKUp1OeC7BxVSWbdmRU-gXmvA7dHXjo5","timestamp":1552301799332},{"file_id":"1VNv0lh0SqiUNgxOGrEDNcc3EvzY0G_Ju","timestamp":1551615990477}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"rJ7PCCE7niJi","colab_type":"text"},"cell_type":"markdown","source":["## The majority of the code in this notebook is work produced by Khodak et al. (2017) and can be found at the following links: https://github.com/NLPrinceton/SARC and https://github.com/NLPrinceton/text_embedding\n","\n","## Code from these repositories has been referenced below."]},{"metadata":{"id":"PB2veJx4ZfVZ","colab_type":"text"},"cell_type":"markdown","source":["## Import libraries and data"]},{"metadata":{"id":"X_eCkXg0pW1o","colab_type":"code","outputId":"6e0aee6e-b736-44c3-8df8-c59c6191a99e","executionInfo":{"status":"ok","timestamp":1552732995439,"user_tz":0,"elapsed":102068,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":3404}},"cell_type":"code","source":["!pip install allennlp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/32/d6d0a93a23763f366df2dbd4e007e45ce4d2ad97e6315506db9da8af7731/allennlp-0.8.2-py3-none-any.whl (5.6MB)\n","\u001b[K    100% |████████████████████████████████| 5.6MB 3.1MB/s \n","\u001b[?25hCollecting unidecode (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n","\u001b[K    100% |████████████████████████████████| 245kB 25.0MB/s \n","\u001b[?25hCollecting sqlparse==0.2.4 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/65/85/20bdd72f4537cf2c4d5d005368d502b2f464ede22982e724a82c86268eda/sqlparse-0.2.4-py2.py3-none-any.whl\n","Collecting matplotlib==2.2.3 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n","\u001b[K    100% |████████████████████████████████| 12.6MB 2.7MB/s \n","\u001b[?25hCollecting gevent==1.3.6 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/3d/a19fece28ba1b5133cf74bd22a229d77b4d9cc4b24aa8f263cca2845c555/gevent-1.3.6-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n","\u001b[K    100% |████████████████████████████████| 4.5MB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.18.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n","Requirement already satisfied: spacy<2.1,>=2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0.18)\n","Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n","Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.6)\n","Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n","\u001b[K    100% |████████████████████████████████| 122kB 29.6MB/s \n","\u001b[?25hCollecting flask-cors==3.0.7 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.113)\n","Collecting ftfy (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 20.0MB/s \n","\u001b[?25hCollecting numpydoc==0.8.0 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/95/a8/b4706a6270f0475541c5c1ee3373c7a3b793936ec1f517f1a1dab4f896c0/numpydoc-0.8.0.tar.gz\n","Collecting flaky (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n","Collecting responses>=0.7 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\n","Collecting jsonnet==0.10.0; sys_platform != \"win32\" (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/83/d49904ee98dd4fbba6a003938e30e76251951c4bdb49628b4f92e5009a42/jsonnet-0.10.0.tar.gz (124kB)\n","\u001b[K    100% |████████████████████████████████| 133kB 29.3MB/s \n","\u001b[?25hCollecting pytz==2017.3 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/7f/e7d1acbd433b929168a4fb4182a2ff3c33653717195a26c1de099ad1ef29/pytz-2017.3-py2.py3-none-any.whl (511kB)\n","\u001b[K    100% |████████████████████████████████| 512kB 15.4MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.20.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.14.6)\n","Collecting awscli>=1.11.91 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/5e/e3d2b2f7c5d327f75e8f5fdef56c23f9a0983d972d35943ebbd65affb98a/awscli-1.16.125-py2.py3-none-any.whl (1.5MB)\n","\u001b[K    100% |████████████████████████████████| 1.5MB 14.5MB/s \n","\u001b[?25hCollecting tensorboardX==1.2 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/22/43f4f0318f7c68a1000dbb700a353b745584bc2397437832d15ba69ea5f1/tensorboardX-1.2-py2.py3-none-any.whl (44kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 19.7MB/s \n","\u001b[?25hRequirement already satisfied: flask==1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.2)\n","Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n","Collecting conllu==0.11 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.1.post2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n","Collecting overrides (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n","Collecting moto==1.3.4 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/8f/7b36e81ff067d0e7bf90f7210b351c0cfe6657f79fa4dcb0cb4787462e05/moto-1.3.4-py2.py3-none-any.whl (548kB)\n","\u001b[K    100% |████████████████████████████████| 552kB 25.3MB/s \n","\u001b[?25hCollecting parsimonious==0.8.0 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/4a/89/32c55944cd30dff856f16859ee325b13c83c260d0c56c0eed511e8063c87/parsimonious-0.8.0.tar.gz\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (1.0.1)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (1.11.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (2.5.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (2.3.1)\n","Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent==1.3.6->allennlp) (0.4.15)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.3.9)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.6)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.22)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2.0.2)\n","Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.2.9)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2.0.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.0.2)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.9.6)\n","Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.35)\n","Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (6.12.1)\n","Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2018.1.10)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.113 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.12.113)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n","Requirement already satisfied: sphinx>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc==0.8.0->allennlp) (1.8.5)\n","Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc==0.8.0->allennlp) (2.10)\n","Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 17.8MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n","Collecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n","Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n","Requirement already satisfied: protobuf>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.2->allennlp) (3.7.0)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->allennlp) (7.0)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->allennlp) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->allennlp) (0.14.1)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (40.8.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (6.0.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n","Collecting cryptography>=2.0.0 (from moto==1.3.4->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n","\u001b[K    100% |████████████████████████████████| 2.3MB 10.5MB/s \n","\u001b[?25hCollecting xmltodict (from moto==1.3.4->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n","Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.0.0)\n","Requirement already satisfied: boto>=2.36.0 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.49.0)\n","Collecting cookies (from moto==1.3.4->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/60/557f84aa2db629e5124aa05408b975b1b5d0e1cec16cde0bfa06aae097d3/cookies-2.2.1-py2.py3-none-any.whl (44kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 21.1MB/s \n","\u001b[?25hCollecting pyaml (from moto==1.3.4->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/c5/e1/1523fb1dab744e2c6b1f02446f2139a78726c18c062a8ddd53875abb20f8/pyaml-18.11.0-py2.py3-none-any.whl\n","Collecting aws-xray-sdk<0.96,>=0.93 (from moto==1.3.4->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl (52kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 22.5MB/s \n","\u001b[?25hCollecting python-jose<3.0.0 (from moto==1.3.4->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl\n","Collecting docker>=2.5.1 (from moto==1.3.4->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/3c/b610f22b170b0f8fe4d8f78974878e116562389f666f99e6549567eb9d87/docker-3.7.0-py2.py3-none-any.whl (133kB)\n","\u001b[K    100% |████████████████████████████████| 143kB 29.6MB/s \n","\u001b[?25hCollecting jsondiff==1.1.1 (from moto==1.3.4->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/bd/5f/13e28a2f9abeda2ffb3f44f2f809b01b52bc02cdb63816e05b8c9cbbdfc5/jsondiff-1.1.1.tar.gz\n","Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (1.10.11)\n","Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0.1)\n","Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.4.3.2)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (19.0)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (0.7.12)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.2.1)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.1.3)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc==0.8.0->allennlp) (1.1.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n","Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0.0->moto==1.3.4->allennlp) (1.12.2)\n","Collecting asn1crypto>=0.21.0 (from cryptography>=2.0.0->moto==1.3.4->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n","\u001b[K    100% |████████████████████████████████| 102kB 22.7MB/s \n","\u001b[?25hRequirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock->moto==1.3.4->allennlp) (5.1.3)\n","Collecting jsonpickle (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/dc/12/8c44eabb501e2bc0aec0dd152b328074d98a50968d3a02be28f6037f0c6a/jsonpickle-1.1-py2.py3-none-any.whl\n","Collecting ecdsa<1.0 (from python-jose<3.0.0->moto==1.3.4->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f4/73669d51825516ce8c43b816c0a6b64cd6eb71d08b99820c00792cb42222/ecdsa-0.13-py2.py3-none-any.whl (86kB)\n","\u001b[K    100% |████████████████████████████████| 92kB 27.3MB/s \n","\u001b[?25hRequirement already satisfied: future<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<3.0.0->moto==1.3.4->allennlp) (0.16.0)\n","Collecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto==1.3.4->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/cf/4b66bf1ac2484ca39599b4576d681186b61b543c2d2c29f9aa4ba3cc53b5/pycryptodome-3.7.3-cp36-cp36m-manylinux1_x86_64.whl (7.5MB)\n","\u001b[K    100% |████████████████████████████████| 7.5MB 4.2MB/s \n","\u001b[?25hCollecting websocket-client>=0.32.0 (from docker>=2.5.1->moto==1.3.4->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/54/684db2ba1b7a203602808446b8686ee786f93b4a7e080cdc440cc7e06e56/websocket_client-0.55.0-py2.py3-none-any.whl (200kB)\n","\u001b[K    100% |████████████████████████████████| 204kB 25.4MB/s \n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto==1.3.4->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0.0->moto==1.3.4->allennlp) (2.19)\n","Building wheels for collected packages: numpydoc, jsonnet, overrides, parsimonious, jsondiff\n","  Building wheel for numpydoc (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/55/7f/3e25d754760ccd62d6796e5b2cfe25629346f52ea00753d549\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a0/aa/f0/b4ab8854cf00f922a87787425cfbb789aac01ab2c2cd1b4ca4\n","  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n","  Building wheel for parsimonious (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/bb/51/82/ae9b22a790f11e7be918939d01aa397c545ebb3723453c5fb4\n","  Building wheel for jsondiff (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/68/08/07/69d839606fb7fdc778fa86476abc0a864693d45969a0c1936c\n","Successfully built numpydoc jsonnet overrides parsimonious jsondiff\n","\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n","\u001b[31mfastai 1.0.48 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n","\u001b[31mawscli 1.16.125 has requirement botocore==1.12.115, but you'll have botocore 1.12.113 which is incompatible.\u001b[0m\n","Installing collected packages: unidecode, sqlparse, pytz, matplotlib, gevent, pytorch-pretrained-bert, flask-cors, ftfy, numpydoc, flaky, responses, jsonnet, rsa, colorama, awscli, tensorboardX, conllu, overrides, asn1crypto, cryptography, xmltodict, cookies, pyaml, jsonpickle, aws-xray-sdk, ecdsa, pycryptodome, python-jose, websocket-client, docker-pycreds, docker, jsondiff, moto, parsimonious, allennlp\n","  Found existing installation: sqlparse 0.3.0\n","    Uninstalling sqlparse-0.3.0:\n","      Successfully uninstalled sqlparse-0.3.0\n","  Found existing installation: pytz 2018.9\n","    Uninstalling pytz-2018.9:\n","      Successfully uninstalled pytz-2018.9\n","  Found existing installation: matplotlib 3.0.3\n","    Uninstalling matplotlib-3.0.3:\n","      Successfully uninstalled matplotlib-3.0.3\n","  Found existing installation: gevent 1.4.0\n","    Uninstalling gevent-1.4.0:\n","      Successfully uninstalled gevent-1.4.0\n","  Found existing installation: rsa 4.0\n","    Uninstalling rsa-4.0:\n","      Successfully uninstalled rsa-4.0\n","Successfully installed allennlp-0.8.2 asn1crypto-0.24.0 aws-xray-sdk-0.95 awscli-1.16.125 colorama-0.3.9 conllu-0.11 cookies-2.2.1 cryptography-2.6.1 docker-3.7.0 docker-pycreds-0.4.0 ecdsa-0.13 flaky-3.5.3 flask-cors-3.0.7 ftfy-5.5.1 gevent-1.3.6 jsondiff-1.1.1 jsonnet-0.10.0 jsonpickle-1.1 matplotlib-2.2.3 moto-1.3.4 numpydoc-0.8.0 overrides-1.9 parsimonious-0.8.0 pyaml-18.11.0 pycryptodome-3.7.3 python-jose-2.0.2 pytorch-pretrained-bert-0.6.1 pytz-2017.3 responses-0.10.6 rsa-3.4.2 sqlparse-0.2.4 tensorboardX-1.2 unidecode-1.0.23 websocket-client-0.55.0 xmltodict-0.12.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits","pytz","rsa"]}}},"metadata":{"tags":[]}}]},{"metadata":{"id":"g8o0dxPrZfVd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"24d548ac-20d9-41f7-a596-b630f67d283f","executionInfo":{"status":"ok","timestamp":1552733000286,"user_tz":0,"elapsed":106908,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import csv\n","import json\n","from sklearn.model_selection import train_test_split\n","import pickle\n","\n","import argparse\n","import nltk\n","from sklearn.linear_model import LogisticRegressionCV as LogitCV\n","from sklearn.preprocessing import normalize\n","from sklearn.metrics import f1_score\n","\n","from collections import Counter\n","from itertools import chain\n","from itertools import groupby\n","from operator import itemgetter\n","#from string import punctuation\n","from unicodedata import category\n","import nltk\n","import numpy as np\n","from scipy import sparse as sp\n","\n","from allennlp.commands.elmo import ElmoEmbedder"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"],"name":"stdout"}]},{"metadata":{"id":"gArFUBK2g8sh","colab_type":"code","outputId":"e194b606-0170-43ab-a405-1f38ae3fbc44","executionInfo":{"status":"ok","timestamp":1552733146597,"user_tz":0,"elapsed":253214,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":189}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","SARC_POL = '/content/gdrive/My Drive/SARC pol/project_data/'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"0xOj9eUNZfVk","colab_type":"code","colab":{}},"cell_type":"code","source":["############################################ Khodak et al. 2017\n","\n","def tokenize(documents):\n","  '''tokenizes documents\n","  Args:\n","    documents: iterable of strings\n","  Returns:\n","    list of list of strings\n","  '''\n","\n","  return [list(split_on_punctuation(doc)) for doc in documents]\n","\n","###############################################################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JRyd0lwMZfVo","colab_type":"code","colab":{}},"cell_type":"code","source":["############################################ Khodak et al. 2017\n","\n","#PUNCTUATION = set(punctuation)\n","PUNCTUATION = {'M', 'P', 'S'}\n","UINT = np.uint16\n","from unicodedata import category\n","\n","def split_on_punctuation(document):\n","  '''tokenizes string by splitting on spaces and punctuation\n","  Args:\n","    document: string\n","  Returns:\n","    str generator\n","  '''\n","\n","  for token in document.split():\n","    if len(token) == 1:\n","      yield token\n","    else:\n","      chunk = token[0]\n","      for char0, char1 in zip(token[:-1], token[1:]):\n","        #if (char0 in PUNCTUATION) == (char1 in PUNCTUATION):\n","        if (category(char0)[0] in PUNCTUATION) == (category(char1)[0] in PUNCTUATION):\n","          chunk += char1\n","        else:\n","          yield chunk\n","          chunk = char1\n","      if chunk:\n","        yield chunk\n","\n","###############################################################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6yTdpsvUZfVr","colab_type":"code","colab":{}},"cell_type":"code","source":["############################################ Khodak et al. 2017\n","\n","def feature_counts(documents):\n","  '''computes feature counts from featurized documents\n","  Args:\n","    documents: iterable of lists of hashable features\n","  Returns:\n","    dict mapping features to counts\n","  '''\n","\n","  return Counter(feat for doc in documents for feat in doc)\n","\n","\n","def feature_vocab(documents, min_count=1, sorted_features=sorted):\n","  '''gets feature vocabulary from featurized documents\n","  Args:\n","    documents: iterable of lists of hashable features\n","    min_count: minimum number of times feature must appear to be included in the vocabulary\n","    sorted_features: function that sorts the features\n","  Returns:\n","    {feature: index} dict\n","  '''\n","  \n","  return {feat: i for i, feat in enumerate(sorted_features(feat for feat, count in feature_counts(documents).items() if count >= min_count))}\n","\n","\n","def docs2bofs(documents, vocabulary=None, weights=None, default=1.0, format='csr', **kwargs):\n","  '''constructs sparse BoF representations from featurized documents\n","  Args:\n","    documents: iterable of lists of hashable features\n","    vocabulary: dict mapping features to indices (nonnegative ints) or a list of features; if None will compute automatically from documents\n","    weights: dict mapping features to weights (floats) or a list/np.ndarray of weights; if None will compute unweighted BoFs\n","    default: default feature weight if not feature in weights; ignored if weights is None\n","    format: sparse matrix format\n","    kwargs: passed to feature_vocab; ignored if not vocabulary is None\n","  Returns:\n","    sparse BoF matrix in CSR format of size (len(documents), len(vocabulary))\n","  '''\n","\n","  if vocabulary is None:\n","    vocabulary = feature_vocab(documents, **kwargs)\n","  elif type(vocabulary) == list:\n","    vocabulary = {feat: i for i, feat in enumerate(vocabulary)}\n","\n","  rows, cols, values = zip(*((row, col, count) for (row, col), count in Counter((i, vocabulary.get(feat, -1)) for i, doc in enumerate(documents) for feat in doc).items() if not col==-1))\n","  m = len(documents)\n","  V = len(vocabulary)\n","  if weights is None:\n","    return sp.coo_matrix((values, (rows, cols)), shape=(m, V), dtype=UINT).asformat(format)\n","  bofs = sp.coo_matrix((values, (rows, cols)), shape=(m, V)).tocsr()\n","\n","  if type(weights) == dict:\n","    diag = np.empty(V)\n","    for feat, i in vocabulary.items():\n","      diag[i] = weights.gets(feat, default)\n","  else:\n","    assert len(weights) == V, \"if weights passed as a list/np.ndarray, length must be same as vocabulary size\"\n","    if type(weights) == list:\n","      diag = np.array(weights)\n","    else:\n","      diag = weights\n","  return bofs.dot(sp.diags(diag, 0)).asformat(format)\n","\n","###############################################################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A8n47210ayue","colab_type":"code","colab":{}},"cell_type":"code","source":["############################################ Khodak et al. 2017\n","import numpy as np\n","from numpy.linalg import norm\n","from scipy.linalg import svd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import normalize\n","\n","\n","FLOAT = np.float32\n","# NOTE: filepath for Common Crawl GloVe embeddings goes here\n","CCGLOVE = \"/content/gdrive/My Drive/SARC pol/embeddings/glove/amazon_glove1600.txt\"\n","\n","\n","# NOTE: Some files have 2d or 2d+2 numbers on each line, with the last d of them being meaningless; avoid loading them by setting dimension=d\n","def load(vectorfile, vocabulary=None, dimension=None):\n","  '''generates word embeddings from file\n","  Args:\n","    vectorfile: word embedding text file or HDF5 file with keys 'words' and 'vectors'\n","    vocabulary: dict/set of strings, or int specifying number of words to load; if None loads all words from file\n","    dimension: number of dimensions to load\n","  Returns:\n","    (word, vector) generator\n","  '''\n","\n","  try:\n","    f = h5py.File(vectorfile, 'r')\n","    words, vectors = np.array(f['words']), np.array(f['vectors'])\n","    for word, vector in zip(words, vectors):\n","      if vocabulary is None or word in vocabulary:\n","        yield word, vector\n","    f.close()\n","\n","  except OSError:\n","    if vocabulary is None:\n","      V = float('inf')\n","    elif type(vocabulary) == int:\n","      V = vocabulary\n","      vocabulary = None\n","    else:\n","      V = len(vocabulary)\n","    dimension = -1 if dimension is None else dimension\n","\n","    with open(vectorfile, 'r') as f:\n","      n = 0\n","      for line in f:\n","        index = line.index(' ')\n","        word = line[:index]\n","        if vocabulary is None or word in vocabulary:\n","          yield word, np.fromstring(line[index+1:], dtype=FLOAT, count=dimension, sep=' ')\n","          n += 1\n","        if n == V:\n","          break\n","\n","\n","def text2hdf5(textfile, hdf5file, **kwargs):\n","  '''converts word embeddings file from text to HDF5 format\n","  Args:\n","      textfile: word embeddings file in format \"word float ... float\\n\"\n","      hdf5file: output file ; will have keys 'words' and 'vectors'\n","      kwargs: passed to load\n","  Returns:\n","      None\n","  '''\n","\n","  words, vectors = zip(*load(textfile, **kwargs))\n","  f = h5py.File(hdf5file)\n","  f.create_dataset('words', (len(words),), dtype=h5py.special_dtype(vlen=str))\n","  for i, word in enumerate(words):\n","      f['words'][i] = word\n","  f.create_dataset('vectors', data=np.vstack(vectors))\n","  f.close()\n","\n","\n","def vocab2mat(vocabulary=None, random=None, vectorfile=CCGLOVE, dimension=None, unit=True):\n","  '''constructs matrix of word vectors\n","  Args:\n","    vocabulary: dict mapping strings to indices, or iterable of strings, or int specifying vocab size; if None loads all words in vectorfile\n","    random: type ('Gaussian' or 'Rademacher') of random vectors to use; if None uses pretrained vectors; if tuple (low, high) uses uniform distribution over [low, high)\n","    vectorfile: word embedding text file; ignored if not random is None\n","    dimension: embedding dimension\n","    unit: normalize embeddings\n","  Returns:\n","    numpy matrix of size (len(vocabulary), dimension)\n","  '''\n","\n","  assert random is None or not vocabulary is None, \"needs vocabulary size information for random vectors\"\n","  assert random is None or not dimension is None, \"needs dimension information for random vectors\"\n","\n","  if random is None:\n","\n","    if type(vocabulary) == set:\n","      vocabulary = sorted(vocabulary)\n","    if type(vocabulary) == list:\n","      vocabulary = {word: i for i, word in enumerate(vocabulary)}\n","    if type(vocabulary) == dict:\n","      matrix = np.zeros((len(vocabulary), dimension), dtype=FLOAT)\n","      for word, vector in load(vectorfile, vocabulary, dimension):\n","        matrix[vocabulary[word]] = vector\n","    else:\n","      matrix = np.vstack(vector for _, vector in load(vectorfile, vocabulary, dimension))\n","  \n","  else:\n","\n","    if not type(vocabulary) == int:\n","      vocabulary = len(vocabulary)\n","    if type(random) == tuple:\n","      return np.random.uniform(*random, size=(vocabulary, dimension)).astype(FLOAT)\n","    elif random.lower() == 'gaussian':\n","      matrix = np.random.normal(scale=1.0/np.sqrt(dimension), size=(vocabulary, dimension)).astype(FLOAT)\n","    elif random.lower() == 'rademacher':\n","      return (2.0*np.random.randint(2, size=(vocabulary, dimension)).astype(FLOAT)-1.0)/np.sqrt(dimension)\n","    else:\n","      raise(NotImplementedError)\n","\n","  if unit:\n","    return normalize(matrix)\n","  return matrix\n","\n","\n","def vocab2vecs(vocabulary=None, random=None, vectorfile=CCGLOVE, dimension=None, unit=True):\n","  '''constructs dict mapping words to vectors\n","  Args:\n","    vocabulary: iterable of strings, or int specifying vocab size; if None loads all words in vectorfile\n","    random: type ('Gaussian' or 'Rademacher') of random vectors to use; if None uses pretrained vectors\n","    vectorfile: word embedding text file; ignored if not random is None\n","    dimension: embedding dimension\n","    unit: normalize embeddings\n","  Returns:\n","    {word: vector} dict; words not in vectorfile are not included\n","  '''\n","\n","  assert random is None or not (vocabulary is None or type(vocabulary) == int), \"needs word information for random vectors\"\n","\n","  if random is None:\n","    if unit:\n","      return {word: vector/norm(vector) for word, vector in load(vectorfile, vocabulary, dimension)}\n","    return dict(load(vectorfile, vocabulary, dimension))\n","  return dict(zip(vocabulary, vocab2mat(vocabulary, random=random, dimension=dimension, unit=unit)))\n","\n","\n","def docs2vecs(documents, f2v=None, weights=None, default=1.0, avg=False, **kwargs):\n","  '''computes document embeddings from documents\n","  Args:\n","    documents: iterable of lists of hashable features\n","    f2v: dict mapping features to vectors; if None will compute this using vocab2vecs\n","    weights: dict mapping features to weights; unweighted if None\n","    default: default weight to assign if feature not in weights; ignored if weights is None\n","    avg: divide embeddings by the document length\n","    kwargs: passed to vocab2vecs; ignored if not f2v is None\n","  Returns:\n","    matrix of size (len(documents), dimension)\n","  '''\n","\n","  if f2v is None:\n","    f2v = vocab2vecs({word for document in documents for word in documents}, **kwargs)\n","    dimension = kwargs.get('dimension', 300)\n","  else:\n","    dimensions = {v.shape for v in f2v.values()}\n","    assert len(dimensions) == 1, \"all feature vectors must have same dimension\"\n","    dimension = dimensions.pop()\n","  if not weights is None:\n","    f2v = {feat: weights.get(feat, default)*vec for feat, vec in f2v.items()}\n","    \n","  z = np.zeros(dimension, dtype=FLOAT)\n","  if avg:\n","    return np.vstack(sum((f2v.get(feat, z) for feat in document), z) / max(1.0, len(document)) for document in documents)\n","  return np.vstack(sum((f2v.get(feat, z) for feat in document), z) for document in documents)\n","\n","\n","class OrthogonalProcrustes:\n","  '''sklearn-style class for solving the Orthogonal Procrustes problem\n","  '''\n","\n","  def __init__(self, fit_intercept=False):\n","    '''initializes object\n","    Args:\n","      fit_intercept: whether to find best transformation after translation\n","    Returns:\n","      None\n","    '''\n","\n","    self.fit_intercept = fit_intercept\n","\n","  def fit(self, X, Y):\n","    '''finds orthogonal matrix M minimizing |XM^T-Y|\n","    Args:\n","      X: numpy array of shape (n, d)\n","      Y: numpy array of shape (n, d)\n","    Returns:\n","      self (with attribute coef_, a numpy array of shape (d, d)\n","    '''\n","\n","    if self.fit_intercept:\n","      Xbar, Ybar = np.mean(X, axis=0), np.mean(Y, axis=0)\n","      X, Y = X-Xbar, Y-Ybar\n","    U, _, VT = svd(Y.T.dot(X))\n","    self.coef_ = U.dot(VT)\n","    if self.fit_intercept:\n","      self.intercept_ = Ybar - self.coef_.dot(Xbar)\n","    else:\n","      self.intercept_ = np.zeros(self.coef_.shape[0], dtype=self.coef_.dtype)\n","    return self\n","\n","\n","def align_vocab(func):\n","  '''wrapper to align vocab to allow word-to-vector dict inputs to functions taking two word-vector matrices as inputs\n","  '''\n","\n","  def wrapper(X, Y, **kwargs):\n","    assert type(X) == type(Y), \"first two arguments must be the same type\"\n","    if type(X) == dict:\n","      vocab = sorted(set(X.keys()).intersection(Y.keys()))\n","      X = np.vstack(X[w] for w in vocab)\n","      Y = np.vstack(Y[w] for w in vocab)\n","    else:\n","      assert type(X) == np.ndarray, \"first two arguments must be 'dict' or 'numpy.ndarray'\"\n","    return func(X, Y, **kwargs)\n","\n","  return wrapper\n","\n","\n","@align_vocab\n","def best_transform(source, target, orthogonal=True, fit_intercept=False):\n","  '''computes best matrix between two sets of word embeddings in terms of least-squares error\n","  Args:\n","    source: numpy array of size (len(vocabulary), dimension) or dict mapping words to vectors; must be same type as target\n","    target: numpy array of size (len(vocabulary), dimension) or dict mapping words to vectors; must be same type as source\n","    orthogonal: if True constrains best transform to be orthogonal\n","    fit_intercept: whether to find best transformation after translation\n","  Returns:\n","    numpy array of size (dimension, dimension)\n","  '''\n","\n","  if orthogonal:\n","    transform = OrthogonalProcrustes(fit_intercept=fit_intercept).fit(source, target)\n","  else:\n","    transform = LinearRegression(fit_intercept=fit_intercept).fit(source, target)\n","    if not fit_intercept:\n","      transform.intercept_ = np.zeros(target.shape[1])\n","  return transform.coef_.astype(target.dtype), transform.intercept_.astype(target.dtype)\n","\n","\n","@align_vocab\n","def average_cosine_similarity(X, Y):\n","  '''computes the average cosine similarity between two sets of word embeddings\n","  Args:\n","    X: numpy array of size (len(vocabulary), dimension) or dict mapping words to vectors; must be same type as target\n","    Y: numpy array of size (len(vocabulary), dimension) or dict mapping words to vectors; must be same type as source\n","  Returns:\n","    average cosine similarity as a float\n","  '''\n","\n","  return np.mean((normalize(X) * normalize(Y)).sum(1))\n","\n","###############################################################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Up1_Hj3srJan","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_elmo_embeddings(elmo, tokens):\n","\n","    embeddings = []\n","    \n","    for elmo_embedding in elmo.embed_sentences(tokens):  \n","        # Mean pool the 3 layers returned from ELMo\n","        avg_elmo_embedding = np.average(elmo_embedding, axis=0)\n","        \n","        #Mean pool over the words\n","        avg_elmo_embedding = np.average(avg_elmo_embedding, axis=0)\n","             \n","        embeddings.append(avg_elmo_embedding)        \n","            \n","    return np.array(embeddings)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yYpXNBrMrJTA","colab_type":"code","colab":{}},"cell_type":"code","source":["CUDA_VISIBLE_DEVICES = 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I3LMBpHtrOvD","colab_type":"code","outputId":"64dae2c3-6496-43d8-99c0-8f5506b88a90","executionInfo":{"status":"ok","timestamp":1552733205558,"user_tz":0,"elapsed":43410,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["elmo = ElmoEmbedder(cuda_device=0)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["100%|██████████| 336/336 [00:00<00:00, 31821.67B/s]\n","100%|██████████| 374434792/374434792 [00:14<00:00, 26421775.04B/s]\n"],"name":"stderr"}]},{"metadata":{"id":"2BSGM1-YpdDa","colab_type":"text"},"cell_type":"markdown","source":["## Let's go"]},{"metadata":{"id":"sgbYmGDxrph1","colab_type":"text"},"cell_type":"markdown","source":["## Embed the test set once"]},{"metadata":{"id":"ArtlyEIWriIg","colab_type":"code","colab":{}},"cell_type":"code","source":["testdf = pd.read_csv(SARC_POL+'balanced_test.csv', index_col = 0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5TU_LzwiriBs","colab_type":"code","colab":{}},"cell_type":"code","source":["test_resp = testdf['response'].values.tolist()\n","test_labels = testdf['label'].values.tolist()\n","\n","# Test resp\n","first_resp = []\n","second_resp = []\n","for idx, resp in enumerate(test_resp):\n","\n","  if idx % 2 == 0:\n","    first_resp.append(resp)\n","  else:\n","    second_resp.append(resp)\n","\n","test_docs = {0: first_resp, 1: second_resp}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GPT4pW_0ry2U","colab_type":"code","colab":{}},"cell_type":"code","source":["# Test labels\n","first_label = []\n","second_label = []\n","for idx, label in enumerate(test_labels):\n","\n","  if idx % 2 == 0:\n","    first_label.append(label)\n","  else:\n","    second_label.append(label)\n","\n","test_labels = {0: first_label, 1: second_label}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_uygMEq8r2Sc","colab_type":"code","colab":{}},"cell_type":"code","source":["test_all_docs_tok = tokenize(test_docs[0] + test_docs[1])\n","test_all_labels = np.array(test_labels[0] + test_labels[1])\n","#test_all_vecs = get_elmo_embeddings(elmo, test_all_docs_tok)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bAVz6yMCr2Pr","colab_type":"code","colab":{}},"cell_type":"code","source":["#with open(\"/content/gdrive/My Drive/SARC pol/elmo/testembeddings.pickle\", 'wb') as handle:\n","#  pickle.dump(test_all_vecs, handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QfUU_WycwCef","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(\"/content/gdrive/My Drive/SARC pol/elmo/testembeddings.pickle\", 'rb') as handle:\n","  test_all_vecs = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4wo64YWepYs6","colab_type":"text"},"cell_type":"markdown","source":["### 100% project training set"]},{"metadata":{"id":"nGHO_YZbpKP1","colab_type":"code","outputId":"50b0aa59-7a27-41eb-9022-2469a8ddc020","executionInfo":{"status":"ok","timestamp":1552734935323,"user_tz":0,"elapsed":663545,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["filename =  ['project_training_100.csv']\n","\n","record_100 = pd.DataFrame()\n","\n","#Load in the test set\n","validdf = pd.read_csv(SARC_POL+'balanced_test.csv', index_col = 0)\n","\n","for fname in filename:\n","  \n","  #Load in the training set\n","  traindf = pd.read_csv(SARC_POL+fname, index_col = 0)\n","  \n","  # Only use responses for this method. Ignore ancestors.\n","  train_resp = traindf['response'].values.tolist()\n","  \n","  train_labels = traindf['label'].values.tolist()\n","  \n","\n","  # Train resp\n","  first_resp = []\n","  second_resp = []\n","  for idx, resp in enumerate(train_resp):\n","\n","    if idx % 2 == 0:\n","      first_resp.append(resp)\n","    else:\n","      second_resp.append(resp)\n","\n","  train_docs = {0: first_resp, 1: second_resp}\n","\n","  # Train labels\n","  first_label = []\n","  second_label = []\n","  for idx, label in enumerate(train_labels):\n","\n","    if idx % 2 == 0:\n","      first_label.append(label)\n","    else:\n","      second_label.append(label)\n","\n","  train_labels = {0: first_label, 1: second_label}\n","\n","  # Train a classifier on all responses in training data. We will later use this\n","  # classifier to determine for every sequence which of the 2 responses is more sarcastic.\n","  train_all_docs_tok = tokenize(train_docs[0] + train_docs[1])\n","\n","  train_all_labels = np.array(train_labels[0] + train_labels[1])\n","\n","  print(fname, \"\\n----------------------------------\")\n","    \n","  for seed in [45, 89, 123, 9, 54]:\n","    \n","    train_all_vecs = get_elmo_embeddings(elmo, train_all_docs_tok)\n","    \n","    #with open(f\"/content/gdrive/My Drive/SARC pol/elmo/100embeddings_{seed}.pickle\", 'wb') as handle:\n","    #  pickle.dump(train_all_vecs, handle)\n","\n","  ############################################ Khodak et al. 2017\n","      \n","    # Evaluate this classifier on all responses.\n","#     print('Evaluate the classifier on all responses')\n","    clf = LogitCV(Cs=[10**i for i in range(-2, 3)], fit_intercept=False, cv=2, dual=np.less(*train_all_vecs.shape), solver='liblinear', n_jobs=-1, random_state=seed) \n","    clf.fit(train_all_vecs, train_all_labels)\n","    #print('Train acc: ', clf.score(train_all_vecs, train_all_labels))\n","    test_acc = clf.score(test_all_vecs, test_all_labels)\n","    #print('Test acc: ', test_acc )\n","    \n","  ############################################  \n","  \n","    # Balanced Test Score Calculation\n","    nAncestors = len(test_labels[0])\n","    countCorrect = 0\n","    for i in range(nAncestors):\n","      \n","      scoreResponse0 = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0,1]\n","      scoreResponse1 = clf.predict_proba(test_all_vecs[i+nAncestors].reshape(1,-1))[0,1]\n","     \n","      if scoreResponse0 > scoreResponse1 and test_labels[0][i] == 1:\n","        countCorrect += 1\n","        \n","      elif scoreResponse1 > scoreResponse0 and test_labels[1][i] == 1:\n","        countCorrect += 1\n","       \n","    bal_test_score =  countCorrect/nAncestors\n","    #print(\"Balanced Test Score:\", bal_test_score )\n","    \n","    # F1 Prediction\n","    \n","    y_pred = []\n","    y_true = []\n","    \n","    for i in range(len(test_labels[0])+len(test_labels[1])):\n","      \n","      scoreResponse = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0][1]\n","      \n","      if i < nAncestors:\n","        \n","        y_true.append(test_labels[0][i])\n","        \n","      else:\n","        \n","        y_true.append(test_labels[1][i-nAncestors])\n","      \n","      if scoreResponse > 0.5:\n","        y_pred.append(1)\n","        \n","      else:\n","        y_pred.append(0)\n","       \n","    f1Score = f1_score(y_true, y_pred, pos_label=1, average='binary', sample_weight=None)\n","    #print(\"F1 Score:\", f1Score)\n","    \n","    #Store the results\n","    record_100 = record_100.append({'Acc (bal, bal)': bal_test_score, 'Acc (bal, reg)':test_acc , 'F1 (bal, reg)': f1Score}, ignore_index=True)\n","    \n","print(record_100)\n","\n","with open(\"/content/gdrive/My Drive/SARC pol/elmo/test100.pickle\", 'wb') as handle:\n","  pickle.dump(record_100, handle)\n","  "],"execution_count":17,"outputs":[{"output_type":"stream","text":["project_training_100.csv \n","----------------------------------\n","   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.745156        0.682913       0.678188\n","1        0.745156        0.682913       0.678188\n","2        0.745156        0.682913       0.678188\n","3        0.745156        0.682913       0.678188\n","4        0.745156        0.682913       0.678188\n"],"name":"stdout"}]},{"metadata":{"id":"M4cGRwPGpWI8","colab_type":"text"},"cell_type":"markdown","source":["### 50% project training set"]},{"metadata":{"id":"nrsB5w1lpKNt","colab_type":"code","outputId":"5ed74781-ecb3-49a9-aac5-63fadc9a0718","executionInfo":{"status":"ok","timestamp":1552473685889,"user_tz":0,"elapsed":363692,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["filename =  ['project_training_50.csv']\n","\n","record_50 = pd.DataFrame()\n","\n","for fname in filename:\n","  \n","  #Load in the training set\n","  traindf = pd.read_csv(SARC_POL+fname, index_col = 0)\n","  \n","  # Only use responses for this method. Ignore ancestors.\n","  train_resp = traindf['response'].values.tolist()\n","  \n","  train_labels = traindf['label'].values.tolist()\n","  \n","\n","  # Train resp\n","  first_resp = []\n","  second_resp = []\n","  for idx, resp in enumerate(train_resp):\n","\n","    if idx % 2 == 0:\n","      first_resp.append(resp)\n","    else:\n","      second_resp.append(resp)\n","\n","  train_docs = {0: first_resp, 1: second_resp}\n","\n","  # Train labels\n","  first_label = []\n","  second_label = []\n","  for idx, label in enumerate(train_labels):\n","\n","    if idx % 2 == 0:\n","      first_label.append(label)\n","    else:\n","      second_label.append(label)\n","\n","  train_labels = {0: first_label, 1: second_label}\n","\n","  ############################################ Khodak et al. 2017\n","  \n","  # Train a classifier on all responses in training data. We will later use this\n","  # classifier to determine for every sequence which of the 2 responses is more sarcastic.\n","  train_all_docs_tok = tokenize(train_docs[0] + train_docs[1])\n","\n","  train_all_labels = np.array(train_labels[0] + train_labels[1])\n","\n","  print(fname, \"\\n----------------------------------\")\n","    \n","  for seed in [1, 2, 3, 4, 5]:\n","    \n","    train_all_vecs = get_elmo_embeddings(elmo, train_all_docs_tok)\n","    \n","    with open(f\"/content/gdrive/My Drive/SARC pol/elmo/50embeddings_{seed}.pickle\", 'wb') as handle:\n","      pickle.dump(train_all_vecs, handle)\n","\n","    # Evaluate this classifier on all responses.\n","#     print('Evaluate the classifier on all responses')\n","    clf = LogitCV(Cs=[10**i for i in range(-2, 3)], fit_intercept=False, cv=2, dual=np.less(*train_all_vecs.shape), solver='liblinear', n_jobs=-1, random_state=seed) \n","    clf.fit(train_all_vecs, train_all_labels)\n","    #print('Train acc: ', clf.score(train_all_vecs, train_all_labels))\n","    test_acc = clf.score(test_all_vecs, test_all_labels)\n","    #print('Test acc: ', test_acc )\n","    \n","    ############################################\n","    \n","    # Balanced Test Score Calculation\n","    nAncestors = len(test_labels[0])\n","    countCorrect = 0\n","    for i in range(nAncestors):\n","      \n","      scoreResponse0 = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0,1]\n","      scoreResponse1 = clf.predict_proba(test_all_vecs[i+nAncestors].reshape(1,-1))[0,1]\n","     \n","      if scoreResponse0 > scoreResponse1 and test_labels[0][i] == 1:\n","        countCorrect += 1\n","        \n","      elif scoreResponse1 > scoreResponse0 and test_labels[1][i] == 1:\n","        countCorrect += 1\n","       \n","    bal_test_score =  countCorrect/nAncestors\n","    #print(\"Balanced Test Score:\", bal_test_score )\n","    \n","    # F1 Prediction\n","    \n","    y_pred = []\n","    y_true = []\n","    \n","    for i in range(len(test_labels[0])+len(test_labels[1])):\n","      \n","      scoreResponse = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0][1]\n","      \n","      if i < nAncestors:\n","        \n","        y_true.append(test_labels[0][i])\n","        \n","      else:\n","        \n","        y_true.append(test_labels[1][i-nAncestors])\n","      \n","      if scoreResponse > 0.5:\n","        y_pred.append(1)\n","        \n","      else:\n","        y_pred.append(0)\n","       \n","    f1Score = f1_score(y_true, y_pred, pos_label=1, average='binary', sample_weight=None)\n","    #print(\"F1 Score:\", f1Score)\n","    \n","    #Store the results\n","    record_50 = record_50.append({'Acc (bal, bal)': bal_test_score, 'Acc (bal, reg)':test_acc , 'F1 (bal, reg)': f1Score}, ignore_index=True)\n","    \n","print(record_50)\n","\n","with open(\"/content/gdrive/My Drive/SARC pol/elmo/test50.pickle\", 'wb') as handle:\n","  pickle.dump(record_50, handle)\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["project_training_50.csv \n","----------------------------------\n","   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.729888         0.66559       0.664901\n","1        0.729888         0.66559       0.664901\n","2        0.729888         0.66559       0.664901\n","3        0.729888         0.66559       0.664901\n","4        0.729888         0.66559       0.664901\n"],"name":"stdout"}]},{"metadata":{"id":"hUSqIhNRpS0y","colab_type":"text"},"cell_type":"markdown","source":["### 25% project training set"]},{"metadata":{"id":"7s9GA5vcpKLX","colab_type":"code","outputId":"d5725063-0cbc-45f9-ebda-67dc394fea68","executionInfo":{"status":"ok","timestamp":1552425570010,"user_tz":0,"elapsed":189973,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["filename =  ['project_training_25.csv']\n","\n","record_25 = pd.DataFrame()\n","\n","for fname in filename:\n","  \n","  #Load in the training set\n","  traindf = pd.read_csv(SARC_POL+fname, index_col = 0)\n","  \n","  # Only use responses for this method. Ignore ancestors.\n","  train_resp = traindf['response'].values.tolist()\n","  \n","  train_labels = traindf['label'].values.tolist()\n","  \n","\n","  # Train resp\n","  first_resp = []\n","  second_resp = []\n","  for idx, resp in enumerate(train_resp):\n","\n","    if idx % 2 == 0:\n","      first_resp.append(resp)\n","    else:\n","      second_resp.append(resp)\n","\n","  train_docs = {0: first_resp, 1: second_resp}\n","\n","  # Train labels\n","  first_label = []\n","  second_label = []\n","  for idx, label in enumerate(train_labels):\n","\n","    if idx % 2 == 0:\n","      first_label.append(label)\n","    else:\n","      second_label.append(label)\n","\n","  train_labels = {0: first_label, 1: second_label}\n","  \n","  ############################################ Khodak et al. 2017\n","  \n","  # Train a classifier on all responses in training data. We will later use this\n","  # classifier to determine for every sequence which of the 2 responses is more sarcastic.\n","  train_all_docs_tok = tokenize(train_docs[0] + train_docs[1])\n","\n","  train_all_labels = np.array(train_labels[0] + train_labels[1])\n","\n","  print(fname, \"\\n----------------------------------\")\n","    \n","  for seed in [1, 2, 3, 4, 5]:\n","    \n","    train_all_vecs = get_elmo_embeddings(elmo, train_all_docs_tok)\n","    \n","    with open(f\"/content/gdrive/My Drive/SARC pol/elmo/25embeddings_{seed}.pickle\", 'wb') as handle:\n","      pickle.dump(train_all_vecs, handle)\n","\n","    # Evaluate this classifier on all responses.\n","#     print('Evaluate the classifier on all responses')\n","    clf = LogitCV(Cs=[10**i for i in range(-2, 3)], fit_intercept=False, cv=2, dual=np.less(*train_all_vecs.shape), solver='liblinear', n_jobs=-1, random_state=seed) \n","    clf.fit(train_all_vecs, train_all_labels)\n","    #print('Train acc: ', clf.score(train_all_vecs, train_all_labels))\n","    test_acc = clf.score(test_all_vecs, test_all_labels)\n","    #print('Test acc: ', test_acc )\n","     \n","   ############################################   \n","   \n","    # Balanced Test Score Calculation\n","    nAncestors = len(test_labels[0])\n","    countCorrect = 0\n","    for i in range(nAncestors):\n","      \n","      scoreResponse0 = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0,1]\n","      scoreResponse1 = clf.predict_proba(test_all_vecs[i+nAncestors].reshape(1,-1))[0,1]\n","     \n","      if scoreResponse0 > scoreResponse1 and test_labels[0][i] == 1:\n","        countCorrect += 1\n","        \n","      elif scoreResponse1 > scoreResponse0 and test_labels[1][i] == 1:\n","        countCorrect += 1\n","       \n","    bal_test_score =  countCorrect/nAncestors\n","    #print(\"Balanced Test Score:\", bal_test_score )\n","    \n","    # F1 Prediction\n","    \n","    y_pred = []\n","    y_true = []\n","    \n","    for i in range(len(test_labels[0])+len(test_labels[1])):\n","      \n","      scoreResponse = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0][1]\n","      \n","      if i < nAncestors:\n","        \n","        y_true.append(test_labels[0][i])\n","        \n","      else:\n","        \n","        y_true.append(test_labels[1][i-nAncestors])\n","      \n","      if scoreResponse > 0.5:\n","        y_pred.append(1)\n","        \n","      else:\n","        y_pred.append(0)\n","       \n","    f1Score = f1_score(y_true, y_pred, pos_label=1, average='binary', sample_weight=None)\n","    #print(\"F1 Score:\", f1Score)\n","    \n","    #Store the results\n","    record_25 = record_25.append({'Acc (bal, bal)': bal_test_score, 'Acc (bal, reg)':test_acc , 'F1 (bal, reg)': f1Score}, ignore_index=True)\n","    \n","print(record_25)\n","\n","with open(\"/content/gdrive/My Drive/SARC pol/elmo/test25.pickle\", 'wb') as handle:\n","  pickle.dump(record_25, handle)\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["project_training_25.csv \n","----------------------------------\n","   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.713447        0.647093       0.643957\n","1        0.713447        0.647093       0.643957\n","2        0.713447        0.647093       0.643957\n","3        0.713447        0.647093       0.643957\n","4        0.713447        0.647093       0.643957\n"],"name":"stdout"}]},{"metadata":{"id":"uIxNxWuRpOvE","colab_type":"text"},"cell_type":"markdown","source":["###12.5% project training set"]},{"metadata":{"id":"NXSVCfygpKIL","colab_type":"code","outputId":"cb081052-023f-4ca9-9ecf-441223032232","executionInfo":{"status":"ok","timestamp":1552472702443,"user_tz":0,"elapsed":81306,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["filename =  ['project_training_12.csv']\n","\n","record_12 = pd.DataFrame()\n","\n","for fname in filename:\n","  \n","  #Load in the training set\n","  traindf = pd.read_csv(SARC_POL+fname, index_col = 0)\n","  \n","  # Only use responses for this method. Ignore ancestors.\n","  train_resp = traindf['response'].values.tolist()\n","  \n","  train_labels = traindf['label'].values.tolist()\n","  \n","\n","  # Train resp\n","  first_resp = []\n","  second_resp = []\n","  for idx, resp in enumerate(train_resp):\n","\n","    if idx % 2 == 0:\n","      first_resp.append(resp)\n","    else:\n","      second_resp.append(resp)\n","\n","  train_docs = {0: first_resp, 1: second_resp}\n","\n","  # Train labels\n","  first_label = []\n","  second_label = []\n","  for idx, label in enumerate(train_labels):\n","\n","    if idx % 2 == 0:\n","      first_label.append(label)\n","    else:\n","      second_label.append(label)\n","\n","  train_labels = {0: first_label, 1: second_label}\n","\n"," ############################################ Khodak et al. 2017\n","  \n","  # Train a classifier on all responses in training data. We will later use this\n","  # classifier to determine for every sequence which of the 2 responses is more sarcastic.\n","  train_all_docs_tok = tokenize(train_docs[0] + train_docs[1])\n","\n","  train_all_labels = np.array(train_labels[0] + train_labels[1])\n","\n","  print(fname, \"\\n----------------------------------\")\n","    \n","  for seed in [1, 2, 3, 4, 5]:\n","    \n","    train_all_vecs = get_elmo_embeddings(elmo, train_all_docs_tok)\n","    \n","    with open(f\"/content/gdrive/My Drive/SARC pol/elmo/12embeddings_{seed}.pickle\", 'wb') as handle:\n","      pickle.dump(train_all_vecs, handle)\n","\n","    # Evaluate this classifier on all responses.\n","#     print('Evaluate the classifier on all responses')\n","    clf = LogitCV(Cs=[10**i for i in range(-2, 3)], fit_intercept=False, cv=2, dual=np.less(*train_all_vecs.shape), solver='liblinear', n_jobs=-1, random_state=seed) \n","    clf.fit(train_all_vecs, train_all_labels)\n","    #print('Train acc: ', clf.score(train_all_vecs, train_all_labels))\n","    test_acc = clf.score(test_all_vecs, test_all_labels)\n","    #print('Test acc: ', test_acc )\n","  \n","  ############################################ \n","  \n","    # Balanced Test Score Calculation\n","    nAncestors = len(test_labels[0])\n","    countCorrect = 0\n","    for i in range(nAncestors):\n","      \n","      scoreResponse0 = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0,1]\n","      scoreResponse1 = clf.predict_proba(test_all_vecs[i+nAncestors].reshape(1,-1))[0,1]\n","     \n","      if scoreResponse0 > scoreResponse1 and test_labels[0][i] == 1:\n","        countCorrect += 1\n","        \n","      elif scoreResponse1 > scoreResponse0 and test_labels[1][i] == 1:\n","        countCorrect += 1\n","       \n","    bal_test_score =  countCorrect/nAncestors\n","    #print(\"Balanced Test Score:\", bal_test_score )\n","    \n","    # F1 Prediction\n","    \n","    y_pred = []\n","    y_true = []\n","    \n","    for i in range(len(test_labels[0])+len(test_labels[1])):\n","      \n","      scoreResponse = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0][1]\n","      \n","      if i < nAncestors:\n","        \n","        y_true.append(test_labels[0][i])\n","        \n","      else:\n","        \n","        y_true.append(test_labels[1][i-nAncestors])\n","      \n","      if scoreResponse > 0.5:\n","        y_pred.append(1)\n","        \n","      else:\n","        y_pred.append(0)\n","       \n","    f1Score = f1_score(y_true, y_pred, pos_label=1, average='binary', sample_weight=None)\n","    #print(\"F1 Score:\", f1Score)\n","    \n","    #Store the results\n","    record_12 = record_12.append({'Acc (bal, bal)': bal_test_score, 'Acc (bal, reg)':test_acc , 'F1 (bal, reg)': f1Score}, ignore_index=True)\n","    \n","print(record_12)\n","\n","with open(\"/content/gdrive/My Drive/SARC pol/elmo/test12.pickle\", 'wb') as handle:\n","  pickle.dump(record_12, handle)\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["project_training_12.csv \n","----------------------------------\n","   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.694069         0.64357       0.636962\n","1        0.694069         0.64357       0.636962\n","2        0.694069         0.64357       0.636962\n","3        0.694069         0.64357       0.636962\n","4        0.694069         0.64357       0.636962\n"],"name":"stdout"}]},{"metadata":{"id":"x97f8h26o_0I","colab_type":"text"},"cell_type":"markdown","source":["###6.25% project training set"]},{"metadata":{"id":"mEnsVqZ3h-0q","colab_type":"code","outputId":"fc5e0332-d5b7-43c7-b3a3-2582941c76f5","executionInfo":{"status":"ok","timestamp":1552424441692,"user_tz":0,"elapsed":44881,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["filename =  ['project_training_6.csv']\n","\n","record_6 = pd.DataFrame()\n","\n","for fname in filename:\n","  \n","  #Load in the training set\n","  traindf = pd.read_csv(SARC_POL+fname, index_col = 0)\n","  \n","  # Only use responses for this method. Ignore ancestors.\n","  train_resp = traindf['response'].values.tolist()\n","  \n","  train_labels = traindf['label'].values.tolist()\n","  \n","\n","  # Train resp\n","  first_resp = []\n","  second_resp = []\n","  for idx, resp in enumerate(train_resp):\n","\n","    if idx % 2 == 0:\n","      first_resp.append(resp)\n","    else:\n","      second_resp.append(resp)\n","\n","  train_docs = {0: first_resp, 1: second_resp}\n","\n","  # Train labels\n","  first_label = []\n","  second_label = []\n","  for idx, label in enumerate(train_labels):\n","\n","    if idx % 2 == 0:\n","      first_label.append(label)\n","    else:\n","      second_label.append(label)\n","\n","  train_labels = {0: first_label, 1: second_label}\n","\n","  ############################################ Khodak et al. 2017 \n","  \n","  # Train a classifier on all responses in training data. We will later use this\n","  # classifier to determine for every sequence which of the 2 responses is more sarcastic.\n","  train_all_docs_tok = tokenize(train_docs[0] + train_docs[1])\n","\n","  train_all_labels = np.array(train_labels[0] + train_labels[1])\n","\n","  print(fname, \"\\n----------------------------------\")\n","    \n","  for seed in [1, 2, 3, 4, 5]:\n","    \n","    train_all_vecs = get_elmo_embeddings(elmo, train_all_docs_tok)\n","    \n","    with open(f\"/content/gdrive/My Drive/SARC pol/elmo/6embeddings_{seed}.pickle\", 'wb') as handle:\n","      pickle.dump(train_all_vecs, handle)\n","\n","   \n","      \n","    # Evaluate this classifier on all responses.\n","#     print('Evaluate the classifier on all responses')\n","    clf = LogitCV(Cs=[10**i for i in range(-2, 3)], fit_intercept=False, cv=2, dual=np.less(*train_all_vecs.shape), solver='liblinear', n_jobs=-1, random_state=seed) \n","    clf.fit(train_all_vecs, train_all_labels)\n","    #print('Train acc: ', clf.score(train_all_vecs, train_all_labels))\n","    test_acc = clf.score(test_all_vecs, test_all_labels)\n","    #print('Test acc: ', test_acc )\n","   \n","  ############################################ \n","    # Balanced Test Score Calculation\n","    nAncestors = len(test_labels[0])\n","    countCorrect = 0\n","    for i in range(nAncestors):\n","      \n","      scoreResponse0 = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0,1]\n","      scoreResponse1 = clf.predict_proba(test_all_vecs[i+nAncestors].reshape(1,-1))[0,1]\n","     \n","      if scoreResponse0 > scoreResponse1 and test_labels[0][i] == 1:\n","        countCorrect += 1\n","        \n","      elif scoreResponse1 > scoreResponse0 and test_labels[1][i] == 1:\n","        countCorrect += 1\n","       \n","    bal_test_score =  countCorrect/nAncestors\n","    #print(\"Balanced Test Score:\", bal_test_score )\n","    \n","    # F1 Prediction\n","    \n","    y_pred = []\n","    y_true = []\n","    \n","    for i in range(len(test_labels[0])+len(test_labels[1])):\n","      \n","      scoreResponse = clf.predict_proba(test_all_vecs[i].reshape(1,-1))[0][1]\n","      \n","      if i < nAncestors:\n","        \n","        y_true.append(test_labels[0][i])\n","        \n","      else:\n","        \n","        y_true.append(test_labels[1][i-nAncestors])\n","      \n","      if scoreResponse > 0.5:\n","        y_pred.append(1)\n","        \n","      else:\n","        y_pred.append(0)\n","       \n","    f1Score = f1_score(y_true, y_pred, pos_label=1, average='binary', sample_weight=None)\n","    #print(\"F1 Score:\", f1Score)\n","    \n","    #Store the results\n","    record_6 = record_6.append({'Acc (bal, bal)': bal_test_score, 'Acc (bal, reg)':test_acc , 'F1 (bal, reg)': f1Score}, ignore_index=True)\n","    \n","print(record_6)\n","\n","with open(\"/content/gdrive/My Drive/SARC pol/elmo/test6.pickle\", 'wb') as handle:\n","  pickle.dump(record_6, handle)\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["project_training_6.csv \n","----------------------------------\n","   Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","0        0.675279        0.622137       0.622912\n","1        0.675279        0.623312       0.624524\n","2        0.675279        0.623312       0.624524\n","3        0.675279        0.623312       0.624524\n","4        0.675279        0.623312       0.624524\n"],"name":"stdout"}]},{"metadata":{"id":"0_nl_okfrkRv","colab_type":"text"},"cell_type":"markdown","source":["## One data frame for GloVE results"]},{"metadata":{"id":"-eGOfK4Xrjtb","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(\"/content/gdrive/My Drive/SARC pol/elmo/test6.pickle\", 'rb') as handle:\n","  record_6 = pickle.load(handle)\n","  \n","with open(\"/content/gdrive/My Drive/SARC pol/elmo/test12.pickle\", 'rb') as handle:\n","  record_12 = pickle.load(handle)\n","\n","with open(\"/content/gdrive/My Drive/SARC pol/elmo/test25.pickle\", 'rb') as handle:\n","  record_25 = pickle.load(handle)\n","\n","with open(\"/content/gdrive/My Drive/SARC pol/elmo/test50.pickle\", 'rb') as handle:\n","  record_50 = pickle.load(handle)\n","\n","with open(\"/content/gdrive/My Drive/SARC pol/elmo/test100.pickle\", 'rb') as handle:\n","  record_100 = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aoFBdG05rpOf","colab_type":"code","colab":{}},"cell_type":"code","source":["Elmo_results = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-IDgHl4yr5AV","colab_type":"code","colab":{}},"cell_type":"code","source":["Elmo_results = Elmo_results.append(np.mean(record_100, axis=0), ignore_index=True)\n","Elmo_results = Elmo_results.append(np.mean(record_50, axis=0), ignore_index=True)\n","Elmo_results = Elmo_results.append(np.mean(record_25, axis=0), ignore_index=True)\n","Elmo_results = Elmo_results.append(np.mean(record_12, axis=0), ignore_index=True)\n","Elmo_results = Elmo_results.append(np.mean(record_6, axis=0), ignore_index=True)\n","Elmo_results.index = ['100%','50%','25%','12.5%','6.25%']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Nb7zQdaKRg35","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(\"/content/gdrive/My Drive/SARC pol/elmo/elmo_logreg_results.pickle\", 'wb') as handle:\n","  pickle.dump(Elmo_results, handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BTyBKH_tRirb","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(\"/content/gdrive/My Drive/SARC pol/elmo/elmo_logreg_results.pickle\", 'rb') as handle:\n","  Elmo_results = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ij9OhYLUsH_y","colab_type":"code","outputId":"f98ab2db-c0ee-412d-b7b8-42e1f397fca3","executionInfo":{"status":"ok","timestamp":1552735498528,"user_tz":0,"elapsed":722,"user":{"displayName":"Joe Farrington","photoUrl":"https://lh5.googleusercontent.com/-bNEtdlkbGn8/AAAAAAAAAAI/AAAAAAAAAF4/7VpSGeR6wrY/s64/photo.jpg","userId":"09132137217736073231"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["Elmo_results"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Acc (bal, bal)</th>\n","      <th>Acc (bal, reg)</th>\n","      <th>F1 (bal, reg)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>100%</th>\n","      <td>0.745156</td>\n","      <td>0.682913</td>\n","      <td>0.678188</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.729888</td>\n","      <td>0.665590</td>\n","      <td>0.664901</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.713447</td>\n","      <td>0.647093</td>\n","      <td>0.643957</td>\n","    </tr>\n","    <tr>\n","      <th>12.5%</th>\n","      <td>0.694069</td>\n","      <td>0.643570</td>\n","      <td>0.636962</td>\n","    </tr>\n","    <tr>\n","      <th>6.25%</th>\n","      <td>0.675279</td>\n","      <td>0.623077</td>\n","      <td>0.624202</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Acc (bal, bal)  Acc (bal, reg)  F1 (bal, reg)\n","100%         0.745156        0.682913       0.678188\n","50%          0.729888        0.665590       0.664901\n","25%          0.713447        0.647093       0.643957\n","12.5%        0.694069        0.643570       0.636962\n","6.25%        0.675279        0.623077       0.624202"]},"metadata":{"tags":[]},"execution_count":24}]}]}